{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== V2 GOLD CLIMATE EMISSIONS PROCESSOR ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/28 17:53:12 WARN Utils: Your hostname, 3rnese resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
            "25/08/28 17:53:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/home/ernese/miniconda3/envs/SO/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /home/ernese/.ivy2/cache\n",
            "The jars for the packages stored in: /home/ernese/.ivy2/jars\n",
            "io.delta#delta-core_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c8b41067-6f3b-4bfe-a53e-e36db4dc9b07;1.0\n",
            "\tconfs: [default]\n",
            "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
            "\tfound io.delta#delta-storage;2.4.0 in central\n",
            "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
            ":: resolution report :: resolve 148ms :: artifacts dl 6ms\n",
            "\t:: modules in use:\n",
            "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
            "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
            "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c8b41067-6f3b-4bfe-a53e-e36db4dc9b07\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
            "25/08/28 17:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.4.0\n",
            "Delta Lake support enabled\n",
            "Processing timestamp: 2025-08-28 17:53:14.443104\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries and initialize Spark session with Delta Lake\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from delta import *\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=== V2 GOLD CLIMATE EMISSIONS PROCESSOR ===\")\n",
        "\n",
        "# Initialize Spark session with Delta Lake support\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"v2-Gold-Climate-Emissions\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") \\\n",
        "    .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(\"Delta Lake support enabled\")\n",
        "print(f\"Processing timestamp: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CONFIGURATION ===\n",
            "Silver Path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver\n",
            "Gold Path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-gold\n",
            "Processing Time: 2025-08-28 17:53:23.151883\n",
            "Climate data: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver/fact_climate_weather_v2\n",
            "Location data: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver/dim_location_v2\n",
            "✓ Climate table found\n",
            "✓ Location dimension found\n"
          ]
        }
      ],
      "source": [
        "# Configuration with correct paths\n",
        "SILVER_PATH = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver\"\n",
        "GOLD_PATH = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-gold\"\n",
        "PROCESSING_TIMESTAMP = datetime.now()\n",
        "\n",
        "# Create gold directory if it doesn't exist\n",
        "os.makedirs(GOLD_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"=== CONFIGURATION ===\")\n",
        "print(f\"Silver Path: {SILVER_PATH}\")\n",
        "print(f\"Gold Path: {GOLD_PATH}\")\n",
        "print(f\"Processing Time: {PROCESSING_TIMESTAMP}\")\n",
        "\n",
        "# Define table paths with correct names\n",
        "CLIMATE_TABLE_PATH = os.path.join(SILVER_PATH, \"fact_climate_weather_v2\")\n",
        "LOCATION_TABLE_PATH = os.path.join(SILVER_PATH, \"dim_location_v2\")\n",
        "\n",
        "print(f\"Climate data: {CLIMATE_TABLE_PATH}\")\n",
        "print(f\"Location data: {LOCATION_TABLE_PATH}\")\n",
        "\n",
        "# Verify paths exist\n",
        "if os.path.exists(CLIMATE_TABLE_PATH):\n",
        "    print(\"✓ Climate table found\")\n",
        "else:\n",
        "    print(\"✗ Climate table NOT found\")\n",
        "    \n",
        "if os.path.exists(LOCATION_TABLE_PATH):\n",
        "    print(\"✓ Location dimension found\")\n",
        "else:\n",
        "    print(\"✗ Location dimension NOT found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA LOADING ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Climate data loaded: 12,880,169 records\n",
            "Climate data schema:\n",
            "root\n",
            " |-- climate_weather_id: integer (nullable = true)\n",
            " |-- location_id: integer (nullable = true)\n",
            " |-- date_id: long (nullable = true)\n",
            " |-- indicator_id: integer (nullable = true)\n",
            " |-- measurement_date: date (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- climate_metric: string (nullable = true)\n",
            " |-- metric_code: string (nullable = true)\n",
            " |-- measurement_value: double (nullable = true)\n",
            " |-- temperature_celsius: double (nullable = true)\n",
            " |-- precipitation_mm: double (nullable = true)\n",
            " |-- pressure_pascals: double (nullable = true)\n",
            " |-- humidity_percentage: double (nullable = true)\n",
            " |-- unit_of_measure: string (nullable = true)\n",
            " |-- measurement_type: string (nullable = true)\n",
            " |-- quality_flag: string (nullable = true)\n",
            " |-- data_quality_score: double (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- source_dataset: string (nullable = true)\n",
            " |-- processing_version: string (nullable = true)\n",
            " |-- created_at: timestamp (nullable = true)\n",
            " |-- updated_at: timestamp (nullable = true)\n",
            "\n",
            "Location dimension loaded: 50 records\n",
            "Location dimension schema:\n",
            "root\n",
            " |-- location_id: integer (nullable = true)\n",
            " |-- location_code: string (nullable = true)\n",
            " |-- location_name: string (nullable = true)\n",
            " |-- display_name: string (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- location_type: string (nullable = true)\n",
            " |-- iso_code: string (nullable = true)\n",
            " |-- region_code: string (nullable = true)\n",
            " |-- province_code: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- population: long (nullable = true)\n",
            " |-- is_active: boolean (nullable = true)\n",
            " |-- valid_from: timestamp (nullable = true)\n",
            " |-- valid_to: timestamp (nullable = true)\n",
            " |-- processing_version: string (nullable = true)\n",
            " |-- created_at: timestamp (nullable = true)\n",
            " |-- updated_at: timestamp (nullable = true)\n",
            "\n",
            "Location types distribution:\n",
            "+-------------+-----+----------------+\n",
            "|location_type|count|with_coordinates|\n",
            "+-------------+-----+----------------+\n",
            "|         city|   20|              20|\n",
            "|       region|   17|              17|\n",
            "|     province|   12|              12|\n",
            "|      country|    1|               1|\n",
            "+-------------+-----+----------------+\n",
            "\n",
            "Creating enriched dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 32:===================================>                    (15 + 9) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enriched dataset: 12,880,169 records\n",
            "Location join quality: 100.0% (0 records without location data)\n",
            "Sample enriched data:\n",
            "+----------------+-------------+-----------+-----------------+--------+---------+-----------+-------------+\n",
            "|measurement_date|location_name|metric_code|measurement_value|latitude|longitude|region_code|location_type|\n",
            "+----------------+-------------+-----------+-----------------+--------+---------+-----------+-------------+\n",
            "|2011-01-01      |Antipolo     |C09        |5.36             |14.5873 |121.1759 |IV-A       |city         |\n",
            "|2011-01-02      |Antipolo     |C09        |2.56             |14.5873 |121.1759 |IV-A       |city         |\n",
            "|2011-01-01      |Antipolo     |C09        |5.36             |14.5873 |121.1759 |IV-A       |city         |\n",
            "|2011-01-02      |Antipolo     |C09        |2.56             |14.5873 |121.1759 |IV-A       |city         |\n",
            "|2011-01-01      |Antipolo     |C09        |5.36             |14.5873 |121.1759 |IV-A       |city         |\n",
            "+----------------+-------------+-----------+-----------------+--------+---------+-----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "def load_climate_data_with_locations():\n",
        "    \"\"\"Load climate data and location dimension with validation\"\"\"\n",
        "    \n",
        "    print(\"=== DATA LOADING ===\")\n",
        "    \n",
        "    # Load climate fact data\n",
        "    try:\n",
        "        climate_data = spark.read.format(\"delta\").load(CLIMATE_TABLE_PATH)\n",
        "        climate_count = climate_data.count()\n",
        "        print(f\"Climate data loaded: {climate_count:,} records\")\n",
        "        \n",
        "        if climate_count == 0:\n",
        "            raise Exception(\"No climate data available\")\n",
        "            \n",
        "        print(\"Climate data schema:\")\n",
        "        climate_data.printSchema()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Cannot load climate data: {e}\")\n",
        "        raise\n",
        "        \n",
        "    # Load location dimension\n",
        "    try:\n",
        "        location_dim = spark.read.format(\"delta\").load(LOCATION_TABLE_PATH)\n",
        "        location_count = location_dim.count()\n",
        "        print(f\"Location dimension loaded: {location_count:,} records\")\n",
        "        \n",
        "        if location_count == 0:\n",
        "            print(\"WARNING: No location dimension data\")\n",
        "            return None, None, 0, 0\n",
        "        else:\n",
        "            print(\"Location dimension schema:\")\n",
        "            location_dim.printSchema()\n",
        "            \n",
        "            # Show location distribution\n",
        "            print(\"Location types distribution:\")\n",
        "            location_dim.groupBy(\"location_type\").agg(\n",
        "                count(\"*\").alias(\"count\"),\n",
        "                sum(when(col(\"latitude\").isNotNull(), 1).otherwise(0)).alias(\"with_coordinates\")\n",
        "            ).orderBy(desc(\"count\")).show()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Cannot load location dimension: {e}\")\n",
        "        raise\n",
        "        \n",
        "    # Join climate data with location dimension\n",
        "    print(\"Creating enriched dataset...\")\n",
        "    \n",
        "    # Required location columns for analysis\n",
        "    required_location_cols = [\n",
        "        \"location_id\", \"location_name\", \"display_name\", \"full_name\", \n",
        "        \"location_type\", \"latitude\", \"longitude\", \"population\",\n",
        "        \"region_code\", \"province_code\", \"iso_code\"\n",
        "    ]\n",
        "    \n",
        "    # Add missing columns if they don't exist\n",
        "    for col_name in required_location_cols:\n",
        "        if col_name not in location_dim.columns:\n",
        "            if col_name in [\"latitude\", \"longitude\"]:\n",
        "                location_dim = location_dim.withColumn(col_name, lit(None).cast(DoubleType()))\n",
        "            elif col_name == \"population\":\n",
        "                location_dim = location_dim.withColumn(col_name, lit(None).cast(LongType()))\n",
        "            else:\n",
        "                location_dim = location_dim.withColumn(col_name, lit(\"unknown\"))\n",
        "                \n",
        "    # Perform join\n",
        "    climate_with_locations = climate_data.join(\n",
        "        location_dim.select(*required_location_cols),\n",
        "        \"location_id\",\n",
        "        \"left\"\n",
        "    )\n",
        "    \n",
        "    # Cache the joined dataset\n",
        "    climate_with_locations.cache()\n",
        "    joined_count = climate_with_locations.count()\n",
        "    print(f\"Enriched dataset: {joined_count:,} records\")\n",
        "    \n",
        "    # Validate join quality\n",
        "    null_locations = climate_with_locations.filter(col(\"location_name\").isNull()).count()\n",
        "    join_quality = ((joined_count - null_locations) / joined_count * 100) if joined_count > 0 else 0\n",
        "    print(f\"Location join quality: {join_quality:.1f}% ({null_locations:,} records without location data)\")\n",
        "    \n",
        "    if join_quality < 95:\n",
        "        print(\"WARNING: Low location join quality may affect mapping\")\n",
        "        \n",
        "    # Show sample joined data\n",
        "    print(\"Sample enriched data:\")\n",
        "    climate_with_locations.select(\n",
        "        \"measurement_date\", \"location_name\", \"metric_code\", \"measurement_value\",\n",
        "        \"latitude\", \"longitude\", \"region_code\", \"location_type\"\n",
        "    ).show(5, truncate=False)\n",
        "    \n",
        "    return climate_with_locations, location_dim, joined_count, join_quality\n",
        "\n",
        "# Execute the function\n",
        "climate_data, location_dim, total_records, location_quality = load_climate_data_with_locations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA QUALITY ANALYSIS ===\n",
            "Total records: 12,880,169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Geographic Coverage:\n",
            "  Unique locations: 27\n",
            "  Unique regions: 15\n",
            "  Unique provinces: 12\n",
            "  Climate metrics: 7\n",
            "  Records with coordinates: 12,880,169\n",
            "  Coordinate coverage: 100.0%\n",
            "\n",
            "Data Coverage:\n",
            "  Date range: 1981-01-01 to 2025-07-11\n",
            "  Value range: 0.00 to 573.72\n",
            "  Average measurement: 44.35\n",
            "\n",
            "Climate metrics distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----------------+------------------+---------+---------+\n",
            "|metric_code|record_count|unique_locations|avg_value         |min_value|max_value|\n",
            "+-----------+------------+----------------+------------------+---------+---------+\n",
            "|C01        |2146584     |27              |26.00481007032577 |13.37    |33.16    |\n",
            "|C03        |2146584     |27              |29.18326591458813 |15.68    |42.2     |\n",
            "|C04        |2146584     |27              |23.704765958378733|9.11     |30.28    |\n",
            "|C09        |2146584     |27              |6.204945429575948 |0.0      |573.72   |\n",
            "|C12        |2146716     |27              |98.4753380419185  |90.39    |102.6    |\n",
            "|C13        |2146584     |27              |82.53220199162902 |42.94    |98.51    |\n",
            "|C23        |533         |1               |27.771707317073172|25.13    |30.02    |\n",
            "+-----------+------------+----------------+------------------+---------+---------+\n",
            "\n",
            "\n",
            "Location type distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------+---------+-------+-----------------+\n",
            "|location_type|total_records|locations|metrics|avg_value        |\n",
            "+-------------+-------------+---------+-------+-----------------+\n",
            "|city         |11709293     |15       |7      |44.32972416865778|\n",
            "|province     |1170876      |12       |6      |44.56108955175462|\n",
            "+-------------+-------------+---------+-------+-----------------+\n",
            "\n",
            "\n",
            "Regional distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+---------+-------+------------------+\n",
            "|region_code|total_records|locations|metrics|avg_value         |\n",
            "+-----------+-------------+---------+-------+------------------+\n",
            "|IV-A       |10733563     |5        |7      |44.32887569672768 |\n",
            "|VI         |390292       |4        |6      |44.41441630881502 |\n",
            "|III        |292719       |3        |6      |44.43002029249902 |\n",
            "|VII        |195146       |2        |6      |44.6679963719472  |\n",
            "|NCR        |195146       |2        |6      |44.682634540292966|\n",
            "|XI         |195146       |2        |6      |44.607504945015634|\n",
            "|CAR        |97573        |1        |6      |42.38717288594179 |\n",
            "|XIII       |97573        |1        |6      |44.69028839945477 |\n",
            "|IV-B       |97573        |1        |6      |44.66710739651339 |\n",
            "|V          |97573        |1        |6      |45.27187562132963 |\n",
            "|IX         |97573        |1        |6      |44.54227532206656 |\n",
            "|X          |97573        |1        |6      |44.587479938097644|\n",
            "|I          |97573        |1        |6      |44.49224847037604 |\n",
            "|VIII       |97573        |1        |6      |45.20127893987065 |\n",
            "|XII        |97573        |1        |6      |43.41992805386732 |\n",
            "+-----------+-------------+---------+-------+------------------+\n",
            "\n",
            "\n",
            "Temporal distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 63:==========================================>             (18 + 6) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------------+---------+-------+------------------+\n",
            "|year|total_records|locations|metrics|avg_value         |\n",
            "+----+-------------+---------+-------+------------------+\n",
            "|1981|289092       |27       |7      |43.74847439569366 |\n",
            "|1982|289092       |27       |7      |43.44326684930749 |\n",
            "|1983|289092       |27       |7      |43.0613443471281  |\n",
            "|1984|289884       |27       |7      |43.39610092312773 |\n",
            "|1985|289092       |27       |7      |43.54828051277818 |\n",
            "|1986|289092       |27       |7      |43.66883251698429 |\n",
            "|1987|289092       |27       |7      |43.48654985264187 |\n",
            "|1988|289884       |27       |7      |44.06459263015544 |\n",
            "|1989|289092       |27       |7      |44.21164688057769 |\n",
            "|1990|289092       |27       |7      |44.07338947463139 |\n",
            "|1991|289092       |27       |7      |43.78850085785809 |\n",
            "|1992|289884       |27       |7      |43.51043296628976 |\n",
            "|1993|289092       |27       |7      |43.86579562907306 |\n",
            "|1994|289092       |27       |7      |44.03189057462701 |\n",
            "|1995|289092       |27       |7      |44.193797718373474|\n",
            "|1996|289884       |27       |7      |44.39505698831221 |\n",
            "|1997|289092       |27       |7      |43.7812805957964  |\n",
            "|1998|289092       |27       |7      |44.0676034272826  |\n",
            "|1999|289092       |27       |7      |44.97296991961041 |\n",
            "|2000|289884       |27       |7      |44.92865953277831 |\n",
            "+----+-------------+---------+-------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "DATA QUALITY: VALIDATED\n",
            "Dataset ready for aggregation and visualization\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
        "try:\n",
        "    # Basic statistics with geographic context\n",
        "    print(f\"Total records: {total_records:,}\")\n",
        "    \n",
        "    # Geographic coverage analysis\n",
        "    geo_coverage = climate_data.agg(\n",
        "        countDistinct(\"location_id\").alias(\"unique_locations\"),\n",
        "        countDistinct(\"region_code\").alias(\"unique_regions\"),\n",
        "        countDistinct(\"province_code\").alias(\"unique_provinces\"),\n",
        "        countDistinct(\"metric_code\").alias(\"unique_metrics\"),\n",
        "        sum(when(col(\"latitude\").isNotNull(), 1).otherwise(0)).alias(\"records_with_coordinates\"),\n",
        "        min(\"measurement_date\").alias(\"earliest_date\"),\n",
        "        max(\"measurement_date\").alias(\"latest_date\"),\n",
        "        avg(\"measurement_value\").alias(\"avg_value\"),\n",
        "        min(\"measurement_value\").alias(\"min_value\"),\n",
        "        max(\"measurement_value\").alias(\"max_value\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"\\nGeographic Coverage:\")\n",
        "    print(f\"  Unique locations: {geo_coverage['unique_locations']:,}\")\n",
        "    print(f\"  Unique regions: {geo_coverage['unique_regions']:,}\")\n",
        "    print(f\"  Unique provinces: {geo_coverage['unique_provinces']:,}\")\n",
        "    print(f\"  Climate metrics: {geo_coverage['unique_metrics']:,}\")\n",
        "    print(f\"  Records with coordinates: {geo_coverage['records_with_coordinates']:,}\")\n",
        "    \n",
        "    coord_percentage = (geo_coverage['records_with_coordinates'] / total_records * 100) if total_records > 0 else 0\n",
        "    print(f\"  Coordinate coverage: {coord_percentage:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nData Coverage:\")\n",
        "    print(f\"  Date range: {geo_coverage['earliest_date']} to {geo_coverage['latest_date']}\")\n",
        "    print(f\"  Value range: {geo_coverage['min_value']:.2f} to {geo_coverage['max_value']:.2f}\")\n",
        "    print(f\"  Average measurement: {geo_coverage['avg_value']:.2f}\")\n",
        "    \n",
        "    # Climate metrics distribution\n",
        "    print(f\"\\nClimate metrics distribution:\")\n",
        "    metric_dist = climate_data.groupBy(\"metric_code\") \\\n",
        "                                .agg(count(\"*\").alias(\"record_count\"),\n",
        "                                     countDistinct(\"location_id\").alias(\"unique_locations\"),\n",
        "                                     avg(\"measurement_value\").alias(\"avg_value\"),\n",
        "                                     min(\"measurement_value\").alias(\"min_value\"),\n",
        "                                     max(\"measurement_value\").alias(\"max_value\")) \\\n",
        "                                .orderBy(\"metric_code\")\n",
        "                                \n",
        "    metric_dist.show(20, truncate=False)\n",
        "    \n",
        "    # Location type distribution\n",
        "    print(f\"\\nLocation type distribution:\")\n",
        "    location_dist = climate_data.groupBy(\"location_type\") \\\n",
        "                                  .agg(count(\"*\").alias(\"total_records\"),\n",
        "                                       countDistinct(\"location_id\").alias(\"locations\"),\n",
        "                                       countDistinct(\"metric_code\").alias(\"metrics\"),\n",
        "                                       avg(\"measurement_value\").alias(\"avg_value\")) \\\n",
        "                                  .orderBy(desc(\"total_records\"))\n",
        "                                  \n",
        "    location_dist.show(20, truncate=False)\n",
        "    \n",
        "    # Regional distribution for maps\n",
        "    if geo_coverage['unique_regions'] > 1:\n",
        "        print(f\"\\nRegional distribution:\")\n",
        "        regional_dist = climate_data.filter(col(\"region_code\") != \"unknown\") \\\n",
        "                                      .groupBy(\"region_code\") \\\n",
        "                                      .agg(count(\"*\").alias(\"total_records\"),\n",
        "                                           countDistinct(\"location_id\").alias(\"locations\"),\n",
        "                                           countDistinct(\"metric_code\").alias(\"metrics\"),\n",
        "                                           avg(\"measurement_value\").alias(\"avg_value\")) \\\n",
        "                                      .orderBy(desc(\"total_records\"))\n",
        "                                      \n",
        "        regional_dist.show(20, truncate=False)\n",
        "        \n",
        "    # Time series analysis\n",
        "    print(f\"\\nTemporal distribution:\")\n",
        "    temporal_dist = climate_data.groupBy(\"year\") \\\n",
        "                                .agg(count(\"*\").alias(\"total_records\"),\n",
        "                                     countDistinct(\"location_id\").alias(\"locations\"),\n",
        "                                     countDistinct(\"metric_code\").alias(\"metrics\"),\n",
        "                                     avg(\"measurement_value\").alias(\"avg_value\")) \\\n",
        "                                .orderBy(\"year\")\n",
        "                                \n",
        "    temporal_dist.show(20, truncate=False)\n",
        "    \n",
        "    print(f\"\\nDATA QUALITY: VALIDATED\")\n",
        "    print(f\"Dataset ready for aggregation and visualization\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error in data quality analysis: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== QUARTERLY AGGREGATIONS ===\n",
            "Creating quarterly aggregations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid records for quarterly aggregation: 12,880,169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quarterly aggregations created: 29,176 records\n",
            "Sample quarterly data:\n",
            "+-------+-------------+-----------+------------------+--------+---------+-----------+\n",
            "|quarter|location_name|metric_code|quarterly_value   |latitude|longitude|region_name|\n",
            "+-------+-------------+-----------+------------------+--------+---------+-----------+\n",
            "|1981-Q1|Antipolo     |C01        |24.557411949685434|14.5873 |121.1759 |Calabarzon |\n",
            "|1981-Q1|Antipolo     |C03        |28.37868658280918 |14.5873 |121.1759 |Calabarzon |\n",
            "|1981-Q1|Antipolo     |C04        |21.841535639413213|14.5873 |121.1759 |Calabarzon |\n",
            "|1981-Q1|Antipolo     |C09        |2.6336299790355975|14.5873 |121.1759 |Calabarzon |\n",
            "|1981-Q1|Antipolo     |C12        |98.57743501048324 |14.5873 |121.1759 |Calabarzon |\n",
            "+-------+-------------+-----------+------------------+--------+---------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "QUARTERLY AGGREGATION: SUCCESS\n"
          ]
        }
      ],
      "source": [
        "# Quarterly aggregations with complete geographic context\n",
        "print(\"=== QUARTERLY AGGREGATIONS ===\")\n",
        "\n",
        "try:\n",
        "    print(\"Creating quarterly aggregations...\")\n",
        "    \n",
        "    # Prepare base dataset for quarterly aggregation\n",
        "    quarterly_base = climate_data.select(\n",
        "        \"location_id\", \"indicator_id\", \"measurement_date\", \"year\", \"metric_code\", \"climate_metric\",\n",
        "        \"measurement_value\", \"unit_of_measure\", \"location_name\", \"display_name\", \"full_name\",\n",
        "        \"location_type\", \"latitude\", \"longitude\", \"population\", \"region_code\", \"province_code\", \"iso_code\"\n",
        "    ).filter(\n",
        "        col(\"measurement_value\").isNotNull() &\n",
        "        col(\"measurement_date\").isNotNull() &\n",
        "        col(\"metric_code\").isNotNull() &\n",
        "        col(\"location_id\").isNotNull() &\n",
        "        col(\"location_name\").isNotNull()\n",
        "    )\n",
        "\n",
        "    # Add temporal components\n",
        "    quarterly_base = quarterly_base.withColumn(\n",
        "        \"quarter_number\", quarter(col(\"measurement_date\"))\n",
        "    ).withColumn(\n",
        "        \"quarter\", concat(col(\"year\"), lit(\"-Q\"), col(\"quarter_number\"))\n",
        "    ).withColumn(\n",
        "        \"quarter_start_date\", date_trunc(\"quarter\", col(\"measurement_date\"))\n",
        "    )\n",
        "    \n",
        "    # Cache and validate\n",
        "    quarterly_base.cache()\n",
        "    base_count = quarterly_base.count()\n",
        "    print(f\"Valid records for quarterly aggregation: {base_count:,}\")\n",
        "    \n",
        "    if base_count == 0:\n",
        "        raise Exception(\"No valid records for quarterly aggregation\")\n",
        "    \n",
        "    # Create comprehensive quarterly aggregations\n",
        "    quarterly_climate = quarterly_base.groupBy(\n",
        "        # Core identifiers\n",
        "        \"location_id\", \"indicator_id\", \"year\", \"quarter\", \"quarter_number\", \n",
        "        \"quarter_start_date\", \"metric_code\", \"climate_metric\",\n",
        "        # Geographic dimensions\n",
        "        \"location_name\", \"display_name\", \"full_name\", \"location_type\",\n",
        "        \"latitude\", \"longitude\", \"population\", \"region_code\", \"province_code\", \"iso_code\"\n",
        "    ).agg(\n",
        "        avg(\"measurement_value\").alias(\"quarterly_value\"),\n",
        "        min(\"measurement_value\").alias(\"min_daily_value\"),\n",
        "        max(\"measurement_value\").alias(\"max_daily_value\"),\n",
        "        stddev(\"measurement_value\").alias(\"stddev_daily_value\"),\n",
        "        count(\"measurement_value\").alias(\"daily_records_count\"),\n",
        "        min(\"measurement_date\").alias(\"period_start_date\"),\n",
        "        max(\"measurement_date\").alias(\"period_end_date\"),\n",
        "        first(\"unit_of_measure\").alias(\"unit_of_measure\")\n",
        "    )\n",
        "    \n",
        "    # Add metadata\n",
        "    quarterly_climate = quarterly_climate.withColumn(\n",
        "        \"aggregation_type\", lit(\"AVG\")\n",
        "    ).withColumn(\n",
        "        \"quarterly_climate_id\", \n",
        "        row_number().over(Window.partitionBy(lit(1)).orderBy(\"year\", \"quarter_number\", \"location_id\", \"metric_code\"))\n",
        "    ).withColumn(\n",
        "        \"created_at\", lit(PROCESSING_TIMESTAMP)\n",
        "    ).withColumn(\n",
        "        \"updated_at\", lit(PROCESSING_TIMESTAMP)\n",
        "    ).withColumn(\n",
        "        \"processing_version\", lit(\"V2\")\n",
        "    ).withColumn(\n",
        "        \"country_name\", lit(\"Philippines\")\n",
        "    ).withColumn(\n",
        "        \"region_name\", \n",
        "        when(col(\"region_code\") == \"NCR\", \"National Capital Region\")\n",
        "        .when(col(\"region_code\") == \"CAR\", \"Cordillera Administrative Region\")\n",
        "        .when(col(\"region_code\") == \"I\", \"Ilocos Region\")\n",
        "        .when(col(\"region_code\") == \"II\", \"Cagayan Valley\")\n",
        "        .when(col(\"region_code\") == \"III\", \"Central Luzon\")\n",
        "        .when(col(\"region_code\") == \"IV-A\", \"Calabarzon\")\n",
        "        .when(col(\"region_code\") == \"IV-B\", \"MIMAROPA\")\n",
        "        .when(col(\"region_code\") == \"V\", \"Bicol Region\")\n",
        "        .when(col(\"region_code\") == \"VI\", \"Western Visayas\")\n",
        "        .when(col(\"region_code\") == \"VII\", \"Central Visayas\")\n",
        "        .when(col(\"region_code\") == \"VIII\", \"Eastern Visayas\")\n",
        "        .when(col(\"region_code\") == \"IX\", \"Zamboanga Peninsula\")\n",
        "        .when(col(\"region_code\") == \"X\", \"Northern Mindanao\")\n",
        "        .when(col(\"region_code\") == \"XI\", \"Davao Region\")\n",
        "        .when(col(\"region_code\") == \"XII\", \"SOCCSKSARGEN\")\n",
        "        .when(col(\"region_code\") == \"XIII\", \"Caraga\")\n",
        "        .when(col(\"region_code\") == \"BARMM\", \"Bangsamoro Autonomous Region in Muslim Mindanao\")\n",
        "        .otherwise(col(\"region_code\"))\n",
        "    ).withColumn(\n",
        "        \"coordinate_text\", \n",
        "        when(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull(),\n",
        "            concat(format_number(\"latitude\", 4), lit(\", \"), format_number(\"longitude\", 4))\n",
        "        ).otherwise(lit(\"No coordinates\"))\n",
        "    )\n",
        "    \n",
        "    # Final column selection\n",
        "    quarterly_columns = [\n",
        "        \"quarterly_climate_id\", \"location_id\", \"indicator_id\", \"year\", \"quarter\", \n",
        "        \"quarter_number\", \"quarter_start_date\", \"period_start_date\", \"period_end_date\",\n",
        "        \"metric_code\", \"climate_metric\", \"quarterly_value\", \"aggregation_type\",\n",
        "        \"unit_of_measure\", \"min_daily_value\", \"max_daily_value\", \"stddev_daily_value\",\n",
        "        \"daily_records_count\",\n",
        "        # Geographic columns\n",
        "        \"location_name\", \"display_name\", \"full_name\", \"location_type\",\n",
        "        \"latitude\", \"longitude\", \"coordinate_text\", \"population\",\n",
        "        \"country_name\", \"region_code\", \"region_name\", \"province_code\", \"iso_code\",\n",
        "        # Metadata\n",
        "        \"processing_version\", \"created_at\", \"updated_at\"\n",
        "    ]\n",
        "    \n",
        "    quarterly_climate = quarterly_climate.select(*quarterly_columns)\n",
        "    \n",
        "    # Cache and validate\n",
        "    quarterly_climate.cache()\n",
        "    quarterly_count = quarterly_climate.count()\n",
        "    print(f\"Quarterly aggregations created: {quarterly_count:,} records\")\n",
        "    \n",
        "    if quarterly_count == 0:\n",
        "        raise Exception(\"No quarterly aggregations created\")\n",
        "    \n",
        "    # Show sample\n",
        "    print(\"Sample quarterly data:\")\n",
        "    quarterly_climate.select(\n",
        "        \"quarter\", \"location_name\", \"metric_code\", \"quarterly_value\", \n",
        "        \"latitude\", \"longitude\", \"region_name\"\n",
        "    ).show(5, truncate=False)\n",
        "    \n",
        "    print(\"QUARTERLY AGGREGATION: SUCCESS\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR in quarterly aggregation: {e}\")\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    try:\n",
        "        quarterly_base.unpersist()\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ANNUAL AGGREGATIONS ===\n",
            "Creating annual aggregations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid records for annual aggregation: 12,880,169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annual aggregations created: 7,335 records\n",
            "Sample annual data:\n",
            "+----+-------------+-----------+------------------+--------+---------+-----------+\n",
            "|year|location_name|metric_code|annual_value      |latitude|longitude|region_name|\n",
            "+----+-------------+-----------+------------------+--------+---------+-----------+\n",
            "|1981|Antipolo     |C01        |25.897904368053315|14.5873 |121.1759 |Calabarzon |\n",
            "|1981|Antipolo     |C03        |29.29851641251008 |14.5873 |121.1759 |Calabarzon |\n",
            "|1981|Antipolo     |C04        |23.47222176272974 |14.5873 |121.1759 |Calabarzon |\n",
            "|1981|Antipolo     |C09        |4.490132850865767 |14.5873 |121.1759 |Calabarzon |\n",
            "|1981|Antipolo     |C12        |98.3710049108307  |14.5873 |121.1759 |Calabarzon |\n",
            "+----+-------------+-----------+------------------+--------+---------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "ANNUAL AGGREGATION: SUCCESS\n"
          ]
        }
      ],
      "source": [
        "# Annual aggregations with complete geographic context\n",
        "print(\"=== ANNUAL AGGREGATIONS ===\")\n",
        "\n",
        "try:\n",
        "    print(\"Creating annual aggregations...\")\n",
        "\n",
        "    # Prepare base dataset for annual aggregation\n",
        "    annual_base = climate_data.select(\n",
        "        \"location_id\", \"indicator_id\", \"measurement_date\", \"year\", \"metric_code\", \"climate_metric\",\n",
        "        \"measurement_value\", \"unit_of_measure\", \"location_name\", \"display_name\", \"full_name\",\n",
        "        \"location_type\", \"latitude\", \"longitude\", \"population\", \"region_code\", \"province_code\", \"iso_code\"\n",
        "    ).filter(\n",
        "        col(\"measurement_value\").isNotNull() &\n",
        "        col(\"measurement_date\").isNotNull() &\n",
        "        col(\"metric_code\").isNotNull() &\n",
        "        col(\"location_id\").isNotNull() &\n",
        "        col(\"location_name\").isNotNull()\n",
        "    )\n",
        "\n",
        "    # Add year start date\n",
        "    annual_base = annual_base.withColumn(\n",
        "        \"year_start_date\", date_trunc(\"year\", col(\"measurement_date\"))\n",
        "    )\n",
        "\n",
        "    # Cache and validate\n",
        "    annual_base.cache()\n",
        "    base_count = annual_base.count()\n",
        "    print(f\"Valid records for annual aggregation: {base_count:,}\")\n",
        "\n",
        "    if base_count == 0:\n",
        "        raise Exception(\"No valid records for annual aggregation\")\n",
        "\n",
        "    # Perform annual aggregation\n",
        "    annual_climate = annual_base.groupBy(\n",
        "        \"location_id\", \"indicator_id\", \"year\", \"year_start_date\",\n",
        "        \"metric_code\", \"climate_metric\",\n",
        "        \"location_name\", \"display_name\", \"full_name\", \"location_type\",\n",
        "        \"latitude\", \"longitude\", \"population\", \"region_code\", \"province_code\", \"iso_code\"\n",
        "    ).agg(\n",
        "        avg(\"measurement_value\").alias(\"annual_value\"),\n",
        "        min(\"measurement_value\").alias(\"min_daily_value\"),\n",
        "        max(\"measurement_value\").alias(\"max_daily_value\"),\n",
        "        stddev(\"measurement_value\").alias(\"stddev_daily_value\"),\n",
        "        count(\"measurement_value\").alias(\"daily_records_count\"),\n",
        "        min(\"measurement_date\").alias(\"period_start_date\"),\n",
        "        max(\"measurement_date\").alias(\"period_end_date\"),\n",
        "        first(\"unit_of_measure\").alias(\"unit_of_measure\")\n",
        "    )\n",
        "\n",
        "    # Add metadata columns\n",
        "    annual_climate = annual_climate.withColumn(\"aggregation_type\", lit(\"AVG\")) \\\n",
        "        .withColumn(\n",
        "            \"annual_climate_id\",\n",
        "            row_number().over(Window.partitionBy(lit(1)).orderBy(\"year\", \"location_id\", \"metric_code\"))\n",
        "        ) \\\n",
        "        .withColumn(\"created_at\", lit(PROCESSING_TIMESTAMP)) \\\n",
        "        .withColumn(\"updated_at\", lit(PROCESSING_TIMESTAMP)) \\\n",
        "        .withColumn(\"processing_version\", lit(\"V2\")) \\\n",
        "        .withColumn(\"country_name\", lit(\"Philippines\")) \\\n",
        "        .withColumn(\n",
        "            \"region_name\",\n",
        "            when(col(\"region_code\") == \"NCR\", \"National Capital Region\")\n",
        "            .when(col(\"region_code\") == \"CAR\", \"Cordillera Administrative Region\")\n",
        "            .when(col(\"region_code\") == \"I\", \"Ilocos Region\")\n",
        "            .when(col(\"region_code\") == \"II\", \"Cagayan Valley\")\n",
        "            .when(col(\"region_code\") == \"III\", \"Central Luzon\")\n",
        "            .when(col(\"region_code\") == \"IV-A\", \"Calabarzon\")\n",
        "            .when(col(\"region_code\") == \"IV-B\", \"MIMAROPA\")\n",
        "            .when(col(\"region_code\") == \"V\", \"Bicol Region\")\n",
        "            .when(col(\"region_code\") == \"VI\", \"Western Visayas\")\n",
        "            .when(col(\"region_code\") == \"VII\", \"Central Visayas\")\n",
        "            .when(col(\"region_code\") == \"VIII\", \"Eastern Visayas\")\n",
        "            .when(col(\"region_code\") == \"IX\", \"Zamboanga Peninsula\")\n",
        "            .when(col(\"region_code\") == \"X\", \"Northern Mindanao\")\n",
        "            .when(col(\"region_code\") == \"XI\", \"Davao Region\")\n",
        "            .when(col(\"region_code\") == \"XII\", \"SOCCSKSARGEN\")\n",
        "            .when(col(\"region_code\") == \"XIII\", \"Caraga\")\n",
        "            .when(col(\"region_code\") == \"BARMM\", \"Bangsamoro Autonomous Region in Muslim Mindanao\")\n",
        "            .otherwise(col(\"region_code\"))\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"coordinate_text\",\n",
        "            when(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull(),\n",
        "                 concat(format_number(col(\"latitude\"), 4), lit(\", \"), format_number(col(\"longitude\"), 4))\n",
        "                 ).otherwise(lit(\"No coordinates\"))\n",
        "        )\n",
        "\n",
        "    # Final column selection\n",
        "    annual_columns = [\n",
        "        \"annual_climate_id\", \"location_id\", \"indicator_id\", \"year\", \"year_start_date\",\n",
        "        \"period_start_date\", \"period_end_date\", \"metric_code\", \"climate_metric\",\n",
        "        \"annual_value\", \"aggregation_type\", \"unit_of_measure\", \"min_daily_value\",\n",
        "        \"max_daily_value\", \"stddev_daily_value\", \"daily_records_count\",\n",
        "        \"location_name\", \"display_name\", \"full_name\", \"location_type\",\n",
        "        \"latitude\", \"longitude\", \"coordinate_text\", \"population\",\n",
        "        \"country_name\", \"region_code\", \"region_name\", \"province_code\", \"iso_code\",\n",
        "        \"processing_version\", \"created_at\", \"updated_at\"\n",
        "    ]\n",
        "\n",
        "    annual_climate = annual_climate.select(*annual_columns)\n",
        "\n",
        "    # Cache and validate\n",
        "    annual_climate.cache()\n",
        "    annual_count = annual_climate.count()\n",
        "    print(f\"Annual aggregations created: {annual_count:,} records\")\n",
        "\n",
        "    if annual_count == 0:\n",
        "        raise Exception(\"No annual aggregations created\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"Sample annual data:\")\n",
        "    annual_climate.select(\n",
        "        \"year\", \"location_name\", \"metric_code\", \"annual_value\",\n",
        "        \"latitude\", \"longitude\", \"region_name\"\n",
        "    ).show(5, truncate=False)\n",
        "\n",
        "    print(\"ANNUAL AGGREGATION: SUCCESS\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR in annual aggregation: {e}\")\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    try:\n",
        "        annual_base.unpersist()\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save function ready\n"
          ]
        }
      ],
      "source": [
        "def save_gold_table(df, table_name, partition_cols=None):\n",
        "    \"\"\"Save DataFrame as Delta table in Gold layer\"\"\"\n",
        "    output_path = os.path.join(GOLD_PATH, table_name)\n",
        "\n",
        "    print(f\"\\n=== SAVING GOLD TABLE: {table_name.upper()} ===\")\n",
        "    print(f\"Target path: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        # Validate DataFrame\n",
        "        if df is None:\n",
        "            print(\"ERROR: Input DataFrame is None\")\n",
        "            return False, 0\n",
        "\n",
        "        record_count = df.count()\n",
        "        print(f\"Records to save: {record_count:,}\")\n",
        "\n",
        "        if record_count == 0:\n",
        "            print(\"WARNING: No data to save.\")\n",
        "            return False, 0\n",
        "\n",
        "        # Configure writer with optimizations\n",
        "        writer = (\n",
        "            df.write.format(\"delta\")\n",
        "            .mode(\"overwrite\")\n",
        "            .option(\"overwriteSchema\", \"true\")\n",
        "            .option(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
        "            .option(\"delta.autoOptimize.autoCompact\", \"true\")\n",
        "        )\n",
        "\n",
        "        # Apply partitioning if specified\n",
        "        if partition_cols:\n",
        "            valid_partition_cols = [c for c in partition_cols if c in df.columns]\n",
        "            if valid_partition_cols:\n",
        "                writer = writer.partitionBy(*valid_partition_cols)\n",
        "                print(f\"Partitioning by: {valid_partition_cols}\")\n",
        "\n",
        "        # Save to Delta table\n",
        "        print(\"Writing data to Delta...\")\n",
        "        writer.save(output_path)\n",
        "        print(\"Save completed.\")\n",
        "\n",
        "        # Validate saved data\n",
        "        saved_count = spark.read.format(\"delta\").load(output_path).count()\n",
        "        print(f\"Records saved: {saved_count:,}\")\n",
        "\n",
        "        return True, saved_count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR saving table {table_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False, 0\n",
        "\n",
        "    finally:\n",
        "        try:\n",
        "            df.unpersist()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"Save function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GOLD LAYER SAVES ===\n",
            "\n",
            "=== SAVING GOLD TABLE: GOLD_CLIMATE_QUARTERLY_V2 ===\n",
            "Target path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-gold/gold_climate_quarterly_v2\n",
            "Records to save: 29,176\n",
            "Partitioning by: ['year', 'region_code']\n",
            "Writing data to Delta...\n",
            "You are setting a property: delta.autooptimize.autocompact that is not recognized by this version of Delta\n",
            "You are setting a property: delta.autooptimize.optimizewrite that is not recognized by this version of Delta\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save completed.\n",
            "Records saved: 29,176\n",
            "\n",
            "=== SAVING GOLD TABLE: GOLD_CLIMATE_ANNUAL_V2 ===\n",
            "Target path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-gold/gold_climate_annual_v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records to save: 7,335\n",
            "Partitioning by: ['year', 'region_code']\n",
            "Writing data to Delta...\n",
            "You are setting a property: delta.autooptimize.autocompact that is not recognized by this version of Delta\n",
            "You are setting a property: delta.autooptimize.optimizewrite that is not recognized by this version of Delta\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save completed.\n",
            "Records saved: 7,335\n",
            "\n",
            "=== PROCESSING SUMMARY ===\n",
            "Source records: 12,880,169\n",
            "Location join quality: 100.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geographic coverage: 27 locations across 15 regions\n",
            "\n",
            "QUARTERLY TABLE:\n",
            "  Status: SUCCESS\n",
            "  Records saved: 29,176\n",
            "  Table: gold_climate_quarterly_v2\n",
            "  Partitioning: year, region_code\n",
            "\n",
            "ANNUAL TABLE:\n",
            "  Status: SUCCESS\n",
            "  Records saved: 7,335\n",
            "  Table: gold_climate_annual_v2\n",
            "  Partitioning: year, region_code\n",
            "\n",
            "OVERALL STATUS: SUCCESS\n",
            "Total saved records: 36,511\n",
            "Gold layer path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-gold\n",
            "Processing version: V2\n",
            "Processing timestamp: 2025-08-28 17:53:23.151883\n"
          ]
        }
      ],
      "source": [
        "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\", \"true\")\n",
        "\n",
        "print(\"=== GOLD LAYER SAVES ===\")\n",
        "\n",
        "# Save quarterly table\n",
        "quarterly_success, quarterly_saved_count = save_gold_table(\n",
        "    quarterly_climate,\n",
        "    \"gold_climate_quarterly_v2\",\n",
        "    [\"year\", \"region_code\"]\n",
        ")\n",
        "\n",
        "# Save annual table\n",
        "annual_success, annual_saved_count = save_gold_table(\n",
        "    annual_climate,\n",
        "    \"gold_climate_annual_v2\",\n",
        "    [\"year\", \"region_code\"]\n",
        ")\n",
        "\n",
        "# Final processing summary\n",
        "print(\"\\n=== PROCESSING SUMMARY ===\")\n",
        "print(f\"Source records: {total_records:,}\")\n",
        "print(f\"Location join quality: {location_quality:.1f}%\")\n",
        "\n",
        "# Geographic coverage summary\n",
        "unique_locations = climate_data.select(\"location_name\").distinct().count()\n",
        "unique_regions = climate_data.agg(countDistinct(\"region_code\")).collect()[0][0]\n",
        "print(f\"Geographic coverage: {unique_locations} locations across {unique_regions} regions\")\n",
        "\n",
        "print(\"\\nQUARTERLY TABLE:\")\n",
        "print(f\"  Status: {'SUCCESS' if quarterly_success else 'FAILED'}\")\n",
        "print(f\"  Records saved: {quarterly_saved_count:,}\")\n",
        "print(\"  Table: gold_climate_quarterly_v2\")\n",
        "print(\"  Partitioning: year, region_code\")\n",
        "\n",
        "print(\"\\nANNUAL TABLE:\")\n",
        "print(f\"  Status: {'SUCCESS' if annual_success else 'FAILED'}\")\n",
        "print(f\"  Records saved: {annual_saved_count:,}\")\n",
        "print(\"  Table: gold_climate_annual_v2\")\n",
        "print(\"  Partitioning: year, region_code\")\n",
        "\n",
        "overall_status = \"SUCCESS\" if (quarterly_success and annual_success) else \"PARTIAL COMPLETION\"\n",
        "print(f\"\\nOVERALL STATUS: {overall_status}\")\n",
        "\n",
        "if quarterly_success or annual_success:\n",
        "    total_saved = quarterly_saved_count + annual_saved_count\n",
        "    print(f\"Total saved records: {total_saved:,}\")\n",
        "    print(f\"Gold layer path: {GOLD_PATH}\")\n",
        "    print(\"Processing version: V2\")\n",
        "    print(f\"Processing timestamp: {PROCESSING_TIMESTAMP}\")\n",
        "else:\n",
        "    print(\"No data was saved - check error logs above\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SO",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
