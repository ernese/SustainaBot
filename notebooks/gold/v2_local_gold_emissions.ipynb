{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and initialize a local Spark session with Delta Lake\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== V2 LOCAL GOLD EMISSIONS PROCESSOR ===\")\n",
    "\n",
    "# Initialize a local Spark session with Delta Lake support\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"v2-Local-Gold-Emissions\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Delta Lake support enabled for local environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for local file paths\n",
    "PROCESSING_TIMESTAMP = datetime.now()\n",
    "\n",
    "# Corrected relative paths from 'notebooks/new/gold' to project root\n",
    "base_path = \"../../../\"\n",
    "silver_path = os.path.abspath(os.path.join(base_path, 'final-spark-silver'))\n",
    "gold_path = os.path.abspath(os.path.join(base_path, 'final-spark-gold'))\n",
    "\n",
    "print(\"=== V2 LOCAL GOLD PROCESSING CONFIGURATION ===\")\n",
    "print(f\"Silver source path: {silver_path}\")\n",
    "print(f\"Gold target path: {gold_path}\")\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(gold_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Silver Layer data from local Delta paths\n",
    "def load_climate_data_with_locations_local():\n",
    "    \"\"\"Load climate and location data from local Silver Delta tables.\"\"\"\n",
    "    print(\"=== DATA LOADING (LOCAL DELTA PATHS) ===\")\n",
    "    \n",
    "    try:\n",
    "        # Define paths to the specific silver tables\n",
    "        climate_table_path = os.path.join(silver_path, 'fact_climate_weather') # Assuming this is the correct folder name\n",
    "        location_table_path = os.path.join(silver_path, 'dim_location')\n",
    "        \n",
    "        print(f'Reading climate data from: {climate_table_path}')\n",
    "        daily_climate = spark.read.format(\"delta\").load(climate_table_path)\n",
    "        print(f'Reading location data from: {location_table_path}')\n",
    "        location_dim = spark.read.format(\"delta\").load(location_table_path)\n",
    "        \n",
    "        print(f\"Climate data loaded: {daily_climate.count():,} records\")\n",
    "        print(f\"Location dimension loaded: {location_dim.count():,} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load Silver layer tables. Please check paths and table names. Error: {e}\")\n",
    "        raise\n",
    "        \n",
    "    # Join climate data with location dimension\n",
    "    climate_with_locations = daily_climate.join(location_dim, \"location_id\", \"left\")\n",
    "    \n",
    "    # Cache for performance\n",
    "    climate_with_locations.cache()\n",
    "    joined_count = climate_with_locations.count()\n",
    "    print(f\"Successfully joined climate and location data: {joined_count:,} records\")\n",
    "    \n",
    "    return climate_with_locations\n",
    "\n",
    "# Execute the loading function\n",
    "climate_data = load_climate_data_with_locations_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Analysis\n",
    "The following cell performs a deep analysis of the loaded data, calculating statistics on geographic coverage, data completeness, and distributions. This step is for validation and does not transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is kept for its valuable data profiling capabilities.\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
    "try:\n",
    "    total_records = climate_data.count()\n",
    "    print(f\"Total records for analysis: {total_records:,}\")\n",
    "    \n",
    "    # Geographic coverage analysis\n",
    "    geo_coverage = climate_data.agg(\n",
    "        countDistinct(\"location_id\").alias(\"unique_locations\"),\n",
    "        countDistinct(\"region_code\").alias(\"unique_regions\"),\n",
    "        countDistinct(\"metric_code\").alias(\"unique_metrics\"),\n",
    "        min(\"measurement_date\").alias(\"earliest_date\"),\n",
    "        max(\"measurement_date\").alias(\"latest_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nGeographic Coverage:\")\n",
    "    print(f\"  Unique locations: {geo_coverage['unique_locations']:,}\")\n",
    "    print(f\"  Unique regions: {geo_coverage['unique_regions']:,}\")\n",
    "    print(f\"  Climate metrics: {geo_coverage['unique_metrics']:,}\")\n",
    "    print(f\"  Date range: {geo_coverage['earliest_date']} to {geo_coverage['latest_date']}\")\n",
    "    \n",
    "    # Climate metrics distribution\n",
    "    print(f\"\\nClimate metrics distribution:\")\n",
    "    climate_data.groupBy(\"metric_code\").agg(count(\"*\").alias(\"record_count\")).orderBy(\"metric_code\").show()\n",
    "    \n",
    "    print(f\"\\nDATA QUALITY: VALIDATED\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data quality analysis: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
