{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Climate Data Quarterly & Annual Aggregations\n",
          "\n",
          "Creates quarterly and annual aggregated views of climate data from the daily fact_climate_weather_v2 table.\n",
          "These aggregations enable efficient time-series analysis and reporting for the lakehouse chatbot.\n",
          "\n",
          "**Quarterly Aggregations**: SUM() for precipitation, AVG() for temperature, pressure, humidity\n",
          "**Annual Aggregations**: SUM() for precipitation, AVG() for temperature, pressure, humidity\n",
          "\n",
          "**Outputs**:\n",
          "- `fact_climate_quarterly` - Quarterly climate metrics (C01, C03, C04, C09, C12, C13)\n",
          "- `fact_climate_annual` - Annual climate metrics (C01, C03, C04, C09, C12, C13)\n",
          "\n",
          "**Climate Metrics**:\n",
          "- C01: Mean air surface temperature (°C)\n",
          "- C03: Highest temperature (°C) \n",
          "- C04: Lowest temperature (°C)\n",
          "- C09: Total precipitation (mm)\n",
          "- C12: Mean surface pressure (Pascals)\n",
          "- C13: Mean humidity level (%)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "import os\n",
          "import warnings\n",
          "warnings.filterwarnings('ignore')\n",
          "\n",
          "from pyspark.sql import SparkSession\n",
          "from pyspark.sql.functions import *\n",
          "from pyspark.sql.types import *\n",
          "from pyspark.sql.window import Window\n",
          "from datetime import datetime\n",
          "import json"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Initialize Spark Session\n",
          "spark = SparkSession.builder \\\n",
          "    .appName(\"ClimateAggregationsProcessor\") \\\n",
          "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
          "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
          "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
          "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
          "    .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") \\\n",
          "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
          "    .getOrCreate()\n",
          "\n",
          "spark.sparkContext.setLogLevel(\"ERROR\")\n",
          "\n",
          "print(f\"Spark Version: {spark.version}\")\n",
          "print(\"Climate Aggregations Processor Started\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Configuration\n",
          "SILVER_PATH = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver\"\n",
          "PROCESSING_TIMESTAMP = datetime.now()\n",
          "\n",
          "print(f\"Silver Path: {SILVER_PATH}\")\n",
          "print(f\"Processing Time: {PROCESSING_TIMESTAMP}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Load Daily Climate Data"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Load the daily climate weather fact table\n",
          "try:\n",
          "    # Try v2 first (latest version)\n",
          "    climate_path = os.path.join(SILVER_PATH, \"fact_climate_weather_v2\")\n",
          "    daily_climate = spark.read.format(\"delta\").load(climate_path)\n",
          "    print(f\"Loaded fact_climate_weather_v2\")\n",
          "except:\n",
          "    try:\n",
          "        # Fallback to original version\n",
          "        climate_path = os.path.join(SILVER_PATH, \"fact_climate_weather\")\n",
          "        daily_climate = spark.read.format(\"delta\").load(climate_path)\n",
          "        print(f\"Loaded fact_climate_weather (fallback)\")\n",
          "    except Exception as e:\n",
          "        print(f\"Error loading climate data: {e}\")\n",
          "        raise\n",
          "\n",
          "print(f\"\\n=== DAILY CLIMATE DATA OVERVIEW ===\")\n",
          "print(f\"Total daily records: {daily_climate.count():,}\")\n",
          "daily_climate.printSchema()\n",
          "\n",
          "# Show data quality summary\n",
          "print(f\"\\nData Quality Summary:\")\n",
          "quality_stats = daily_climate.agg(\n",
          "    min(\"measurement_date\").alias(\"earliest_date\"),\n",
          "    max(\"measurement_date\").alias(\"latest_date\"),\n",
          "    countDistinct(\"location_id\").alias(\"unique_locations\"),\n",
          "    countDistinct(\"metric_code\").alias(\"unique_metrics\"),\n",
          "    avg(\"measurement_value\").alias(\"avg_measurement_value\")\n",
          ").collect()[0]\n",
          "\n",
          "print(f\"   Date range: {quality_stats['earliest_date']} to {quality_stats['latest_date']}\")\n",
          "print(f\"   Unique locations: {quality_stats['unique_locations']:,}\")\n",
          "print(f\"   Unique metrics: {quality_stats['unique_metrics']}\")\n",
          "print(f\"   Average measurement value: {quality_stats['avg_measurement_value']:.2f}\")\n",
          "\n",
          "# Show metric distribution\n",
          "print(f\"\\nMetric Code Distribution:\")\n",
          "daily_climate.groupBy(\"metric_code\", \"climate_metric\").count().orderBy(\"metric_code\").show(truncate=False)\n",
          "\n",
          "# Sample data\n",
          "print(f\"\\nSample Daily Data:\")\n",
          "daily_climate.select(\n",
          "    \"measurement_date\", \"location_id\", \"metric_code\", \"climate_metric\", \n",
          "    \"measurement_value\", \"unit_of_measure\"\n",
          ").show(10, truncate=False)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Create Quarterly Aggregations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create quarterly aggregations\n",
          "print(\"=== CREATING QUARTERLY CLIMATE AGGREGATIONS ===\")\n",
          "\n",
          "# Add quarter and year columns\n",
          "quarterly_base = daily_climate.withColumn(\n",
          "    \"quarter\", \n",
          "    concat(col(\"year\"), lit(\"-Q\"), quarter(col(\"measurement_date\")))\n",
          ").withColumn(\n",
          "    \"quarter_number\", \n",
          "    quarter(col(\"measurement_date\"))\n",
          ").withColumn(\n",
          "    \"quarter_start_date\",\n",
          "    date_trunc(\"quarter\", col(\"measurement_date\"))\n",
          ")\n",
          "\n",
          "# Define aggregation logic based on metric type\n",
          "quarterly_climate = quarterly_base.groupBy(\n",
          "    \"location_id\", \"indicator_id\", \"year\", \"quarter\", \"quarter_number\", \n",
          "    \"quarter_start_date\", \"metric_code\", \"climate_metric\", \"unit_of_measure\"\n",
          ").agg(\n",
          "    # Use SUM for precipitation (C09), AVG for others\n",
          "    when(col(\"metric_code\") == \"C09\", sum(\"measurement_value\"))\n",
          "    .otherwise(avg(\"measurement_value\")).alias(\"quarterly_value\"),\n",
          "    \n",
          "    # Additional statistics\n",
          "    min(\"measurement_value\").alias(\"min_daily_value\"),\n",
          "    max(\"measurement_value\").alias(\"max_daily_value\"),\n",
          "    stddev(\"measurement_value\").alias(\"stddev_daily_value\"),\n",
          "    count(\"measurement_value\").alias(\"daily_records_count\"),\n",
          "    \n",
          "    # Date range\n",
          "    min(\"measurement_date\").alias(\"period_start_date\"),\n",
          "    max(\"measurement_date\").alias(\"period_end_date\")\n",
          ")\n",
          "\n",
          "# Add aggregation type and computed columns\n",
          "quarterly_climate = quarterly_climate.withColumn(\n",
          "    \"aggregation_type\",\n",
          "    when(col(\"metric_code\") == \"C09\", lit(\"SUM\")).otherwise(lit(\"AVG\"))\n",
          ").withColumn(\n",
          "    \"quarterly_climate_id\", \n",
          "    row_number().over(Window.orderBy(\"location_id\", \"year\", \"quarter_number\", \"metric_code\"))\n",
          ").withColumn(\n",
          "    \"created_at\", lit(PROCESSING_TIMESTAMP)\n",
          ").withColumn(\n",
          "    \"updated_at\", lit(PROCESSING_TIMESTAMP)\n",
          ")\n",
          "\n",
          "# Add specific metric columns for easier querying\n",
          "quarterly_climate = quarterly_climate.withColumn(\n",
          "    \"temperature_celsius\",\n",
          "    when(col(\"unit_of_measure\") == \"Celsius\", col(\"quarterly_value\")).otherwise(lit(None))\n",
          ").withColumn(\n",
          "    \"precipitation_mm\",\n",
          "    when(col(\"unit_of_measure\") == \"Millimeters\", col(\"quarterly_value\")).otherwise(lit(None))\n",
          ").withColumn(\n",
          "    \"pressure_pascals\",\n",
          "    when(col(\"unit_of_measure\") == \"Pascals\", col(\"quarterly_value\")).otherwise(lit(None))\n",
          ").withColumn(\n",
          "    \"humidity_percentage\",\n",
          "    when(col(\"unit_of_measure\") == \"Percentage\", col(\"quarterly_value\")).otherwise(lit(None))\n",
          ")\n",
          "\n",
          "# Select final quarterly columns\n",
          "quarterly_climate = quarterly_climate.select(\n",
          "    \"quarterly_climate_id\", \"location_id\", \"indicator_id\", \"year\", \"quarter\", \n",
          "    \"quarter_number\", \"quarter_start_date\", \"period_start_date\", \"period_end_date\",\n",
          "    \"metric_code\", \"climate_metric\", \"quarterly_value\", \"aggregation_type\",\n",
          "    \"temperature_celsius\", \"precipitation_mm\", \"pressure_pascals\", \"humidity_percentage\",\n",
          "    \"unit_of_measure\", \"min_daily_value\", \"max_daily_value\", \"stddev_daily_value\",\n",
          "    \"daily_records_count\", \"created_at\", \"updated_at\"\n",
          ")\n",
          "\n",
          "print(f\"Quarterly aggregations created: {quarterly_climate.count():,} records\")\n",
          "quarterly_climate.printSchema()\n",
          "\n",
          "# Show quarterly summary\n",
          "print(f\"\\nQuarterly Aggregation Summary:\")\n",
          "quarterly_summary = quarterly_climate.agg(\n",
          "    countDistinct(\"location_id\").alias(\"unique_locations\"),\n",
          "    countDistinct(\"quarter\").alias(\"unique_quarters\"),\n",
          "    countDistinct(\"metric_code\").alias(\"unique_metrics\"),\n",
          "    min(\"period_start_date\").alias(\"earliest_quarter\"),\n",
          "    max(\"period_end_date\").alias(\"latest_quarter\")\n",
          ").collect()[0]\n",
          "\n",
          "print(f\"   Locations: {quarterly_summary['unique_locations']:,}\")\n",
          "print(f\"   Quarters: {quarterly_summary['unique_quarters']:,}\")\n",
          "print(f\"   Metrics: {quarterly_summary['unique_metrics']}\")\n",
          "print(f\"   Time range: {quarterly_summary['earliest_quarter']} to {quarterly_summary['latest_quarter']}\")\n",
          "\n",
          "# Show sample quarterly data\n",
          "print(f\"\\nSample Quarterly Data by Metric:\")\n",
          "for metric in [\"C01\", \"C09\", \"C12\"]:\n",
          "    print(f\"\\n{metric} Quarterly Sample:\")\n",
          "    quarterly_climate.filter(col(\"metric_code\") == metric).select(\n",
          "        \"quarter\", \"location_id\", \"climate_metric\", \"quarterly_value\", \n",
          "        \"aggregation_type\", \"daily_records_count\"\n",
          "    ).limit(3).show(truncate=False)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Create Annual Aggregations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create annual aggregations\n",
          "print(\"=== CREATING ANNUAL CLIMATE AGGREGATIONS ===\")\n",
          "\n",
          "# Add year start date\n",
          "annual_base = daily_climate.withColumn(\n",
          "    \"year_start_date\",\n",
          "    date_trunc(\"year\", col(\"measurement_date\"))\n",
          ")\n",
          "\n",
          "# Define aggregation logic based on metric type\n",
          "annual_climate = annual_base.groupBy(\n",
          "    \"location_id\", \"indicator_id\", \"year\", \"year_start_date\", \n",
          "    \"metric_code\", \"climate_metric\", \"unit_of_measure\"\n",
          ").agg(\n",
          "    # Use SUM for precipitation (C09), AVG for others\n",
          "    when(col(\"metric_code\") == \"C09\", sum(\"measurement_value\"))\n",
          "    .otherwise(avg(\"measurement_value\")).alias(\"annual_value\"),\n",
          "    \n",
          "    # Additional statistics\n",
          "    min(\"measurement_value\").alias(\"min_daily_value\"),\n",
          "    max(\"measurement_value\").alias(\"max_daily_value\"),\n",
          "    stddev(\"measurement_value\").alias(\"stddev_daily_value\"),\n",
          "    count(\"measurement_value\").alias(\"daily_records_count\"),\n",
          "    \n",
          "    # Date range\n",
          "    min(\"measurement_date\").alias(\"period_start_date\"),\n",
          "    max(\"measurement_date\").alias(\"period_end_date\")\n",
          ")\n",
          "\n",
          "# Add aggregation type and computed columns\n",
          "annual_climate = annual_climate.withColumn(\n",
          "    \"aggregation_type\",\n",
          "    when(col(\"metric_code\") == \"C09\", lit(\"SUM\")).otherwise(lit(\"AVG\"))\n",
          ").withColumn(\n",
          "    \"annual_climate_id\", \n",
          "    row_number().over(Window.orderBy(\"location_id\", \"year\", \"metric_code\"))\n",
          ").withColumn(\n",
          "    \"created_at\", lit(PROCESSING_TIMESTAMP)\n",
          ").withColumn(\n",
          "    \"updated_at\", lit(PROCESSING_TIMESTAMP)\n",
          ")\n",
          "\n",
          "# Add specific metric columns for easier querying\n",
          "annual_climate = annual_climate.withColumn(\n",
          "    \"temperature_celsius\",\n",
          "    when(col(\"unit_of_measure\") == \"Celsius\", col(\"annual_value\")).otherwise(lit(None))\n",
          ").withColumn(\n",
          "    \"precipitation_mm\",\n",
          "    when(col(\"unit_of_measure\") == \"Millimeters\", col(\"annual_value\")).otherwise(lit(None))\n",
          ").withColumn(\n",
          "    \"pressure_pascals\",\n",
          "    when(col(\"unit_of_measure\") == \"Pascals\", col(\"annual_value\")).otherwise(lit(None))\n",
          ").withColumn(\n",
          "    \"humidity_percentage\",\n",
          "    when(col(\"unit_of_measure\") == \"Percentage\", col(\"annual_value\")).otherwise(lit(None))\n",
          ")\n",
          "\n",
          "# Select final annual columns\n",
          "annual_climate = annual_climate.select(\n",
          "    \"annual_climate_id\", \"location_id\", \"indicator_id\", \"year\", \n",
          "    \"year_start_date\", \"period_start_date\", \"period_end_date\",\n",
          "    \"metric_code\", \"climate_metric\", \"annual_value\", \"aggregation_type\",\n",
          "    \"temperature_celsius\", \"precipitation_mm\", \"pressure_pascals\", \"humidity_percentage\",\n",
          "    \"unit_of_measure\", \"min_daily_value\", \"max_daily_value\", \"stddev_daily_value\",\n",
          "    \"daily_records_count\", \"created_at\", \"updated_at\"\n",
          ")\n",
          "\n",
          "print(f\"Annual aggregations created: {annual_climate.count():,} records\")\n",
          "annual_climate.printSchema()\n",
          "\n",
          "# Show annual summary\n",
          "print(f\"\\nAnnual Aggregation Summary:\")\n",
          "annual_summary = annual_climate.agg(\n",
          "    countDistinct(\"location_id\").alias(\"unique_locations\"),\n",
          "    countDistinct(\"year\").alias(\"unique_years\"),\n",
          "    countDistinct(\"metric_code\").alias(\"unique_metrics\"),\n",
          "    min(\"period_start_date\").alias(\"earliest_year\"),\n",
          "    max(\"period_end_date\").alias(\"latest_year\")\n",
          ").collect()[0]\n",
          "\n",
          "print(f\"   Locations: {annual_summary['unique_locations']:,}\")\n",
          "print(f\"   Years: {annual_summary['unique_years']:,}\")\n",
          "print(f\"   Metrics: {annual_summary['unique_metrics']}\")\n",
          "print(f\"   Time range: {annual_summary['earliest_year']} to {annual_summary['latest_year']}\")\n",
          "\n",
          "# Show sample annual data\n",
          "print(f\"\\nSample Annual Data by Metric:\")\n",
          "for metric in [\"C01\", \"C09\", \"C12\"]:\n",
          "    print(f\"\\n{metric} Annual Sample:\")\n",
          "    annual_climate.filter(col(\"metric_code\") == metric).select(\n",
          "        \"year\", \"location_id\", \"climate_metric\", \"annual_value\", \n",
          "        \"aggregation_type\", \"daily_records_count\"\n",
          "    ).limit(3).show(truncate=False)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Data Quality Validation"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Comprehensive data quality validation\n",
          "print(\"=== DATA QUALITY VALIDATION ===\")\n",
          "\n",
          "# Validate quarterly aggregations\n",
          "print(f\"\\n1. Quarterly Aggregations Validation:\")\n",
          "\n",
          "quarterly_validation = quarterly_climate.agg(\n",
          "    avg(\"quarterly_value\").alias(\"avg_quarterly_value\"),\n",
          "    min(\"quarterly_value\").alias(\"min_quarterly_value\"),\n",
          "    max(\"quarterly_value\").alias(\"max_quarterly_value\"),\n",
          "    sum(when(col(\"quarterly_value\") == 0.0, 1).otherwise(0)).alias(\"zero_values\"),\n",
          "    sum(when(col(\"quarterly_value\").isNull(), 1).otherwise(0)).alias(\"null_values\")\n",
          ").collect()[0]\n",
          "\n",
          "print(f\"   Average quarterly value: {quarterly_validation['avg_quarterly_value']:.2f}\")\n",
          "print(f\"   Value range: {quarterly_validation['min_quarterly_value']:.2f} to {quarterly_validation['max_quarterly_value']:.2f}\")\n",
          "print(f\"   Zero values: {quarterly_validation['zero_values']:,}\")\n",
          "print(f\"   Null values: {quarterly_validation['null_values']:,}\")\n",
          "\n",
          "# Validate aggregation consistency\n",
          "print(f\"\\n   Quarterly Aggregation Type Distribution:\")\n",
          "quarterly_climate.groupBy(\"metric_code\", \"aggregation_type\").count().orderBy(\"metric_code\").show()\n",
          "\n",
          "# Validate annual aggregations\n",
          "print(f\"\\n2. Annual Aggregations Validation:\")\n",
          "\n",
          "annual_validation = annual_climate.agg(\n",
          "    avg(\"annual_value\").alias(\"avg_annual_value\"),\n",
          "    min(\"annual_value\").alias(\"min_annual_value\"),\n",
          "    max(\"annual_value\").alias(\"max_annual_value\"),\n",
          "    sum(when(col(\"annual_value\") == 0.0, 1).otherwise(0)).alias(\"zero_values\"),\n",
          "    sum(when(col(\"annual_value\").isNull(), 1).otherwise(0)).alias(\"null_values\")\n",
          ").collect()[0]\n",
          "\n",
          "print(f\"   Average annual value: {annual_validation['avg_annual_value']:.2f}\")\n",
          "print(f\"   Value range: {annual_validation['min_annual_value']:.2f} to {annual_validation['max_annual_value']:.2f}\")\n",
          "print(f\"   Zero values: {annual_validation['zero_values']:,}\")\n",
          "print(f\"   Null values: {annual_validation['null_values']:,}\")\n",
          "\n",
          "print(f\"\\n   Annual Aggregation Type Distribution:\")\n",
          "annual_climate.groupBy(\"metric_code\", \"aggregation_type\").count().orderBy(\"metric_code\").show()\n",
          "\n",
          "# Cross-validation: Check that precipitation uses SUM, others use AVG\n",
          "print(f\"\\n3. Aggregation Logic Validation:\")\n",
          "\n",
          "precip_quarterly = quarterly_climate.filter(col(\"metric_code\") == \"C09\")\n",
          "temp_quarterly = quarterly_climate.filter(col(\"metric_code\") == \"C01\")\n",
          "\n",
          "precip_agg_type = precip_quarterly.select(\"aggregation_type\").distinct().collect()\n",
          "temp_agg_type = temp_quarterly.select(\"aggregation_type\").distinct().collect()\n",
          "\n",
          "print(f\"   Precipitation (C09) uses: {[row.aggregation_type for row in precip_agg_type]}\")\n",
          "print(f\"   Temperature (C01) uses: {[row.aggregation_type for row in temp_agg_type]}\")\n",
          "\n",
          "# Validate record completeness\n",
          "expected_quarterly_records = quarterly_climate.groupBy(\"location_id\", \"year\").agg(\n",
          "    countDistinct(\"quarter_number\").alias(\"quarters_per_year\")\n",
          ").agg(\n",
          "    min(\"quarters_per_year\").alias(\"min_quarters\"),\n",
          "    max(\"quarters_per_year\").alias(\"max_quarters\"),\n",
          "    avg(\"quarters_per_year\").alias(\"avg_quarters\")\n",
          ").collect()[0]\n",
          "\n",
          "print(f\"\\n4. Completeness Validation:\")\n",
          "print(f\"   Quarters per location/year - Min: {expected_quarterly_records['min_quarters']}, Max: {expected_quarterly_records['max_quarters']}, Avg: {expected_quarterly_records['avg_quarters']:.1f}\")\n",
          "\n",
          "if quarterly_validation['avg_quarterly_value'] == 0.0:\n",
          "    print(f\"\\nWARNING: Quarterly data appears corrupted (all zeros)\")\n",
          "else:\n",
          "    print(f\"\\nData quality validation PASSED - aggregations look healthy\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Save Quarterly Aggregations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save quarterly climate aggregations\n",
          "quarterly_output_path = os.path.join(SILVER_PATH, \"fact_climate_quarterly\")\n",
          "\n",
          "print(f\"=== SAVING QUARTERLY CLIMATE AGGREGATIONS ===\")\n",
          "print(f\"Output path: {quarterly_output_path}\")\n",
          "\n",
          "try:\n",
          "    # Cache for performance\n",
          "    quarterly_climate.cache()\n",
          "    \n",
          "    # Get final count for validation\n",
          "    quarterly_count = quarterly_climate.count()\n",
          "    print(f\"Quarterly records to save: {quarterly_count:,}\")\n",
          "    \n",
          "    # Save as Delta table with partitioning\n",
          "    quarterly_climate.write \\\n",
          "        .format(\"delta\") \\\n",
          "        .mode(\"overwrite\") \\\n",
          "        .option(\"overwriteSchema\", \"true\") \\\n",
          "        .option(\"dataChange\", \"true\") \\\n",
          "        .partitionBy(\"year\", \"metric_code\") \\\n",
          "        .save(quarterly_output_path)\n",
          "    \n",
          "    print(f\"SUCCESS: Quarterly aggregations saved to {quarterly_output_path}\")\n",
          "    \n",
          "    # Post-save validation\n",
          "    saved_quarterly = spark.read.format(\"delta\").load(quarterly_output_path)\n",
          "    saved_quarterly_count = saved_quarterly.count()\n",
          "    \n",
          "    print(f\"\\nPost-save validation:\")\n",
          "    print(f\"   Expected records: {quarterly_count:,}\")\n",
          "    print(f\"   Saved records: {saved_quarterly_count:,}\")\n",
          "    print(f\"   Validation: {'PASSED' if saved_quarterly_count == quarterly_count else 'FAILED'}\")\n",
          "    \n",
          "    # Show partition structure\n",
          "    print(f\"\\nPartition structure sample:\")\n",
          "    saved_quarterly.select(\"year\", \"metric_code\").distinct().orderBy(\"year\", \"metric_code\").show(10)\n",
          "    \n",
          "except Exception as e:\n",
          "    print(f\"ERROR saving quarterly aggregations: {e}\")\n",
          "    import traceback\n",
          "    traceback.print_exc()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Save Annual Aggregations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save annual climate aggregations\n",
          "annual_output_path = os.path.join(SILVER_PATH, \"fact_climate_annual\")\n",
          "\n",
          "print(f\"=== SAVING ANNUAL CLIMATE AGGREGATIONS ===\")\n",
          "print(f\"Output path: {annual_output_path}\")\n",
          "\n",
          "try:\n",
          "    # Cache for performance\n",
          "    annual_climate.cache()\n",
          "    \n",
          "    # Get final count for validation\n",
          "    annual_count = annual_climate.count()\n",
          "    print(f\"Annual records to save: {annual_count:,}\")\n",
          "    \n",
          "    # Save as Delta table with partitioning\n",
          "    annual_climate.write \\\n",
          "        .format(\"delta\") \\\n",
          "        .mode(\"overwrite\") \\\n",
          "        .option(\"overwriteSchema\", \"true\") \\\n",
          "        .option(\"dataChange\", \"true\") \\\n",
          "        .partitionBy(\"year\", \"metric_code\") \\\n",
          "        .save(annual_output_path)\n",
          "    \n",
          "    print(f\"SUCCESS: Annual aggregations saved to {annual_output_path}\")\n",
          "    \n",
          "    # Post-save validation\n",
          "    saved_annual = spark.read.format(\"delta\").load(annual_output_path)\n",
          "    saved_annual_count = saved_annual.count()\n",
          "    \n",
          "    print(f\"\\nPost-save validation:\")\n",
          "    print(f\"   Expected records: {annual_count:,}\")\n",
          "    print(f\"   Saved records: {saved_annual_count:,}\")\n",
          "    print(f\"   Validation: {'PASSED' if saved_annual_count == annual_count else 'FAILED'}\")\n",
          "    \n",
          "    # Show partition structure\n",
          "    print(f\"\\nPartition structure sample:\")\n",
          "    saved_annual.select(\"year\", \"metric_code\").distinct().orderBy(\"year\", \"metric_code\").show(10)\n",
          "    \n",
          "except Exception as e:\n",
          "    print(f\"ERROR saving annual aggregations: {e}\")\n",
          "    import traceback\n",
          "    traceback.print_exc()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Create Summary Views for Chatbot Queries"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create summary views optimized for chatbot queries\n",
          "print(\"=== CREATING CHATBOT-OPTIMIZED SUMMARY VIEWS ===\")\n",
          "\n",
          "# Load dimension tables for enhanced summaries\n",
          "try:\n",
          "    dim_location = spark.read.format(\"delta\").load(os.path.join(SILVER_PATH, \"dim_location_fixed\"))\n",
          "    print(\"Loaded dim_location_fixed for enhanced summaries\")\n",
          "except:\n",
          "    dim_location = spark.read.format(\"delta\").load(os.path.join(SILVER_PATH, \"dim_location\"))\n",
          "    print(\"Loaded dim_location (fallback)\")\n",
          "\n",
          "# Create quarterly summary with location names\n",
          "quarterly_summary_view = quarterly_climate.join(\n",
          "    dim_location.select(\"location_id\", \"location_name\", \"location_type\"), \n",
          "    \"location_id\", \"left\"\n",
          ").select(\n",
          "    \"quarterly_climate_id\", \"location_id\", \"location_name\", \"location_type\",\n",
          "    \"year\", \"quarter\", \"quarter_number\", \"metric_code\", \"climate_metric\",\n",
          "    \"quarterly_value\", \"aggregation_type\", \"temperature_celsius\", \n",
          "    \"precipitation_mm\", \"pressure_pascals\", \"humidity_percentage\",\n",
          "    \"unit_of_measure\", \"daily_records_count\", \"period_start_date\", \"period_end_date\"\n",
          ")\n",
          "\n",
          "# Create annual summary with location names\n",
          "annual_summary_view = annual_climate.join(\n",
          "    dim_location.select(\"location_id\", \"location_name\", \"location_type\"), \n",
          "    \"location_id\", \"left\"\n",
          ").select(\n",
          "    \"annual_climate_id\", \"location_id\", \"location_name\", \"location_type\",\n",
          "    \"year\", \"metric_code\", \"climate_metric\", \"annual_value\", \"aggregation_type\",\n",
          "    \"temperature_celsius\", \"precipitation_mm\", \"pressure_pascals\", \"humidity_percentage\",\n",
          "    \"unit_of_measure\", \"daily_records_count\", \"period_start_date\", \"period_end_date\"\n",
          ")\n",
          "\n",
          "print(f\"\\nQuarterly summary view: {quarterly_summary_view.count():,} records\")\n",
          "print(f\"Annual summary view: {annual_summary_view.count():,} records\")\n",
          "\n",
          "# Show sample enhanced data\n",
          "print(f\"\\nSample Enhanced Quarterly Data:\")\n",
          "quarterly_summary_view.filter(col(\"metric_code\") == \"C01\").select(\n",
          "    \"location_name\", \"quarter\", \"climate_metric\", \"quarterly_value\", \"temperature_celsius\"\n",
          ").limit(5).show(truncate=False)\n",
          "\n",
          "print(f\"\\nSample Enhanced Annual Data:\")\n",
          "annual_summary_view.filter(col(\"metric_code\") == \"C09\").select(\n",
          "    \"location_name\", \"year\", \"climate_metric\", \"annual_value\", \"precipitation_mm\"\n",
          ").limit(5).show(truncate=False)\n",
          "\n",
          "# Generate metric definitions for chatbot\n",
          "metric_definitions = {\n",
          "    \"C01_quarterly\": \"Quarterly mean air surface temperature (°C) - Average of daily temperatures\",\n",
          "    \"C03_quarterly\": \"Quarterly highest temperature (°C) - Average of daily maximum temperatures\", \n",
          "    \"C04_quarterly\": \"Quarterly lowest temperature (°C) - Average of daily minimum temperatures\",\n",
          "    \"C09_quarterly\": \"Quarterly total precipitation (mm) - Sum of daily precipitation\",\n",
          "    \"C12_quarterly\": \"Quarterly mean surface pressure (Pascals) - Average of daily pressure readings\",\n",
          "    \"C13_quarterly\": \"Quarterly mean humidity level (%) - Average of daily humidity readings\",\n",
          "    \"C01_annual\": \"Annual mean air surface temperature (°C) - Average of daily temperatures\",\n",
          "    \"C03_annual\": \"Annual highest temperature (°C) - Average of daily maximum temperatures\",\n",
          "    \"C04_annual\": \"Annual lowest temperature (°C) - Average of daily minimum temperatures\",\n",
          "    \"C09_annual\": \"Annual total precipitation (mm) - Sum of daily precipitation\",\n",
          "    \"C12_annual\": \"Annual mean surface pressure (Pascals) - Average of daily pressure readings\",\n",
          "    \"C13_annual\": \"Annual mean humidity level (%) - Average of daily humidity readings\"\n",
          "}\n",
          "\n",
          "print(f\"\\nMetric Definitions for Chatbot Integration:\")\n",
          "for metric, definition in metric_definitions.items():\n",
          "    print(f\"   {metric}: {definition}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Performance Optimization and Final Summary"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Final summary and performance optimization\n",
          "print(\"=\" * 70)\n",
          "print(\"CLIMATE AGGREGATIONS - PROCESSING SUMMARY\")\n",
          "print(\"=\" * 70)\n",
          "\n",
          "processing_end = datetime.now()\n",
          "processing_duration = processing_end - PROCESSING_TIMESTAMP\n",
          "\n",
          "print(f\"\\nProcessing Details:\")\n",
          "print(f\"   Start time: {PROCESSING_TIMESTAMP}\")\n",
          "print(f\"   End time: {processing_end}\")\n",
          "print(f\"   Duration: {processing_duration}\")\n",
          "\n",
          "# Final statistics\n",
          "if 'daily_climate' in locals() and 'quarterly_climate' in locals() and 'annual_climate' in locals():\n",
          "    daily_count = daily_climate.count()\n",
          "    quarterly_count = quarterly_climate.count() \n",
          "    annual_count = annual_climate.count()\n",
          "    \n",
          "    print(f\"\\nData Processing Results:\")\n",
          "    print(f\"   Daily records processed: {daily_count:,}\")\n",
          "    print(f\"   Quarterly records created: {quarterly_count:,}\")\n",
          "    print(f\"   Annual records created: {annual_count:,}\")\n",
          "    print(f\"   Data reduction ratio: {daily_count/quarterly_count:.1f}:1 (daily to quarterly)\")\n",
          "    print(f\"   Data reduction ratio: {daily_count/annual_count:.1f}:1 (daily to annual)\")\n",
          "\n",
          "print(f\"\\nCreated Climate Aggregation Tables:\")\n",
          "print(f\"   fact_climate_quarterly - Partitioned by year, metric_code\")\n",
          "print(f\"   fact_climate_annual - Partitioned by year, metric_code\")\n",
          "\n",
          "print(f\"\\nAggregation Logic Applied:\")\n",
          "print(f\"   Temperature metrics (C01, C03, C04): AVG() of daily values\")\n",
          "print(f\"   Precipitation (C09): SUM() of daily values\")\n",
          "print(f\"   Pressure (C12): AVG() of daily values\")\n",
          "print(f\"   Humidity (C13): AVG() of daily values\")\n",
          "\n",
          "print(f\"\\nLakehouse Chatbot Benefits:\")\n",
          "print(f\"   Fast quarterly and annual trend analysis\")\n",
          "print(f\"   Efficient time-series queries across multiple years\")\n",
          "print(f\"   Pre-computed aggregations for dashboard performance\")\n",
          "print(f\"   Location-aware climate summaries\")\n",
          "print(f\"   Comprehensive statistical metrics (min, max, stddev)\")\n",
          "print(f\"   Metric-specific columns for easy filtering\")\n",
          "\n",
          "print(f\"\\nOptimal Query Patterns for Chatbot:\")\n",
          "print(f\"   • 'Show quarterly temperature trends in Manila' → fact_climate_quarterly\")\n",
          "print(f\"   • 'Annual rainfall comparison across regions' → fact_climate_annual\")\n",
          "print(f\"   • 'Climate patterns over the last 5 years' → Both tables\")\n",
          "print(f\"   • 'Seasonal analysis by quarter' → fact_climate_quarterly\")\n",
          "\n",
          "print(f\"\\nNext Integration Steps:\")\n",
          "print(f\"   1. Register tables in chatbot metadata catalog\")\n",
          "print(f\"   2. Create time-series visualization templates\")\n",
          "print(f\"   3. Add aggregation tables to query optimizer\")\n",
          "print(f\"   4. Implement intelligent query routing (daily vs quarterly vs annual)\")\n",
          "\n",
          "print(f\"\\n\" + \"=\" * 70)\n",
          "print(\"CLIMATE AGGREGATIONS PROCESSING COMPLETE\")\n",
          "print(\"=\" * 70)\n",
          "\n",
          "# Cleanup\n",
          "if 'quarterly_climate' in locals():\n",
          "    quarterly_climate.unpersist()\n",
          "if 'annual_climate' in locals():\n",
          "    annual_climate.unpersist()\n",
          "\n",
          "# Stop Spark\n",
          "spark.stop()\n",
          "print(\"\\nSpark session stopped.\")"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8.0"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }