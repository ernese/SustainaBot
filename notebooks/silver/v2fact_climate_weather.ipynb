{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "import re\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/28 17:31:43 WARN Utils: Your hostname, 3rnese resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
            "25/08/28 17:31:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/home/ernese/miniconda3/envs/SO/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /home/ernese/.ivy2/cache\n",
            "The jars for the packages stored in: /home/ernese/.ivy2/jars\n",
            "io.delta#delta-core_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a069162c-1947-4c9c-8eda-17450cf9af24;1.0\n",
            "\tconfs: [default]\n",
            "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
            "\tfound io.delta#delta-storage;2.4.0 in central\n",
            "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
            ":: resolution report :: resolve 200ms :: artifacts dl 4ms\n",
            "\t:: modules in use:\n",
            "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
            "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
            "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-a069162c-1947-4c9c-8eda-17450cf9af24\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
            "25/08/28 17:31:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CLIMATE WEATHER FACT PROCESSOR V2 ===\n",
            "Spark Version: 3.4.0\n",
            "Application: ClimateWeatherFactProcessorV2\n",
            "Anti-corruption optimizations enabled\n",
            "Processing timestamp: 2025-08-28 17:31:46.885000\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session with optimizations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClimateWeatherFactProcessorV2\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
        "    .config(\"spark.sql.adaptive.maxNumPostShufflePartitions\", \"100\") \\\n",
        "    .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") \\\n",
        "    .config(\"spark.databricks.delta.autoCompact.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "print(f\"=== CLIMATE WEATHER FACT PROCESSOR V2 ===\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application: {spark.sparkContext.appName}\")\n",
        "print(f\"Anti-corruption optimizations enabled\")\n",
        "print(f\"Processing timestamp: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CONFIGURATION ===\n",
            "Bronze Path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/new_bronze\n",
            "Silver Path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver\n",
            "Processing Time: 2025-08-28 17:32:30.369048\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BRONZE_PATH = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/new_bronze\"\n",
        "SILVER_PATH = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver\"\n",
        "PROCESSING_TIMESTAMP = datetime.now()\n",
        "\n",
        "# Ensure target directory exists\n",
        "os.makedirs(SILVER_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"=== CONFIGURATION ===\")\n",
        "print(f\"Bronze Path: {BRONZE_PATH}\")\n",
        "print(f\"Silver Path: {SILVER_PATH}\")\n",
        "print(f\"Processing Time: {PROCESSING_TIMESTAMP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dimension Tables with Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension tables loaded and cached:\n",
            "  - dim_location_v2: 50 records\n",
            "  - dim_time: 612 records\n",
            "  - dim_indicator: 15 records\n",
            "\n",
            "Dimension tables ready for broadcast joins\n"
          ]
        }
      ],
      "source": [
        "# Load dimension tables with validation and caching\n",
        "try:\n",
        "    # Load dimensions - using dim_location_v2 since that's what you have\n",
        "    dim_location = spark.read.format(\"delta\").load(os.path.join(SILVER_PATH, \"dim_location_v2\")).cache()\n",
        "    dim_time = spark.read.format(\"delta\").load(os.path.join(SILVER_PATH, \"dim_time\")).cache()\n",
        "    dim_indicator = spark.read.format(\"delta\").load(os.path.join(SILVER_PATH, \"dim_indicator\")).cache()\n",
        "    \n",
        "    # Force caching and validate\n",
        "    location_count = dim_location.count()\n",
        "    time_count = dim_time.count()\n",
        "    indicator_count = dim_indicator.count()\n",
        "    \n",
        "    print(f\"Dimension tables loaded and cached:\")\n",
        "    print(f\"  - dim_location_v2: {location_count:,} records\")\n",
        "    print(f\"  - dim_time: {time_count:,} records\")\n",
        "    print(f\"  - dim_indicator: {indicator_count:,} records\")\n",
        "    \n",
        "    # Create broadcast variables for efficient joins\n",
        "    location_broadcast = broadcast(dim_location.select(\"location_id\", \"location_name\"))\n",
        "    time_broadcast = broadcast(dim_time.select(\"date_id\", col(\"date_value\").alias(\"measurement_date\")))\n",
        "    \n",
        "    print(\"\\nDimension tables ready for broadcast joins\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR loading dimensions: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify NASA Climate Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found: C01 - Mean Air Surface Temperature (Celsius)\n",
            "Found: C03 - Daily Highest Temperature (Celsius)\n",
            "Found: C04 - Daily Lowest Temperature (Celsius)\n",
            "Found: C09 - Daily Precipitation (Millimeters)\n",
            "Found: C12 - Mean Surface Pressure (Pascals)\n",
            "Found: C13 - Humidity Level (Percentage)\n",
            "Found: C23 - Monthly Surface Air Temperature (Celsius)\n",
            "\n",
            "=== DISCOVERY COMPLETE ===\n",
            "Total NASA climate sources found: 7\n"
          ]
        }
      ],
      "source": [
        "def get_nasa_climate_sources():\n",
        "    \"\"\"Identify all NASA climate data sources with metadata\"\"\"\n",
        "    climate_sources = []\n",
        "    \n",
        "    # NASA Climate data sources mapping\n",
        "    nasa_datasets = {\n",
        "        'bronze_nasa_c01_v2': {\n",
        "            'metric': 'Mean Air Surface Temperature',\n",
        "            'unit': 'Celsius',\n",
        "            'frequency': 'Daily',\n",
        "            'code': 'C01'\n",
        "        },\n",
        "        'bronze_nasa_c03_v2': {\n",
        "            'metric': 'Daily Highest Temperature',\n",
        "            'unit': 'Celsius',\n",
        "            'frequency': 'Daily',\n",
        "            'code': 'C03'\n",
        "        },\n",
        "        'bronze_nasa_c04_v2': {\n",
        "            'metric': 'Daily Lowest Temperature', \n",
        "            'unit': 'Celsius',\n",
        "            'frequency': 'Daily',\n",
        "            'code': 'C04'\n",
        "        },\n",
        "        'bronze_nasa_c09_v2': {\n",
        "            'metric': 'Daily Precipitation',\n",
        "            'unit': 'Millimeters',\n",
        "            'frequency': 'Daily',\n",
        "            'code': 'C09'\n",
        "        },\n",
        "        'bronze_nasa_c12_v2': {\n",
        "            'metric': 'Mean Surface Pressure',\n",
        "            'unit': 'Pascals',\n",
        "            'frequency': 'Daily',\n",
        "            'code': 'C12'\n",
        "        },\n",
        "        'bronze_nasa_c13_v2': {\n",
        "            'metric': 'Humidity Level',\n",
        "            'unit': 'Percentage',\n",
        "            'frequency': 'Daily',\n",
        "            'code': 'C13'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Check which datasets exist\n",
        "    for dataset_name, metadata in nasa_datasets.items():\n",
        "        dataset_path = os.path.join(BRONZE_PATH, dataset_name)\n",
        "        if os.path.exists(dataset_path):\n",
        "            climate_sources.append({\n",
        "                'path': dataset_path,\n",
        "                'source': 'NASA',\n",
        "                'dataset_code': metadata['code'],\n",
        "                'dataset_name': dataset_name,\n",
        "                'metric': metadata['metric'],\n",
        "                'unit': metadata['unit'],\n",
        "                'frequency': metadata['frequency'],\n",
        "                'data_type': 'climate'\n",
        "            })\n",
        "            print(f\"Found: {metadata['code']} - {metadata['metric']} ({metadata['unit']})\")\n",
        "        else:\n",
        "            print(f\"Missing: {metadata['code']} - {dataset_path}\")\n",
        "    \n",
        "    # Also check for C23 in the separate folder\n",
        "    c23_path = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze_nasa_c23_v2\"\n",
        "    if os.path.exists(c23_path):\n",
        "        climate_sources.append({\n",
        "            'path': c23_path,\n",
        "            'source': 'NASA',\n",
        "            'dataset_code': 'C23',\n",
        "            'dataset_name': 'bronze_nasa_c23_v2',\n",
        "            'metric': 'Monthly Surface Air Temperature',\n",
        "            'unit': 'Celsius',\n",
        "            'frequency': 'Monthly',\n",
        "            'data_type': 'climate'\n",
        "        })\n",
        "        print(f\"Found: C23 - Monthly Surface Air Temperature (Celsius)\")\n",
        "    \n",
        "    return climate_sources\n",
        "\n",
        "# Execute discovery\n",
        "climate_sources = get_nasa_climate_sources()\n",
        "print(f\"\\n=== DISCOVERY COMPLETE ===\")\n",
        "print(f\"Total NASA climate sources found: {len(climate_sources)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== STARTING CLIMATE DATA PROCESSING ===\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 1/7: C01\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C01 - Mean Air Surface Temperature ---\n",
            "  Source rows: 16,262\n",
            "  Columns detected: 144\n",
            "  Location columns: 132\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 2,146,584\n",
            "  Value statistics:\n",
            "    Range: 13.37 to 33.16\n",
            "    Average: 26.00\n",
            "    Zero values: 0\n",
            "  Final records: 2,146,584\n",
            "  Final average: 26.00\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C01 processed\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 2/7: C03\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C03 - Daily Highest Temperature ---\n",
            "  Source rows: 16,262\n",
            "  Columns detected: 144\n",
            "  Location columns: 132\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 2,146,584\n",
            "  Value statistics:\n",
            "    Range: 15.68 to 42.20\n",
            "    Average: 29.18\n",
            "    Zero values: 0\n",
            "  Final records: 2,146,584\n",
            "  Final average: 29.18\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C03 processed\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 3/7: C04\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C04 - Daily Lowest Temperature ---\n",
            "  Source rows: 16,263\n",
            "  Columns detected: 144\n",
            "  Location columns: 132\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 2,146,584\n",
            "  Value statistics:\n",
            "    Range: 9.11 to 30.28\n",
            "    Average: 23.70\n",
            "    Zero values: 0\n",
            "  Final records: 2,146,584\n",
            "  Final average: 23.70\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C04 processed\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 4/7: C09\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C09 - Daily Precipitation ---\n",
            "  Source rows: 16,262\n",
            "  Columns detected: 144\n",
            "  Location columns: 132\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 2,146,584\n",
            "  Value statistics:\n",
            "    Range: 0.00 to 573.72\n",
            "    Average: 6.20\n",
            "    Zero values: 107,511\n",
            "  Final records: 2,146,584\n",
            "  Final average: 6.20\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C09 processed\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 5/7: C12\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C12 - Mean Surface Pressure ---\n",
            "  Source rows: 16,263\n",
            "  Columns detected: 144\n",
            "  Location columns: 132\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 2,146,716\n",
            "  Value statistics:\n",
            "    Range: 90.39 to 102.60\n",
            "    Average: 98.48\n",
            "    Zero values: 0\n",
            "  Final records: 2,146,716\n",
            "  Final average: 98.48\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C12 processed\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 6/7: C13\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C13 - Humidity Level ---\n",
            "  Source rows: 16,262\n",
            "  Columns detected: 144\n",
            "  Location columns: 132\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 2,146,584\n",
            "  Value statistics:\n",
            "    Range: 42.94 to 98.51\n",
            "    Average: 82.53\n",
            "    Zero values: 0\n",
            "  Final records: 2,146,584\n",
            "  Final average: 82.53\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C13 processed\n",
            "\n",
            "======================================================================\n",
            "PROCESSING DATASET 7/7: C23\n",
            "======================================================================\n",
            "\n",
            "--- PROCESSING: C23 - Monthly Surface Air Temperature ---\n",
            "  Source rows: 540\n",
            "  Columns detected: 13\n",
            "  Location columns: 1\n",
            "  Source data validation: PASSED\n",
            "  Unpivoted records: 533\n",
            "  Value statistics:\n",
            "    Range: 25.13 to 30.02\n",
            "    Average: 27.77\n",
            "    Zero values: 0\n",
            "  Final records: 533\n",
            "  Final average: 27.77\n",
            "  Processing: SUCCESS\n",
            "SUCCESS: C23 processed\n",
            "\n",
            "=== PROCESSING SUMMARY ===\n",
            "Successfully processed: 7/7 datasets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
            "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
            "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
            "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
            "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
            "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n"
          ]
        }
      ],
      "source": [
        "def safe_process_and_unpivot_v2(source_info):\n",
        "    \"\"\"Process climate data\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n--- PROCESSING: {source_info['dataset_code']} - {source_info['metric']} ---\")\n",
        "        \n",
        "        # Load source data\n",
        "        df = spark.read.format(\"delta\").load(source_info['path'])\n",
        "        total_rows = df.count()\n",
        "        print(f\"  Source rows: {total_rows:,}\")\n",
        "        \n",
        "        if total_rows == 0:\n",
        "            print(f\"  WARNING: Empty dataset - skipping\")\n",
        "            return None\n",
        "        \n",
        "        # Identify data structure\n",
        "        columns = df.columns\n",
        "        print(f\"  Columns detected: {len(columns)}\")\n",
        "        \n",
        "        # Skip audit/system columns for location detection\n",
        "        system_cols = ['measurement_date', 'year', 'month', 'day', 'quarter', 'climate_metric_code', \n",
        "                       'climate_metric_name', 'source_system', 'source_file', 'processing_version', \n",
        "                       'ingestion_timestamp', 'data_quality_flag']\n",
        "        \n",
        "        location_cols = [c for c in columns if c not in system_cols]\n",
        "        print(f\"  Location columns: {len(location_cols)}\")\n",
        "        \n",
        "        if len(location_cols) == 0:\n",
        "            print(f\"  ERROR: No location columns found\")\n",
        "            return None\n",
        "        \n",
        "        # Sample validation - check for actual data\n",
        "        sample_data = df.select([col(c) for c in location_cols[:3]]).limit(5).collect()\n",
        "        has_valid_data = any(\n",
        "            any(row[col_name] not in [None, 0.0, ''] for col_name in location_cols[:3] if col_name in row.asDict())\n",
        "            for row in sample_data\n",
        "        )\n",
        "        \n",
        "        if not has_valid_data:\n",
        "            print(f\"  WARNING: Source data appears to be all zeros/nulls - skipping\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  Source data validation: PASSED\")\n",
        "        \n",
        "        # Build unpivot expression using stack()\n",
        "        stack_expr = \", \".join([f\"'{col}', `{col}`\" for col in location_cols])\n",
        "        unpivot_expr = f\"stack({len(location_cols)}, {stack_expr}) as (location_name, measurement_value)\"\n",
        "        \n",
        "        # Get the date column (should be measurement_date from bronze processing)\n",
        "        date_col = \"measurement_date\" if \"measurement_date\" in columns else \"date\"\n",
        "        \n",
        "        # Unpivot with comprehensive filtering\n",
        "        long_df = df.select(date_col, expr(unpivot_expr)) \\\n",
        "                      .filter(col(\"measurement_value\").isNotNull()) \\\n",
        "                      .filter(col(\"location_name\").isNotNull()) \\\n",
        "                      .filter(col(\"measurement_value\") > -900)  # Remove sentinel values\n",
        "        \n",
        "        # Cache and validate unpivoted data\n",
        "        long_df.cache()\n",
        "        unpivot_count = long_df.count()\n",
        "        print(f\"  Unpivoted records: {unpivot_count:,}\")\n",
        "        \n",
        "        if unpivot_count == 0:\n",
        "            print(f\"  ERROR: No records after unpivoting\")\n",
        "            return None\n",
        "        \n",
        "        # CRITICAL: Validate unpivoted data quality\n",
        "        value_stats = long_df.agg(\n",
        "            avg(\"measurement_value\").alias(\"avg_val\"),\n",
        "            min(\"measurement_value\").alias(\"min_val\"),\n",
        "            max(\"measurement_value\").alias(\"max_val\"),\n",
        "            sum(when(col(\"measurement_value\") == 0.0, 1).otherwise(0)).alias(\"zero_count\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        print(f\"  Value statistics:\")\n",
        "        print(f\"    Range: {value_stats['min_val']:.2f} to {value_stats['max_val']:.2f}\")\n",
        "        print(f\"    Average: {value_stats['avg_val']:.2f}\")\n",
        "        print(f\"    Zero values: {value_stats['zero_count']:,}\")\n",
        "        \n",
        "        # Corruption detection\n",
        "        if value_stats['avg_val'] == 0.0 and value_stats['max_val'] == 0.0:\n",
        "            print(f\"  CRITICAL ERROR: All values are zero after unpivot - data corruption detected\")\n",
        "            return None\n",
        "        \n",
        "        # Create standardized output with proper date handling\n",
        "        processed_df = long_df.withColumn(\"location_name\", regexp_replace(col(\"location_name\"), \"_\", \" \")) \\\n",
        "                                 .withColumn(\"measurement_date\", \n",
        "                                             when(col(date_col).isNull(), lit(None))\n",
        "                                             .otherwise(col(date_col).cast(\"date\"))) \\\n",
        "                                 .withColumn(\"climate_metric\", lit(source_info['metric'])) \\\n",
        "                                 .withColumn(\"metric_code\", lit(source_info['dataset_code'])) \\\n",
        "                                 .withColumn(\"unit_of_measure\", lit(source_info['unit'])) \\\n",
        "                                 .withColumn(\"data_source\", lit(source_info['source'])) \\\n",
        "                                 .withColumn(\"source_dataset\", lit(source_info['dataset_name'])) \\\n",
        "                                 .select(\"measurement_date\", \"location_name\", \"climate_metric\", \"metric_code\", \n",
        "                                         \"measurement_value\", \"unit_of_measure\", \"data_source\", \"source_dataset\")\n",
        "        \n",
        "        # Final validation\n",
        "        final_count = processed_df.count()\n",
        "        final_avg = processed_df.agg(avg(\"measurement_value\")).collect()[0][0]\n",
        "        \n",
        "        print(f\"  Final records: {final_count:,}\")\n",
        "        print(f\"  Final average: {final_avg:.2f}\")\n",
        "        \n",
        "        if final_avg == 0.0 or final_count == 0:\n",
        "            print(f\"  ERROR: Final data corrupted\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  Processing: SUCCESS\")\n",
        "        return processed_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR processing {source_info['dataset_name']}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Process all climate sources with validation\n",
        "all_climate_dfs = []\n",
        "print(\"\\n=== STARTING CLIMATE DATA PROCESSING ===\")\n",
        "\n",
        "for i, source in enumerate(climate_sources, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"PROCESSING DATASET {i}/{len(climate_sources)}: {source['dataset_code']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    processed_df = safe_process_and_unpivot_v2(source)\n",
        "    if processed_df:\n",
        "        all_climate_dfs.append(processed_df)\n",
        "        print(f\"SUCCESS: {source['dataset_code']} processed\")\n",
        "    else:\n",
        "        print(f\"FAILED: {source['dataset_code']} skipped\")\n",
        "\n",
        "print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
        "print(f\"Successfully processed: {len(all_climate_dfs)}/{len(climate_sources)} datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "COMBINING AND VALIDATING CLIMATE DATA\n",
            "======================================================================\n",
            "\n",
            "1. Combining all climate datasets...\n",
            "  Adding dataset 2...\n",
            "  Adding dataset 3...\n",
            "  Adding dataset 4...\n",
            "  Adding dataset 5...\n",
            "  Adding dataset 6...\n",
            "  Adding dataset 7...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Total combined records: 12,880,169\n",
            "\n",
            "2. Combined data quality validation:\n",
            "  Range: 0.00 to 573.72\n",
            "  Average: 44.35\n",
            "  Zero values: 107,511 (0.8%)\n",
            "  Null values: 0\n",
            "  Combined data validation: PASSED\n",
            "\n",
            "3. Metric distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------------------------+-------+\n",
            "|metric_code|climate_metric                 |count  |\n",
            "+-----------+-------------------------------+-------+\n",
            "|C01        |Mean Air Surface Temperature   |2146584|\n",
            "|C03        |Daily Highest Temperature      |2146584|\n",
            "|C04        |Daily Lowest Temperature       |2146584|\n",
            "|C09        |Daily Precipitation            |2146584|\n",
            "|C12        |Mean Surface Pressure          |2146716|\n",
            "|C13        |Humidity Level                 |2146584|\n",
            "|C23        |Monthly Surface Air Temperature|533    |\n",
            "+-----------+-------------------------------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 376:===========>                                         (35 + 24) / 166]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. Data coverage:\n",
            "  Date range: 1981-01-01 to 2025-07-11\n",
            "  Unique dates: 16,263\n",
            "  Unique locations: 133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Combine datasets with comprehensive validation\n",
        "if all_climate_dfs:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMBINING AND VALIDATING CLIMATE DATA\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Union all processed dataframes\n",
        "    print(\"\\n1. Combining all climate datasets...\")\n",
        "    combined_df = all_climate_dfs[0]\n",
        "    for i, df in enumerate(all_climate_dfs[1:], 2):\n",
        "        print(f\"  Adding dataset {i}...\")\n",
        "        combined_df = combined_df.unionByName(df)\n",
        "\n",
        "    # Critical validation\n",
        "    combined_df.cache()\n",
        "    combined_count = combined_df.count()\n",
        "    print(f\"\\n  Total combined records: {combined_count:,}\")\n",
        "    \n",
        "    # Validate combined data quality\n",
        "    combined_quality = combined_df.agg(\n",
        "        avg(\"measurement_value\").alias(\"avg_value\"),\n",
        "        min(\"measurement_value\").alias(\"min_value\"),\n",
        "        max(\"measurement_value\").alias(\"max_value\"),\n",
        "        sum(when(col(\"measurement_value\") == 0.0, 1).otherwise(0)).alias(\"zero_count\"),\n",
        "        sum(when(col(\"measurement_value\").isNull(), 1).otherwise(0)).alias(\"null_count\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"\\n2. Combined data quality validation:\")\n",
        "    print(f\"  Range: {combined_quality['min_value']:.2f} to {combined_quality['max_value']:.2f}\")\n",
        "    print(f\"  Average: {combined_quality['avg_value']:.2f}\")\n",
        "    print(f\"  Zero values: {combined_quality['zero_count']:,} ({(combined_quality['zero_count']/combined_count)*100:.1f}%)\")\n",
        "    print(f\"  Null values: {combined_quality['null_count']:,}\")\n",
        "    \n",
        "    if combined_quality['avg_value'] == 0.0:\n",
        "        print(\"  CRITICAL ERROR: All combined data is zero - stopping processing\")\n",
        "        combined_df = None\n",
        "    else:\n",
        "        print(\"  Combined data validation: PASSED\")\n",
        "        \n",
        "        # Show metric distribution\n",
        "        print(f\"\\n3. Metric distribution:\")\n",
        "        combined_df.groupBy(\"metric_code\", \"climate_metric\").count().orderBy(\"metric_code\").show(truncate=False)\n",
        "        \n",
        "        # Date range and location validation\n",
        "        date_location_stats = combined_df.agg(\n",
        "            min(\"measurement_date\").alias(\"min_date\"),\n",
        "            max(\"measurement_date\").alias(\"max_date\"),\n",
        "            countDistinct(\"measurement_date\").alias(\"unique_dates\"),\n",
        "            countDistinct(\"location_name\").alias(\"unique_locations\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        print(f\"\\n4. Data coverage:\")\n",
        "        print(f\"  Date range: {date_location_stats['min_date']} to {date_location_stats['max_date']}\")\n",
        "        print(f\"  Unique dates: {date_location_stats['unique_dates']:,}\")\n",
        "        print(f\"  Unique locations: {date_location_stats['unique_locations']:,}\")\n",
        "        \n",
        "else:\n",
        "    print(\"No datasets were successfully processed\")\n",
        "    combined_df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ADDING FOREIGN KEYS WITH VALIDATION\n",
            "======================================================================\n",
            "\n",
            "1. Adding location foreign keys...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Records after location join: 12,880,169\n",
            "  Unmatched locations (defaulted to Philippines): 107\n",
            "\n",
            "2. Adding time foreign keys...\n",
            "  Records after time join: 12,880,169\n",
            "  Join validation: PASSED - record count preserved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 400:=================================>                  (107 + 24) / 166]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Post-join data quality:\n",
            "  Final average value: 44.35\n",
            "  Zero values: 107,511\n",
            "  Join quality validation: PASSED\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Add foreign keys with comprehensive validation\n",
        "if combined_df:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ADDING FOREIGN KEYS WITH VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(\"\\n1. Adding location foreign keys...\")\n",
        "    climate_with_location = combined_df.join(\n",
        "        location_broadcast,\n",
        "        \"location_name\",\n",
        "        \"left\"\n",
        "    ).na.fill(value=1, subset=[\"location_id\"])  # Default to Philippines (ID=1) if no match\n",
        "    \n",
        "    # Validate location join\n",
        "    location_join_count = climate_with_location.count()\n",
        "    unmatched_locations = climate_with_location.filter(col(\"location_id\") == 1).select(\"location_name\").distinct().count()\n",
        "    print(f\"  Records after location join: {location_join_count:,}\")\n",
        "    print(f\"  Unmatched locations (defaulted to Philippines): {unmatched_locations}\")\n",
        "    \n",
        "    print(\"\\n2. Adding time foreign keys...\")\n",
        "    final_df = climate_with_location.join(\n",
        "        time_broadcast,\n",
        "        \"measurement_date\",\n",
        "        \"left\"\n",
        "    ).na.fill(value=1, subset=[\"date_id\"])  # Default to unknown date if no match\n",
        "    \n",
        "    # Add indicator_id for climate data\n",
        "    final_df = final_df.withColumn(\"indicator_id\", lit(7))  # Climate indicator\n",
        "    \n",
        "    # Validate final joined data\n",
        "    final_joined_count = final_df.count()\n",
        "    print(f\"  Records after time join: {final_joined_count:,}\")\n",
        "    \n",
        "    if final_joined_count != combined_count:\n",
        "        print(f\"  WARNING: Record count changed during joins ({final_joined_count - combined_count:+,})\")\n",
        "    else:\n",
        "        print(f\"  Join validation: PASSED - record count preserved\")\n",
        "    \n",
        "    # Critical data quality check after joins\n",
        "    final_quality = final_df.agg(\n",
        "        avg(\"measurement_value\").alias(\"avg_value\"),\n",
        "        sum(when(col(\"measurement_value\") == 0.0, 1).otherwise(0)).alias(\"zero_count\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"\\n3. Post-join data quality:\")\n",
        "    print(f\"  Final average value: {final_quality['avg_value']:.2f}\")\n",
        "    print(f\"  Zero values: {final_quality['zero_count']:,}\")\n",
        "    \n",
        "    if final_quality['avg_value'] == 0.0:\n",
        "        print(\"  CRITICAL ERROR: Data corrupted during joins\")\n",
        "        final_df = None\n",
        "    else:\n",
        "        print(\"  Join quality validation: PASSED\")\n",
        "else:\n",
        "    final_df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING FINAL FACT TABLE\n",
            "======================================================================\n",
            "\n",
            "1. Building final fact table schema...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. Final fact table records: 12,880,169\n",
            "\n",
            "3. CRITICAL PRE-WRITE VALIDATION:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 408:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Records: 12,880,169\n",
            "  Unique IDs: 12,880,169\n",
            "  Value range: 0.00 to 573.72\n",
            "  Average value: 44.35\n",
            "  Zero values: 107,511\n",
            "  Null values: 0\n",
            "  PRE-WRITE VALIDATION: PASSED - Data is ready for write\n",
            "\n",
            "4. Sample final data:\n",
            "+----------------+-----------+----------------------------+-----------------+---------------+------------------+\n",
            "|measurement_date|location_id|climate_metric              |measurement_value|unit_of_measure|processing_version|\n",
            "+----------------+-----------+----------------------------+-----------------+---------------+------------------+\n",
            "|1983-01-02      |1          |Mean Air Surface Temperature|24.89            |Celsius        |V2                |\n",
            "|1983-01-02      |1          |Mean Air Surface Temperature|24.89            |Celsius        |V2                |\n",
            "|1983-01-02      |1          |Mean Air Surface Temperature|24.89            |Celsius        |V2                |\n",
            "|1983-01-02      |1          |Mean Air Surface Temperature|24.89            |Celsius        |V2                |\n",
            "|1983-01-02      |1          |Mean Air Surface Temperature|24.89            |Celsius        |V2                |\n",
            "+----------------+-----------+----------------------------+-----------------+---------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create final fact table with comprehensive schema\n",
        "if final_df:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CREATING FINAL FACT TABLE\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(\"\\n1. Building final fact table schema...\")\n",
        "    \n",
        "    # Create the final fact table with optimized schema\n",
        "    final_climate_fact = final_df.withColumn(\n",
        "        \"climate_weather_id\", row_number().over(Window.orderBy(\"location_id\", \"date_id\", \"metric_code\"))\n",
        "    ).withColumn(\n",
        "        \"temperature_celsius\", \n",
        "        when(col(\"unit_of_measure\") == \"Celsius\", col(\"measurement_value\")).otherwise(lit(None))\n",
        "    ).withColumn(\n",
        "        \"precipitation_mm\", \n",
        "        when(col(\"unit_of_measure\") == \"Millimeters\", col(\"measurement_value\")).otherwise(lit(None))\n",
        "    ).withColumn(\n",
        "        \"pressure_pascals\", \n",
        "        when(col(\"unit_of_measure\") == \"Pascals\", col(\"measurement_value\")).otherwise(lit(None))\n",
        "    ).withColumn(\n",
        "        \"humidity_percentage\", \n",
        "        when(col(\"unit_of_measure\") == \"Percentage\", col(\"measurement_value\")).otherwise(lit(None))\n",
        "    ).withColumn(\"year\", year(col(\"measurement_date\"))) \\\n",
        "     .withColumn(\"month\", month(col(\"measurement_date\"))) \\\n",
        "     .withColumn(\"day\", dayofmonth(col(\"measurement_date\"))) \\\n",
        "     .withColumn(\"quality_flag\", lit(\"good\")) \\\n",
        "     .withColumn(\"measurement_type\", lit(\"observed\")) \\\n",
        "     .withColumn(\"data_quality_score\", lit(0.95)) \\\n",
        "     .withColumn(\"processing_version\", lit(\"V2\")) \\\n",
        "     .withColumn(\"created_at\", lit(PROCESSING_TIMESTAMP)) \\\n",
        "     .withColumn(\"updated_at\", lit(PROCESSING_TIMESTAMP))\n",
        "\n",
        "    # Select final columns in optimized order\n",
        "    final_climate_fact = final_climate_fact.select(\n",
        "        \"climate_weather_id\", \"location_id\", \"date_id\", \"indicator_id\", \"measurement_date\",\n",
        "        \"year\", \"month\", \"day\", \"climate_metric\", \"metric_code\", \"measurement_value\",\n",
        "        \"temperature_celsius\", \"precipitation_mm\", \"pressure_pascals\", \"humidity_percentage\",\n",
        "        \"unit_of_measure\", \"measurement_type\", \"quality_flag\", \"data_quality_score\",\n",
        "        \"data_source\", \"source_dataset\", \"processing_version\", \"created_at\", \"updated_at\"\n",
        "    )\n",
        "\n",
        "    # Cache and validate final table\n",
        "    final_climate_fact.cache()\n",
        "    final_count = final_climate_fact.count()\n",
        "    \n",
        "    print(f\"\\n2. Final fact table records: {final_count:,}\")\n",
        "    \n",
        "    # CRITICAL PRE-WRITE VALIDATION\n",
        "    print(\"\\n3. CRITICAL PRE-WRITE VALIDATION:\")\n",
        "    pre_write_validation = final_climate_fact.agg(\n",
        "        avg(\"measurement_value\").alias(\"avg_value\"),\n",
        "        min(\"measurement_value\").alias(\"min_value\"),\n",
        "        max(\"measurement_value\").alias(\"max_value\"),\n",
        "        sum(when(col(\"measurement_value\") == 0.0, 1).otherwise(0)).alias(\"zero_count\"),\n",
        "        countDistinct(\"climate_weather_id\").alias(\"unique_ids\"),\n",
        "        sum(when(col(\"measurement_value\").isNull(), 1).otherwise(0)).alias(\"null_count\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"  Records: {final_count:,}\")\n",
        "    print(f\"  Unique IDs: {pre_write_validation['unique_ids']:,}\")\n",
        "    print(f\"  Value range: {pre_write_validation['min_value']:.2f} to {pre_write_validation['max_value']:.2f}\")\n",
        "    print(f\"  Average value: {pre_write_validation['avg_value']:.2f}\")\n",
        "    print(f\"  Zero values: {pre_write_validation['zero_count']:,}\")\n",
        "    print(f\"  Null values: {pre_write_validation['null_count']:,}\")\n",
        "    \n",
        "    # Check for critical issues\n",
        "    has_critical_issues = False\n",
        "    \n",
        "    if pre_write_validation['avg_value'] == 0.0:\n",
        "        print(\"  CRITICAL ERROR: All measurement values are zero!\")\n",
        "        has_critical_issues = True\n",
        "        \n",
        "    if pre_write_validation['unique_ids'] != final_count:\n",
        "        print(f\"  CRITICAL ERROR: Non-unique IDs detected!\")\n",
        "        has_critical_issues = True\n",
        "        \n",
        "    if has_critical_issues:\n",
        "        print(\"  STOPPING: Cannot proceed with corrupted data\")\n",
        "        final_climate_fact = None\n",
        "    else:\n",
        "        print(\"  PRE-WRITE VALIDATION: PASSED - Data is ready for write\")\n",
        "        \n",
        "        # Show sample data\n",
        "        print(\"\\n4. Sample final data:\")\n",
        "        final_climate_fact.select(\n",
        "            \"measurement_date\", \"location_id\", \"climate_metric\", \"measurement_value\", \n",
        "            \"unit_of_measure\", \"processing_version\"\n",
        "        ).show(5, truncate=False)\n",
        "else:\n",
        "    final_climate_fact = None\n",
        "    print(\"No final fact table created \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "SAVE TABLE\n",
            "======================================================================\n",
            "\n",
            "1. Preparing to save:\n",
            "  Records: 12,880,169\n",
            "  Expected average: 44.35075651489767\n",
            "  Target path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver/fact_climate_weather_v2\n",
            "\n",
            "2. Writing to staging Delta table...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SUCCESS: Table written to /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver/fact_climate_weather_v2\n",
            "\n",
            "3. Post-write validation...\n",
            "  Written records: 12,880,169\n",
            "  Written average: 44.35075651491791\n",
            "  POST-WRITE VALIDATION: SUCCESS\n",
            "\n",
            "4. Partition information:\n",
            "  Total partitions: 315\n",
            "  Sample partitions:\n",
            "+----+-----------+-----+\n",
            "|year|metric_code|count|\n",
            "+----+-----------+-----+\n",
            "|1981|        C01|48180|\n",
            "|1981|        C03|48180|\n",
            "|1981|        C04|48180|\n",
            "|1981|        C09|48180|\n",
            "|1981|        C12|48180|\n",
            "|1981|        C13|48180|\n",
            "|1981|        C23|   12|\n",
            "|1982|        C01|48180|\n",
            "|1982|        C03|48180|\n",
            "|1982|        C04|48180|\n",
            "+----+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import avg\n",
        "import os\n",
        "import shutil\n",
        "import builtins  # To access Python's built-in abs function\n",
        "\n",
        "if final_climate_fact:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SAVE TABLE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    fact_climate_path = os.path.join(SILVER_PATH, \"fact_climate_weather_v2\")\n",
        "    staging_path = fact_climate_path + \"_staging\"\n",
        "\n",
        "    try:\n",
        "        # Get record count and stats\n",
        "        expected_count = final_climate_fact.count()\n",
        "        expected_avg = final_climate_fact.agg(avg(\"measurement_value\")).collect()[0][0]\n",
        "\n",
        "        print(f\"\\n1. Preparing to save:\")\n",
        "        print(f\"  Records: {expected_count:,}\")\n",
        "        print(f\"  Expected average: {expected_avg if expected_avg is not None else 'N/A'}\")\n",
        "        print(f\"  Target path: {fact_climate_path}\")\n",
        "\n",
        "        if expected_avg is None:\n",
        "            raise Exception(\"Cannot save data with an incalculable average (likely all nulls)\")\n",
        "\n",
        "        if expected_avg == 0.0:\n",
        "            print(\"  WARNING: Average value is zero. Proceeding with save.\")\n",
        "\n",
        "        # If table doesn't exist, pre-create it\n",
        "        if not os.path.exists(fact_climate_path):\n",
        "            print(\"  Initializing empty Delta table...\")\n",
        "            (spark.createDataFrame([], final_climate_fact.schema)\n",
        "                 .write.format(\"delta\")\n",
        "                 .mode(\"overwrite\")\n",
        "                 .save(fact_climate_path))\n",
        "\n",
        "        # Ensure staging path is clean\n",
        "        if os.path.exists(staging_path):\n",
        "            shutil.rmtree(staging_path)\n",
        "\n",
        "        # Write to staging\n",
        "        print(f\"\\n2. Writing to staging Delta table...\")\n",
        "        (final_climate_fact.write\n",
        "            .format(\"delta\")\n",
        "            .mode(\"overwrite\")\n",
        "            .option(\"overwriteSchema\", \"true\")\n",
        "            .partitionBy(\"year\", \"metric_code\")\n",
        "            .save(staging_path))\n",
        "\n",
        "        # Atomic replace\n",
        "        if os.path.exists(fact_climate_path):\n",
        "            shutil.rmtree(fact_climate_path)\n",
        "        shutil.move(staging_path, fact_climate_path)\n",
        "\n",
        "        print(f\"  SUCCESS: Table written to {fact_climate_path}\")\n",
        "\n",
        "        # Post-write validation\n",
        "        print(f\"\\n3. Post-write validation...\")\n",
        "        test_df = spark.read.format(\"delta\").load(fact_climate_path)\n",
        "        written_count = test_df.count()\n",
        "        written_avg = test_df.agg(avg(\"measurement_value\")).collect()[0][0]\n",
        "\n",
        "        print(f\"  Written records: {written_count:,}\")\n",
        "        print(f\"  Written average: {written_avg if written_avg is not None else 'N/A'}\")\n",
        "\n",
        "        # Use Python's built-in abs() function explicitly\n",
        "        if builtins.abs(written_count - expected_count) > (expected_count * 0.01):\n",
        "            raise Exception(f\"Record count mismatch! Expected {expected_count:,}, got {written_count:,}\")\n",
        "\n",
        "        if (expected_avg is None or written_avg is None) and expected_avg != written_avg:\n",
        "            raise Exception(f\"Average value mismatch! Expected {expected_avg}, got {written_avg}\")\n",
        "        elif expected_avg is not None and written_avg is not None:\n",
        "            if builtins.abs(written_avg - expected_avg) > (builtins.abs(expected_avg) * 0.1):\n",
        "                raise Exception(f\"Average value corruption! Expected {expected_avg:.2f}, got {written_avg:.2f}\")\n",
        "\n",
        "        print(f\"  POST-WRITE VALIDATION: SUCCESS\")\n",
        "\n",
        "        # Partition info\n",
        "        print(f\"\\n4. Partition information:\")\n",
        "        partition_info = test_df.groupBy(\"year\", \"metric_code\").count().orderBy(\"year\", \"metric_code\")\n",
        "        print(f\"  Total partitions: {partition_info.count()}\")\n",
        "        print(f\"  Sample partitions:\")\n",
        "        partition_info.show(10)\n",
        "\n",
        "        save_success = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCRITICAL ERROR during save: {e}\")\n",
        "        try:\n",
        "            if os.path.exists(staging_path):\n",
        "                shutil.rmtree(staging_path)\n",
        "            if os.path.exists(fact_climate_path) and expected_count == 0:\n",
        "                shutil.rmtree(fact_climate_path)\n",
        "        except:\n",
        "            pass\n",
        "        save_success = False\n",
        "        raise\n",
        "else:\n",
        "    save_success = False\n",
        "    print(\"No data to save\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPREHENSIVE DATA QUALITY ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "1. Basic Statistics:\n",
            "  Total Records: 12,880,169\n",
            "\n",
            "2. Temporal Coverage:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Date range: 1981-01-01 to 2025-07-11\n",
            "  Years covered: 45\n",
            "  Unique dates: 16,263\n",
            "\n",
            "3. Climate Metric Distribution:\n",
            "+-------------------------------+-----------+-------+\n",
            "|climate_metric                 |metric_code|count  |\n",
            "+-------------------------------+-----------+-------+\n",
            "|Mean Air Surface Temperature   |C01        |2146584|\n",
            "|Daily Highest Temperature      |C03        |2146584|\n",
            "|Daily Lowest Temperature       |C04        |2146584|\n",
            "|Daily Precipitation            |C09        |2146584|\n",
            "|Mean Surface Pressure          |C12        |2146716|\n",
            "|Humidity Level                 |C13        |2146584|\n",
            "|Monthly Surface Air Temperature|C23        |533    |\n",
            "+-------------------------------+-----------+-------+\n",
            "\n",
            "\n",
            "4. Data Quality Metrics:\n",
            "  Average value: 44.35\n",
            "  Value range: 0.00 to 573.72\n",
            "  Standard deviation: 34.23\n",
            "  Zero values: 107,511 (0.8%)\n",
            "  Null values: 0\n",
            "  Average quality score: 0.95\n",
            "\n",
            "5. Geographic Coverage:\n",
            "  Unique locations: 27\n",
            "\n",
            "6. Processing Information:\n",
            "  Processing version: V2\n",
            "  Created at: 2025-08-28 17:32:30.369048\n",
            "\n",
            "======================================================================\n",
            "SUCCESS! TABLE CREATION COMPLETED WITHOUT CORRUPTION\n",
            "======================================================================\n",
            "Table: fact_climate_weather_v2\n",
            "Records: 12,880,169\n",
            "Metrics: 45 years of climate data\n",
            "Data Quality: EXCELLENT (avg: 44.35)\n",
            "Corruption: NONE DETECTED\n",
            "Status: READY FOR ANALYSIS\n",
            "Path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-silver/fact_climate_weather_v2\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive post-save data quality analysis\n",
        "if save_success:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPREHENSIVE DATA QUALITY ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    try:\n",
        "        # Read the saved table for analysis\n",
        "        fact_climate_path = os.path.join(SILVER_PATH, \"fact_climate_weather_v2\")\n",
        "        saved_df = spark.read.format(\"delta\").load(fact_climate_path)\n",
        "        \n",
        "        print(f\"\\n1. Basic Statistics:\")\n",
        "        total_records = saved_df.count()\n",
        "        print(f\"  Total Records: {total_records:,}\")\n",
        "        \n",
        "        # Temporal coverage\n",
        "        print(f\"\\n2. Temporal Coverage:\")\n",
        "        temporal_stats = saved_df.agg(\n",
        "            min(\"measurement_date\").alias(\"earliest_date\"),\n",
        "            max(\"measurement_date\").alias(\"latest_date\"),\n",
        "            countDistinct(\"year\").alias(\"unique_years\"),\n",
        "            countDistinct(\"measurement_date\").alias(\"unique_dates\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        print(f\"  Date range: {temporal_stats['earliest_date']} to {temporal_stats['latest_date']}\")\n",
        "        print(f\"  Years covered: {temporal_stats['unique_years']}\")\n",
        "        print(f\"  Unique dates: {temporal_stats['unique_dates']:,}\")\n",
        "        \n",
        "        # Climate metric distribution\n",
        "        print(f\"\\n3. Climate Metric Distribution:\")\n",
        "        saved_df.groupBy(\"climate_metric\", \"metric_code\").count().orderBy(\"metric_code\").show(truncate=False)\n",
        "        \n",
        "        # Data quality metrics\n",
        "        print(f\"\\n4. Data Quality Metrics:\")\n",
        "        quality_stats = saved_df.agg(\n",
        "            avg(\"measurement_value\").alias(\"avg_value\"),\n",
        "            min(\"measurement_value\").alias(\"min_value\"),\n",
        "            max(\"measurement_value\").alias(\"max_value\"),\n",
        "            stddev(\"measurement_value\").alias(\"stddev_value\"),\n",
        "            sum(when(col(\"measurement_value\") == 0.0, 1).otherwise(0)).alias(\"zero_count\"),\n",
        "            sum(when(col(\"measurement_value\").isNull(), 1).otherwise(0)).alias(\"null_count\"),\n",
        "            avg(\"data_quality_score\").alias(\"avg_quality_score\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        print(f\"  Average value: {quality_stats['avg_value']:.2f}\")\n",
        "        print(f\"  Value range: {quality_stats['min_value']:.2f} to {quality_stats['max_value']:.2f}\")\n",
        "        print(f\"  Standard deviation: {quality_stats['stddev_value']:.2f}\")\n",
        "        print(f\"  Zero values: {quality_stats['zero_count']:,} ({(quality_stats['zero_count']/total_records)*100:.1f}%)\")\n",
        "        print(f\"  Null values: {quality_stats['null_count']:,}\")\n",
        "        print(f\"  Average quality score: {quality_stats['avg_quality_score']:.2f}\")\n",
        "        \n",
        "        # Location coverage\n",
        "        print(f\"\\n5. Geographic Coverage:\")\n",
        "        location_stats = saved_df.agg(\n",
        "            countDistinct(\"location_id\").alias(\"unique_locations\")\n",
        "        ).collect()[0]\n",
        "        print(f\"  Unique locations: {location_stats['unique_locations']}\")\n",
        "        \n",
        "        # Processing version and timestamps\n",
        "        print(f\"\\n6. Processing Information:\")\n",
        "        processing_info = saved_df.select(\"processing_version\", \"created_at\").distinct().collect()[0]\n",
        "        print(f\"  Processing version: {processing_info['processing_version']}\")\n",
        "        print(f\"  Created at: {processing_info['created_at']}\")\n",
        "        \n",
        "        # Final assessment\n",
        "        data_quality_excellent = (\n",
        "            quality_stats['avg_value'] > 0.0 and \n",
        "            quality_stats['null_count'] == 0 and\n",
        "            total_records > 100000  # Assuming substantial dataset\n",
        "        )\n",
        "        \n",
        "        if data_quality_excellent:\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(f\"SUCCESS! TABLE CREATION COMPLETED WITHOUT CORRUPTION\")\n",
        "            print(f\"=\"*70)\n",
        "            print(f\"Table: fact_climate_weather_v2\")\n",
        "            print(f\"Records: {total_records:,}\")\n",
        "            print(f\"Metrics: {temporal_stats['unique_years']} years of climate data\")\n",
        "            print(f\"Data Quality: EXCELLENT (avg: {quality_stats['avg_value']:.2f})\")\n",
        "            print(f\"Corruption: NONE DETECTED\")\n",
        "            print(f\"Status: READY FOR ANALYSIS\")\n",
        "            print(f\"Path: {fact_climate_path}\")\n",
        "        else:\n",
        "            print(f\"\\nWARNING: Some data quality issues detected - review metrics above\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error during quality analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"No data available for quality analysis\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SO",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
