{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSA Population Data to Bronze Layer - Improved\n",
    "\n",
    "This notebook processes PSA Population CSV files to Delta tables with improved encoding handling.\n",
    "Population data includes birth statistics, death records, demographic data, and housing information.\n",
    "\n",
    "Improvements:\n",
    "- Multiple encoding support (utf-8, latin1, iso-8859-1, cp1252, utf-16)\n",
    "- Skip bad lines automatically\n",
    "- Better error handling and reporting\n",
    "- Empty row filtering\n",
    "\n",
    "Expected format:\n",
    "- Row 1: Title (quoted string)\n",
    "- Row 2: Empty\n",
    "- Row 3: Column headers (semicolon-separated)\n",
    "- Row 4+: Data (semicolon-separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Delta Lake\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress pandas warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"PSA-Population-Bronze-Improved\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and helper functions\n",
    "psa_data_path = \"../PSA/Population\"\n",
    "bronze_layer_path = \"../final-spark-bronze/bronze_population_improved\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(bronze_layer_path, exist_ok=True)\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names for Delta compatibility\"\"\"\n",
    "    if not col_name or col_name.strip() == \"\":\n",
    "        return \"unnamed_column\"\n",
    "    \n",
    "    cleaned = str(col_name).strip().replace('\"', '').replace(\"'\", '')\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', cleaned)\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    \n",
    "    if not cleaned:\n",
    "        cleaned = \"unnamed_column\"\n",
    "    elif cleaned[0].isdigit():\n",
    "        cleaned = f\"col_{cleaned}\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def clean_table_name(filename):\n",
    "    \"\"\"Generate clean table name from filename\"\"\"\n",
    "    table_name = filename.replace('.csv', '')\n",
    "    table_name = clean_column_name(table_name)\n",
    "    table_name = f\"psa_population_{table_name}\"\n",
    "    \n",
    "    if len(table_name) > 100:\n",
    "        table_name = table_name[:100].rstrip('_')\n",
    "    \n",
    "    return table_name\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"Detect file encoding by trying to read first few lines\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'utf-16']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                f.read(1000)  # Try to read first 1000 characters\n",
    "            return encoding\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return 'latin1'  # Fallback encoding\n",
    "\n",
    "def read_psa_csv_with_pandas(file_path):\n",
    "    \"\"\"Read PSA CSV using pandas with robust encoding handling\"\"\"\n",
    "    try:\n",
    "        # First detect the most likely encoding\n",
    "        detected_encoding = detect_encoding(file_path)\n",
    "        print(f\"    Detected encoding: {detected_encoding}\")\n",
    "        \n",
    "        # Try with detected encoding first\n",
    "        encodings_to_try = [detected_encoding] + ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "        encodings_to_try = list(dict.fromkeys(encodings_to_try))  # Remove duplicates\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                df_pandas = pd.read_csv(\n",
    "                    file_path,\n",
    "                    sep=';',\n",
    "                    skiprows=2,\n",
    "                    encoding=encoding,\n",
    "                    na_values=['', '....', 'null', 'NULL', 'n/a', 'N/A'],\n",
    "                    keep_default_na=True,\n",
    "                    dtype=str,\n",
    "                    on_bad_lines='skip',\n",
    "                    skipinitialspace=True\n",
    "                )\n",
    "                \n",
    "                print(f\"    Successfully read with encoding: {encoding}\")\n",
    "                break\n",
    "                \n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"    Failed with encoding: {encoding}\")\n",
    "                continue\n",
    "            except pd.errors.EmptyDataError:\n",
    "                return None, False, \"File is empty or has no valid data\"\n",
    "        else:\n",
    "            return None, False, \"Could not decode file with any supported encoding\"\n",
    "        \n",
    "        # Validate DataFrame\n",
    "        if df_pandas.empty:\n",
    "            return None, False, \"DataFrame is empty after parsing\"\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        original_shape = df_pandas.shape\n",
    "        \n",
    "        # Remove completely empty rows\n",
    "        df_pandas = df_pandas.dropna(how='all')\n",
    "        \n",
    "        # Remove rows where all values are empty strings\n",
    "        df_pandas = df_pandas[~df_pandas.apply(lambda x: x.str.strip().eq('').all(), axis=1)]\n",
    "        \n",
    "        if df_pandas.empty:\n",
    "            return None, False, \"No valid data rows found after cleaning\"\n",
    "        \n",
    "        # Clean column names\n",
    "        original_columns = df_pandas.columns.tolist()\n",
    "        df_pandas.columns = [clean_column_name(col) for col in df_pandas.columns]\n",
    "        \n",
    "        # Remove duplicate column names by adding suffix\n",
    "        seen_columns = {}\n",
    "        new_columns = []\n",
    "        for col in df_pandas.columns:\n",
    "            if col in seen_columns:\n",
    "                seen_columns[col] += 1\n",
    "                new_columns.append(f\"{col}_{seen_columns[col]}\")\n",
    "            else:\n",
    "                seen_columns[col] = 0\n",
    "                new_columns.append(col)\n",
    "        \n",
    "        df_pandas.columns = new_columns\n",
    "        \n",
    "        print(f\"    Cleaned data: {original_shape} -> {df_pandas.shape}\")\n",
    "        \n",
    "        # Convert to Spark DataFrame\n",
    "        df_spark = spark.createDataFrame(df_pandas)\n",
    "        \n",
    "        return df_spark, True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, False, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "print(\"Helper functions defined\")\n",
    "print(f\"Source path: {psa_data_path}\")\n",
    "print(f\"Target path: {bronze_layer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of PSA Population CSV files\n",
    "psa_files = [f for f in os.listdir(psa_data_path) if f.endswith('.csv')]\n",
    "psa_files.sort()\n",
    "\n",
    "print(f\"Found {len(psa_files)} PSA Population CSV files to process\")\n",
    "print(\"Files to process:\")\n",
    "for i, file in enumerate(psa_files, 1):\n",
    "    print(f\"  {i:2d}. {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a test file first\n",
    "if psa_files:\n",
    "    test_file = psa_files[0]\n",
    "    test_file_path = os.path.join(psa_data_path, test_file)\n",
    "    \n",
    "    print(f\"Testing with: {test_file}\")\n",
    "    df_test, success, error = read_psa_csv_with_pandas(test_file_path)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Test successful: {df_test.count()} rows, {len(df_test.columns)} columns\")\n",
    "        print(\"Sample columns:\", df_test.columns[:5])\n",
    "        df_test.show(3, truncate=True)\n",
    "    else:\n",
    "        print(f\"Test failed: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all Population files\n",
    "processed_tables = []\n",
    "failed_files = []\n",
    "processing_details = []\n",
    "\n",
    "print(f\"Processing all {len(psa_files)} files...\")\n",
    "\n",
    "for i, filename in enumerate(psa_files, 1):\n",
    "    print(f\"\\nProcessing {i}/{len(psa_files)}: {filename}\")\n",
    "    \n",
    "    table_name = clean_table_name(filename)\n",
    "    file_path = os.path.join(psa_data_path, filename)\n",
    "    \n",
    "    try:\n",
    "        df, parse_success, parse_error = read_psa_csv_with_pandas(file_path)\n",
    "        \n",
    "        if not parse_success:\n",
    "            print(f\"  Failed to parse: {parse_error}\")\n",
    "            failed_files.append((filename, parse_error))\n",
    "            continue\n",
    "        \n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        \n",
    "        print(f\"  Parsed: {row_count} rows, {col_count} columns\")\n",
    "        \n",
    "        if row_count == 0:\n",
    "            print(f\"  Skipped: No data rows\")\n",
    "            failed_files.append((filename, \"No data rows found\"))\n",
    "            continue\n",
    "        \n",
    "        # Save to Delta\n",
    "        delta_path = os.path.join(bronze_layer_path, table_name)\n",
    "        \n",
    "        df.write.format(\"delta\") \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "          .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "          .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "          .save(delta_path)\n",
    "        \n",
    "        # Verify\n",
    "        df_check = spark.read.format(\"delta\").load(delta_path)\n",
    "        verify_count = df_check.count()\n",
    "        \n",
    "        print(f\"  Saved and verified: {verify_count} rows\")\n",
    "        processed_tables.append(table_name)\n",
    "        \n",
    "        processing_details.append({\n",
    "            'filename': filename,\n",
    "            'table_name': table_name,\n",
    "            'rows': verify_count,\n",
    "            'columns': col_count,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Processing error: {str(e)}\"\n",
    "        print(f\"  {error_msg}\")\n",
    "        failed_files.append((filename, error_msg))\n",
    "        \n",
    "        processing_details.append({\n",
    "            'filename': filename,\n",
    "            'error': error_msg,\n",
    "            'status': 'failed'\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed: {len(processed_tables)} tables\")\n",
    "print(f\"Failed: {len(failed_files)} files\")\n",
    "if len(processed_tables) + len(failed_files) > 0:\n",
    "    success_rate = len(processed_tables)/(len(processed_tables)+len(failed_files))*100\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "print(\"\\nDETAILED PROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFailed files ({len(failed_files)}):\")\n",
    "    for i, (filename, error) in enumerate(failed_files, 1):\n",
    "        print(f\"  {i:2d}. {filename}\")\n",
    "        print(f\"      Error: {error}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created Delta tables ({len(processed_tables)}):\")\n",
    "for i, detail in enumerate([d for d in processing_details if d['status'] == 'success'], 1):\n",
    "    print(f\"  {i:2d}. {detail['table_name']}\")\n",
    "    print(f\"      Source: {detail['filename']}\")\n",
    "    print(f\"      Data: {detail['rows']:,} rows, {detail['columns']} columns\")\n",
    "\n",
    "# Calculate statistics\n",
    "if processing_details:\n",
    "    successful_details = [d for d in processing_details if d['status'] == 'success']\n",
    "    if successful_details:\n",
    "        total_rows = sum(d['rows'] for d in successful_details)\n",
    "        avg_columns = sum(d['columns'] for d in successful_details) / len(successful_details)\n",
    "        \n",
    "        print(f\"\\nStatistics:\")\n",
    "        print(f\"  Total rows processed: {total_rows:,}\")\n",
    "        print(f\"  Average columns per table: {avg_columns:.1f}\")\n",
    "        print(f\"  Largest table: {max(successful_details, key=lambda x: x['rows'])['rows']:,} rows\")\n",
    "        print(f\"  Smallest table: {min(successful_details, key=lambda x: x['rows'])['rows']:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive processing report\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_source': 'PSA Population',\n",
    "    'total_files': len(psa_files),\n",
    "    'successful_tables': len(processed_tables),\n",
    "    'failed_files': len(failed_files),\n",
    "    'success_rate': len(processed_tables)/(len(processed_tables)+len(failed_files))*100 if (len(processed_tables)+len(failed_files)) > 0 else 0,\n",
    "    'processing_details': processing_details,\n",
    "    'processed_tables': processed_tables,\n",
    "    'failed_files': [{'filename': f, 'error': e} for f, e in failed_files]\n",
    "}\n",
    "\n",
    "report_path = os.path.join(bronze_layer_path, 'processing_report_detailed.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\nDetailed processing report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example table validation\n",
    "if processed_tables:\n",
    "    print(\"\\nEXAMPLE TABLE VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    example_table = processed_tables[0]\n",
    "    example_path = os.path.join(bronze_layer_path, example_table)\n",
    "    \n",
    "    print(f\"Validating: {example_table}\")\n",
    "    \n",
    "    df_example = spark.read.format(\"delta\").load(example_path)\n",
    "    \n",
    "    print(f\"\\nTable statistics:\")\n",
    "    print(f\"  Rows: {df_example.count():,}\")\n",
    "    print(f\"  Columns: {len(df_example.columns)}\")\n",
    "    \n",
    "    print(f\"\\nSchema:\")\n",
    "    df_example.printSchema()\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    df_example.show(5, truncate=False)\n",
    "    \n",
    "    print(f\"\\nData quality check:\")\n",
    "    total_rows = df_example.count()\n",
    "    for col_name in df_example.columns[:5]:\n",
    "        null_count = df_example.filter(col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")\n",
    "print(\"\\nProcessing complete. Check the bronze_population_improved directory for Delta tables.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}