{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOE Data to Bronze Layer\n",
    "\n",
    "This notebook processes DOE (Department of Energy) CSV and Excel files to Delta tables.\n",
    "Data includes energy consumption, electricity generation, power plant statistics, and energy sector data.\n",
    "\n",
    "Features:\n",
    "- Multiple encoding support for robust file processing\n",
    "- Handles both CSV and Excel files\n",
    "- Skip bad lines automatically\n",
    "- Comprehensive error handling and reporting\n",
    "- Empty row filtering and data cleaning\n",
    "- Automatic data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Delta Lake\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when, regexp_replace\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DOE-Bronze\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and helper functions\n",
    "doe_data_path = \"../DOE\"\n",
    "bronze_layer_path = \"../final-spark-bronze/bronze_doe\"\n",
    "\n",
    "os.makedirs(bronze_layer_path, exist_ok=True)\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names for Delta compatibility\"\"\"\n",
    "    if not col_name or col_name.strip() == \"\":\n",
    "        return \"unnamed_column\"\n",
    "    \n",
    "    cleaned = str(col_name).strip().replace('\"', '').replace(\"'\", '')\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', cleaned)\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    \n",
    "    if not cleaned:\n",
    "        cleaned = \"unnamed_column\"\n",
    "    elif cleaned[0].isdigit():\n",
    "        cleaned = f\"col_{cleaned}\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def clean_table_name(filename):\n",
    "    \"\"\"Generate clean table name from filename\"\"\"\n",
    "    table_name = filename.replace('.csv', '').replace('.xlsx', '').replace('.xls', '')\n",
    "    table_name = clean_column_name(table_name)\n",
    "    \n",
    "    # Add prefix if starts with number\n",
    "    if table_name[0].isdigit():\n",
    "        table_name = f\"doe_{table_name}\"\n",
    "    else:\n",
    "        table_name = f\"doe_{table_name}\"\n",
    "    \n",
    "    if len(table_name) > 100:\n",
    "        table_name = table_name[:100].rstrip('_')\n",
    "    \n",
    "    return table_name\n",
    "\n",
    "def detect_csv_format(file_path):\n",
    "    \"\"\"Detect CSV encoding and separator\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "    separators = [',', ';', '\\t']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                first_line = f.readline().strip()\n",
    "            \n",
    "            # Detect best separator\n",
    "            best_sep = ','\n",
    "            max_splits = 0\n",
    "            \n",
    "            for sep in separators:\n",
    "                splits = len(first_line.split(sep))\n",
    "                if splits > max_splits:\n",
    "                    max_splits = splits\n",
    "                    best_sep = sep\n",
    "            \n",
    "            return encoding, best_sep\n",
    "            \n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    return 'latin1', ','\n",
    "\n",
    "def read_csv_with_pandas(file_path):\n",
    "    \"\"\"Read CSV using pandas with robust handling\"\"\"\n",
    "    try:\n",
    "        encoding, separator = detect_csv_format(file_path)\n",
    "        print(f\"    Detected encoding: {encoding}, separator: '{separator}'\")\n",
    "        \n",
    "        df_pandas = pd.read_csv(\n",
    "            file_path,\n",
    "            encoding=encoding,\n",
    "            sep=separator,\n",
    "            na_values=['', 'N/A', 'n/a', 'NULL', 'null', '-'],\n",
    "            keep_default_na=True,\n",
    "            dtype=str,\n",
    "            on_bad_lines='skip',\n",
    "            skipinitialspace=True\n",
    "        )\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        original_shape = df_pandas.shape\n",
    "        df_pandas = df_pandas.dropna(how='all')\n",
    "        df_pandas = df_pandas[~df_pandas.apply(lambda x: x.str.strip().eq('').all(), axis=1)]\n",
    "        \n",
    "        if df_pandas.empty:\n",
    "            return None, False, \"No valid data rows found after cleaning\"\n",
    "        \n",
    "        print(f\"    Cleaned data: {original_shape} -> {df_pandas.shape}\")\n",
    "        \n",
    "        return df_pandas, True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, False, f\"CSV read error: {str(e)}\"\n",
    "\n",
    "def read_excel_with_pandas(file_path):\n",
    "    \"\"\"Read Excel using pandas\"\"\"\n",
    "    try:\n",
    "        # Get all sheet names\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        print(f\"    Found {len(excel_file.sheet_names)} sheets: {excel_file.sheet_names}\")\n",
    "        \n",
    "        # Read first sheet (or main data sheet)\n",
    "        sheet_name = excel_file.sheet_names[0]\n",
    "        df_pandas = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            na_values=['', 'N/A', 'n/a', 'NULL', 'null', '-'],\n",
    "            keep_default_na=True\n",
    "        )\n",
    "        \n",
    "        # Convert all columns to string to maintain consistency\n",
    "        df_pandas = df_pandas.astype(str)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        original_shape = df_pandas.shape\n",
    "        df_pandas = df_pandas.dropna(how='all')\n",
    "        df_pandas = df_pandas[~df_pandas.apply(lambda x: x.str.strip().eq('').all(), axis=1)]\n",
    "        \n",
    "        if df_pandas.empty:\n",
    "            return None, False, \"No valid data rows found after cleaning\"\n",
    "        \n",
    "        print(f\"    Cleaned data: {original_shape} -> {df_pandas.shape}\")\n",
    "        print(f\"    Using sheet: {sheet_name}\")\n",
    "        \n",
    "        return df_pandas, True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, False, f\"Excel read error: {str(e)}\"\n",
    "\n",
    "def clean_and_convert_dataframe(df_pandas):\n",
    "    \"\"\"Clean column names and handle duplicates\"\"\"\n",
    "    # Clean column names\n",
    "    df_pandas.columns = [clean_column_name(col) for col in df_pandas.columns]\n",
    "    \n",
    "    # Handle duplicate column names\n",
    "    seen_columns = {}\n",
    "    new_columns = []\n",
    "    for col in df_pandas.columns:\n",
    "        if col in seen_columns:\n",
    "            seen_columns[col] += 1\n",
    "            new_columns.append(f\"{col}_{seen_columns[col]}\")\n",
    "        else:\n",
    "            seen_columns[col] = 0\n",
    "            new_columns.append(col)\n",
    "    \n",
    "    df_pandas.columns = new_columns\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    return df_spark\n",
    "\n",
    "print(\"Helper functions defined\")\n",
    "print(f\"Source path: {doe_data_path}\")\n",
    "print(f\"Target path: {bronze_layer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of DOE files\n",
    "doe_files = [f for f in os.listdir(doe_data_path) if f.endswith(('.csv', '.xlsx', '.xls'))]\n",
    "doe_files.sort()\n",
    "\n",
    "print(f\"Found {len(doe_files)} DOE files to process\")\n",
    "print(\"Files to process:\")\n",
    "for i, file in enumerate(doe_files, 1):\n",
    "    file_ext = file.split('.')[-1].upper()\n",
    "    file_size = os.path.getsize(os.path.join(doe_data_path, file)) / 1024\n",
    "    print(f\"  {i:2d}. {file} ({file_ext}, {file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all DOE files\n",
    "processed_tables = []\n",
    "failed_files = []\n",
    "processing_details = []\n",
    "\n",
    "print(f\"Processing all {len(doe_files)} files...\")\n",
    "\n",
    "for i, filename in enumerate(doe_files, 1):\n",
    "    print(f\"\\nProcessing {i}/{len(doe_files)}: {filename}\")\n",
    "    \n",
    "    table_name = clean_table_name(filename)\n",
    "    file_path = os.path.join(doe_data_path, filename)\n",
    "    file_ext = filename.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # Read file based on extension\n",
    "        if file_ext == 'csv':\n",
    "            df_pandas, parse_success, parse_error = read_csv_with_pandas(file_path)\n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            df_pandas, parse_success, parse_error = read_excel_with_pandas(file_path)\n",
    "        else:\n",
    "            parse_success = False\n",
    "            parse_error = f\"Unsupported file type: {file_ext}\"\n",
    "            df_pandas = None\n",
    "        \n",
    "        if not parse_success:\n",
    "            print(f\"  Failed to parse: {parse_error}\")\n",
    "            failed_files.append((filename, parse_error))\n",
    "            processing_details.append({\n",
    "                'filename': filename,\n",
    "                'error': parse_error,\n",
    "                'status': 'failed'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Convert to Spark DataFrame\n",
    "        df_spark = clean_and_convert_dataframe(df_pandas)\n",
    "        \n",
    "        row_count = df_spark.count()\n",
    "        col_count = len(df_spark.columns)\n",
    "        \n",
    "        print(f\"  Processed: {row_count} rows, {col_count} columns\")\n",
    "        \n",
    "        if row_count == 0:\n",
    "            print(f\"  Skipped: No data rows\")\n",
    "            failed_files.append((filename, \"No data rows found\"))\n",
    "            processing_details.append({\n",
    "                'filename': filename,\n",
    "                'error': \"No data rows found\",\n",
    "                'status': 'failed'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Save to Delta\n",
    "        delta_path = os.path.join(bronze_layer_path, table_name)\n",
    "        \n",
    "        df_spark.write.format(\"delta\") \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "          .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "          .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "          .save(delta_path)\n",
    "        \n",
    "        # Verify\n",
    "        df_check = spark.read.format(\"delta\").load(delta_path)\n",
    "        verify_count = df_check.count()\n",
    "        \n",
    "        print(f\"  Saved and verified: {verify_count} rows\")\n",
    "        processed_tables.append(table_name)\n",
    "        \n",
    "        processing_details.append({\n",
    "            'filename': filename,\n",
    "            'table_name': table_name,\n",
    "            'file_type': file_ext,\n",
    "            'rows': verify_count,\n",
    "            'columns': col_count,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Processing error: {str(e)}\"\n",
    "        print(f\"  {error_msg}\")\n",
    "        failed_files.append((filename, error_msg))\n",
    "        processing_details.append({\n",
    "            'filename': filename,\n",
    "            'error': error_msg,\n",
    "            'status': 'failed'\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed: {len(processed_tables)} tables\")\n",
    "print(f\"Failed: {len(failed_files)} files\")\n",
    "if len(processed_tables) + len(failed_files) > 0:\n",
    "    success_rate = len(processed_tables)/(len(processed_tables)+len(failed_files))*100\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"\\nPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFailed files ({len(failed_files)}):\")\n",
    "    for i, (filename, error) in enumerate(failed_files, 1):\n",
    "        print(f\"  {i:2d}. {filename}\")\n",
    "        print(f\"      Error: {error[:100]}...\" if len(error) > 100 else f\"      Error: {error}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created Delta tables ({len(processed_tables)}):\")\n",
    "successful_details = [d for d in processing_details if d['status'] == 'success']\n",
    "for i, detail in enumerate(successful_details, 1):\n",
    "    print(f\"  {i:2d}. {detail['table_name']}\")\n",
    "    print(f\"      Source: {detail['filename']} ({detail['file_type'].upper()})\")\n",
    "    print(f\"      Data: {detail['rows']:,} rows, {detail['columns']} columns\")\n",
    "\n",
    "# Statistics\n",
    "if successful_details:\n",
    "    total_rows = sum(d['rows'] for d in successful_details)\n",
    "    avg_columns = sum(d['columns'] for d in successful_details) / len(successful_details)\n",
    "    \n",
    "    # File type breakdown\n",
    "    file_types = {}\n",
    "    for detail in successful_details:\n",
    "        ft = detail['file_type'].upper()\n",
    "        file_types[ft] = file_types.get(ft, 0) + 1\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total rows processed: {total_rows:,}\")\n",
    "    print(f\"  Average columns per table: {avg_columns:.1f}\")\n",
    "    print(f\"  File type breakdown: {dict(file_types)}\")\n",
    "    if successful_details:\n",
    "        largest = max(successful_details, key=lambda x: x['rows'])\n",
    "        print(f\"  Largest table: {largest['table_name']} ({largest['rows']:,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processing report\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_source': 'DOE',\n",
    "    'total_files': len(doe_files),\n",
    "    'successful_tables': len(processed_tables),\n",
    "    'failed_files': len(failed_files),\n",
    "    'success_rate': len(processed_tables)/(len(processed_tables)+len(failed_files))*100 if (len(processed_tables)+len(failed_files)) > 0 else 0,\n",
    "    'processing_details': processing_details,\n",
    "    'processed_tables': processed_tables,\n",
    "    'failed_files': [{'filename': f, 'error': e} for f, e in failed_files]\n",
    "}\n",
    "\n",
    "report_path = os.path.join(bronze_layer_path, 'processing_report.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessing report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example table validation\n",
    "if processed_tables:\n",
    "    print(\"\\nEXAMPLE TABLE VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    example_table = processed_tables[0]\n",
    "    example_path = os.path.join(bronze_layer_path, example_table)\n",
    "    \n",
    "    print(f\"Validating: {example_table}\")\n",
    "    \n",
    "    df_example = spark.read.format(\"delta\").load(example_path)\n",
    "    \n",
    "    print(f\"\\nTable statistics:\")\n",
    "    print(f\"  Rows: {df_example.count():,}\")\n",
    "    print(f\"  Columns: {len(df_example.columns)}\")\n",
    "    \n",
    "    print(f\"\\nColumn names (first 5):\")\n",
    "    for col_name in df_example.columns[:5]:\n",
    "        print(f\"  - {col_name}\")\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    df_example.show(3, truncate=True)\n",
    "    \n",
    "    print(f\"\\nData quality check:\")\n",
    "    total_rows = df_example.count()\n",
    "    for col_name in df_example.columns[:3]:\n",
    "        null_count = df_example.filter(col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")\n",
    "print(\"\\nProcessing complete. Check the bronze_doe directory for Delta tables.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}