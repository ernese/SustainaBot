{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSA Income and Consumption Data to Bronze Layer\n",
    "\n",
    "This notebook processes PSA Income and Consumption CSV files to Delta tables.\n",
    "Data includes poverty statistics, consumption patterns, and socioeconomic indicators.\n",
    "\n",
    "Features:\n",
    "- Multiple encoding support for robust file processing\n",
    "- Skip bad lines automatically\n",
    "- Comprehensive error handling and reporting\n",
    "- Empty row filtering and data cleaning\n",
    "\n",
    "Expected format:\n",
    "- Row 1: Title (quoted string)\n",
    "- Row 2: Empty\n",
    "- Row 3: Column headers (semicolon-separated)\n",
    "- Row 4+: Data (semicolon-separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Delta Lake\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"PSA-Income-Consumption-Bronze\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and helper functions\n",
    "psa_data_path = \"../PSA/Income and Consumption\"\n",
    "bronze_layer_path = \"../final-spark-bronze/bronze_income_consumption\"\n",
    "\n",
    "os.makedirs(bronze_layer_path, exist_ok=True)\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names for Delta compatibility\"\"\"\n",
    "    if not col_name or col_name.strip() == \"\":\n",
    "        return \"unnamed_column\"\n",
    "    \n",
    "    cleaned = str(col_name).strip().replace('\"', '').replace(\"'\", '')\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', cleaned)\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    \n",
    "    if not cleaned:\n",
    "        cleaned = \"unnamed_column\"\n",
    "    elif cleaned[0].isdigit():\n",
    "        cleaned = f\"col_{cleaned}\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def clean_table_name(filename):\n",
    "    \"\"\"Generate clean table name from filename\"\"\"\n",
    "    table_name = filename.replace('.csv', '')\n",
    "    table_name = clean_column_name(table_name)\n",
    "    table_name = f\"psa_income_consumption_{table_name}\"\n",
    "    \n",
    "    if len(table_name) > 100:\n",
    "        table_name = table_name[:100].rstrip('_')\n",
    "    \n",
    "    return table_name\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"Detect file encoding by trying to read first few lines\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                f.read(1000)\n",
    "            return encoding\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return 'latin1'\n",
    "\n",
    "def read_psa_csv_with_pandas(file_path):\n",
    "    \"\"\"Read PSA CSV using pandas with robust encoding handling\"\"\"\n",
    "    try:\n",
    "        detected_encoding = detect_encoding(file_path)\n",
    "        print(f\"    Detected encoding: {detected_encoding}\")\n",
    "        \n",
    "        encodings_to_try = [detected_encoding, 'utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "        encodings_to_try = list(dict.fromkeys(encodings_to_try))\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                df_pandas = pd.read_csv(\n",
    "                    file_path,\n",
    "                    sep=';',\n",
    "                    skiprows=2,\n",
    "                    encoding=encoding,\n",
    "                    na_values=['', '....', 'null', 'NULL', 'n/a', 'N/A'],\n",
    "                    keep_default_na=True,\n",
    "                    dtype=str,\n",
    "                    on_bad_lines='skip',\n",
    "                    skipinitialspace=True\n",
    "                )\n",
    "                \n",
    "                print(f\"    Successfully read with encoding: {encoding}\")\n",
    "                break\n",
    "                \n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "            except pd.errors.EmptyDataError:\n",
    "                return None, False, \"File is empty or has no valid data\"\n",
    "        else:\n",
    "            return None, False, \"Could not decode file with any supported encoding\"\n",
    "        \n",
    "        if df_pandas.empty:\n",
    "            return None, False, \"DataFrame is empty after parsing\"\n",
    "        \n",
    "        original_shape = df_pandas.shape\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        df_pandas = df_pandas.dropna(how='all')\n",
    "        df_pandas = df_pandas[~df_pandas.apply(lambda x: x.str.strip().eq('').all(), axis=1)]\n",
    "        \n",
    "        if df_pandas.empty:\n",
    "            return None, False, \"No valid data rows found after cleaning\"\n",
    "        \n",
    "        # Clean column names and handle duplicates\n",
    "        df_pandas.columns = [clean_column_name(col) for col in df_pandas.columns]\n",
    "        \n",
    "        seen_columns = {}\n",
    "        new_columns = []\n",
    "        for col in df_pandas.columns:\n",
    "            if col in seen_columns:\n",
    "                seen_columns[col] += 1\n",
    "                new_columns.append(f\"{col}_{seen_columns[col]}\")\n",
    "            else:\n",
    "                seen_columns[col] = 0\n",
    "                new_columns.append(col)\n",
    "        \n",
    "        df_pandas.columns = new_columns\n",
    "        \n",
    "        print(f\"    Cleaned data: {original_shape} -> {df_pandas.shape}\")\n",
    "        \n",
    "        df_spark = spark.createDataFrame(df_pandas)\n",
    "        return df_spark, True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, False, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "print(\"Helper functions defined\")\n",
    "print(f\"Source path: {psa_data_path}\")\n",
    "print(f\"Target path: {bronze_layer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of PSA Income and Consumption CSV files\n",
    "psa_files = [f for f in os.listdir(psa_data_path) if f.endswith('.csv')]\n",
    "psa_files.sort()\n",
    "\n",
    "print(f\"Found {len(psa_files)} PSA Income and Consumption CSV files to process\")\n",
    "print(\"Files to process (first 15):\")\n",
    "for i, file in enumerate(psa_files[:15], 1):\n",
    "    print(f\"  {i:2d}. {file}\")\n",
    "if len(psa_files) > 15:\n",
    "    print(f\"  ... and {len(psa_files) - 15} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all Income and Consumption files\n",
    "processed_tables = []\n",
    "failed_files = []\n",
    "processing_details = []\n",
    "\n",
    "print(f\"Processing all {len(psa_files)} files...\")\n",
    "\n",
    "for i, filename in enumerate(psa_files, 1):\n",
    "    print(f\"\\nProcessing {i}/{len(psa_files)}: {filename}\")\n",
    "    \n",
    "    table_name = clean_table_name(filename)\n",
    "    file_path = os.path.join(psa_data_path, filename)\n",
    "    \n",
    "    try:\n",
    "        df, parse_success, parse_error = read_psa_csv_with_pandas(file_path)\n",
    "        \n",
    "        if not parse_success:\n",
    "            print(f\"  Failed to parse: {parse_error}\")\n",
    "            failed_files.append((filename, parse_error))\n",
    "            processing_details.append({\n",
    "                'filename': filename,\n",
    "                'error': parse_error,\n",
    "                'status': 'failed'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        \n",
    "        print(f\"  Parsed: {row_count} rows, {col_count} columns\")\n",
    "        \n",
    "        if row_count == 0:\n",
    "            print(f\"  Skipped: No data rows\")\n",
    "            failed_files.append((filename, \"No data rows found\"))\n",
    "            processing_details.append({\n",
    "                'filename': filename,\n",
    "                'error': \"No data rows found\",\n",
    "                'status': 'failed'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        delta_path = os.path.join(bronze_layer_path, table_name)\n",
    "        \n",
    "        df.write.format(\"delta\") \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "          .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "          .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "          .save(delta_path)\n",
    "        \n",
    "        df_check = spark.read.format(\"delta\").load(delta_path)\n",
    "        verify_count = df_check.count()\n",
    "        \n",
    "        print(f\"  Saved and verified: {verify_count} rows\")\n",
    "        processed_tables.append(table_name)\n",
    "        \n",
    "        processing_details.append({\n",
    "            'filename': filename,\n",
    "            'table_name': table_name,\n",
    "            'rows': verify_count,\n",
    "            'columns': col_count,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Processing error: {str(e)}\"\n",
    "        print(f\"  {error_msg}\")\n",
    "        failed_files.append((filename, error_msg))\n",
    "        processing_details.append({\n",
    "            'filename': filename,\n",
    "            'error': error_msg,\n",
    "            'status': 'failed'\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed: {len(processed_tables)} tables\")\n",
    "print(f\"Failed: {len(failed_files)} files\")\n",
    "if len(processed_tables) + len(failed_files) > 0:\n",
    "    success_rate = len(processed_tables)/(len(processed_tables)+len(failed_files))*100\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"\\nPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFailed files ({len(failed_files)}):\")\n",
    "    for i, (filename, error) in enumerate(failed_files[:10], 1):\n",
    "        print(f\"  {i:2d}. {filename}\")\n",
    "        print(f\"      Error: {error[:100]}...\" if len(error) > 100 else f\"      Error: {error}\")\n",
    "    if len(failed_files) > 10:\n",
    "        print(f\"  ... and {len(failed_files) - 10} more failures\")\n",
    "\n",
    "print(f\"\\nSuccessfully created Delta tables ({len(processed_tables)}):\")\n",
    "successful_details = [d for d in processing_details if d['status'] == 'success']\n",
    "for i, detail in enumerate(successful_details[:10], 1):\n",
    "    print(f\"  {i:2d}. {detail['table_name']}\")\n",
    "    print(f\"      Data: {detail['rows']:,} rows, {detail['columns']} columns\")\n",
    "if len(successful_details) > 10:\n",
    "    print(f\"  ... and {len(successful_details) - 10} more tables\")\n",
    "\n",
    "# Statistics\n",
    "if successful_details:\n",
    "    total_rows = sum(d['rows'] for d in successful_details)\n",
    "    avg_columns = sum(d['columns'] for d in successful_details) / len(successful_details)\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total rows processed: {total_rows:,}\")\n",
    "    print(f\"  Average columns per table: {avg_columns:.1f}\")\n",
    "    print(f\"  Largest table: {max(successful_details, key=lambda x: x['rows'])['rows']:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processing report\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_source': 'PSA Income and Consumption',\n",
    "    'total_files': len(psa_files),\n",
    "    'successful_tables': len(processed_tables),\n",
    "    'failed_files': len(failed_files),\n",
    "    'success_rate': len(processed_tables)/(len(processed_tables)+len(failed_files))*100 if (len(processed_tables)+len(failed_files)) > 0 else 0,\n",
    "    'processing_details': processing_details,\n",
    "    'processed_tables': processed_tables,\n",
    "    'failed_files': [{'filename': f, 'error': e} for f, e in failed_files]\n",
    "}\n",
    "\n",
    "report_path = os.path.join(bronze_layer_path, 'processing_report.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessing report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example table validation\n",
    "if processed_tables:\n",
    "    print(\"\\nEXAMPLE TABLE VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    example_table = processed_tables[0]\n",
    "    example_path = os.path.join(bronze_layer_path, example_table)\n",
    "    \n",
    "    print(f\"Validating: {example_table}\")\n",
    "    \n",
    "    df_example = spark.read.format(\"delta\").load(example_path)\n",
    "    \n",
    "    print(f\"\\nTable statistics:\")\n",
    "    print(f\"  Rows: {df_example.count():,}\")\n",
    "    print(f\"  Columns: {len(df_example.columns)}\")\n",
    "    \n",
    "    print(f\"\\nColumn names (first 5):\")\n",
    "    for col_name in df_example.columns[:5]:\n",
    "        print(f\"  - {col_name}\")\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    df_example.show(3, truncate=True)\n",
    "    \n",
    "    print(f\"\\nData quality check:\")\n",
    "    total_rows = df_example.count()\n",
    "    for col_name in df_example.columns[:3]:\n",
    "        null_count = df_example.filter(col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")\n",
    "print(\"\\nProcessing complete. Check the bronze_income_consumption directory for Delta tables.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}