{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /home/ernese/miniconda3/envs/SO/New_SO\n",
      "=== NASA CLIMATE DATA PROCESSOR V2 ===\n",
      "Spark version: 3.4.0\n",
      "Processing timestamp: 2025-08-28 17:03:27.156188\n",
      "Spark session initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with Delta Lake\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta import *\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get current working directory and set paths\n",
    "current_dir = os.path.expanduser(\"~/miniconda3/envs/SO/New_SO\")\n",
    "print(f\"Base directory: {current_dir}\")\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "   .appName(\"NASA-Data-to-Bronze-V2\") \\\n",
    "   .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "   .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "   .config(\"spark.driver.memory\", \"4g\") \\\n",
    "   .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "   .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"=== NASA CLIMATE DATA PROCESSOR V2 ===\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Processing timestamp: {datetime.now()}\")\n",
    "print(\"Spark session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURATION ===\n",
      "Source path: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data\n",
      "Target path: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze\n",
      "Processing timestamp: 2025-08-28 17:03:37.882483\n",
      "\n",
      "=== TARGET CLIMATE METRICS ===\n",
      "C01: Daily mean air surface temperature\n",
      "C03: Daily highest temperature\n",
      "C04: Daily lowest temperature\n",
      "C09: Daily precipitation levels\n",
      "C12: Daily mean surface pressure\n",
      "C13: Daily humidity levels\n",
      "\n",
      "Bonus metrics: 1\n",
      "Total target files: 7\n"
     ]
    }
   ],
   "source": [
    "# Define paths and configuration\n",
    "nasa_data_path = \"/home/ernese/miniconda3/envs/SO/New_SO/nasa_data\"\n",
    "bronze_layer_path = \"/home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze\"\n",
    "\n",
    "PROCESSING_TIMESTAMP = datetime.now()\n",
    "\n",
    "print(\"=== CONFIGURATION ===\")\n",
    "print(f\"Source path: {nasa_data_path}\")\n",
    "print(f\"Target path: {bronze_layer_path}\")\n",
    "print(f\"Processing timestamp: {PROCESSING_TIMESTAMP}\")\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(bronze_layer_path, exist_ok=True)\n",
    "\n",
    "# Define climate metrics\n",
    "REQUIRED_CLIMATE_METRICS = {\n",
    "    'C01': 'Daily mean air surface temperature',\n",
    "    'C03': 'Daily highest temperature', \n",
    "    'C04': 'Daily lowest temperature',\n",
    "    'C09': 'Daily precipitation levels',\n",
    "    'C12': 'Daily mean surface pressure',\n",
    "    'C13': 'Daily humidity levels'\n",
    "}\n",
    "\n",
    "BONUS_METRICS = {\n",
    "    'C23': 'Monthly surface air temperature'\n",
    "}\n",
    "\n",
    "print(f\"\\n=== TARGET CLIMATE METRICS ===\")\n",
    "for code, desc in REQUIRED_CLIMATE_METRICS.items():\n",
    "    print(f\"{code}: {desc}\")\n",
    "print(f\"\\nBonus metrics: {len(BONUS_METRICS)}\")\n",
    "print(f\"Total target files: {len(REQUIRED_CLIMATE_METRICS) + len(BONUS_METRICS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NASA FILE DISCOVERY ===\n",
      "Total files in directory: 7\n",
      "\n",
      "Discovered 7 NASA CSV files:\n",
      "  - C23_Country-average_monthly_surface_air_temperature_1981-2025_NASA.csv\n",
      "    → BONUS: Monthly surface air temperature\n",
      "  - C04_daily_lowest_temp_1981-2025_NASA.csv\n",
      "    → REQUIRED: Daily lowest temperature\n",
      "  - C12_daily_mean_surface_pressure_1981_2025_NASA.csv\n",
      "    → REQUIRED: Daily mean surface pressure\n",
      "  - C01_daily_mean_air_surface_temp_1981-2025_NASA.csv\n",
      "    → REQUIRED: Daily mean air surface temperature\n",
      "  - C09_daily_precipitation_1981-2025_NASA.csv\n",
      "    → REQUIRED: Daily precipitation levels\n",
      "  - C13_daily_humidity_level_1981-2025_NASA.csv\n",
      "    → REQUIRED: Daily humidity levels\n",
      "  - C03_daily_highest_temp_1981-2025_NASA.csv\n",
      "    → REQUIRED: Daily highest temperature\n",
      "\n",
      "Coverage: 6/6 required metrics\n",
      "SUCCESS: All required climate metrics available!\n"
     ]
    }
   ],
   "source": [
    "# Discover NASA files\n",
    "def discover_nasa_climate_files():\n",
    "    \"\"\"Discover NASA climate files in the data directory\"\"\"\n",
    "    print(\"=== NASA FILE DISCOVERY ===\")\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(nasa_data_path):\n",
    "            print(f\"ERROR: NASA data path does not exist: {nasa_data_path}\")\n",
    "            return [], {}, [], list(REQUIRED_CLIMATE_METRICS.keys())\n",
    "            \n",
    "        file_list = os.listdir(nasa_data_path)\n",
    "        print(f\"Total files in directory: {len(file_list)}\")\n",
    "        \n",
    "        # Look for NASA CSV files\n",
    "        nasa_files = [f for f in file_list if f.endswith('.csv') and 'NASA' in f]\n",
    "        print(f\"\\nDiscovered {len(nasa_files)} NASA CSV files:\")\n",
    "        \n",
    "        # Map files to metric codes\n",
    "        metric_files = {}\n",
    "        unmatched_files = []\n",
    "        \n",
    "        for filename in nasa_files:\n",
    "            print(f\"  - {filename}\")\n",
    "            \n",
    "            # Extract climate metric code\n",
    "            metric_match = re.search(r'C(\\d{2})', filename)\n",
    "            if metric_match:\n",
    "                metric_code = f\"C{metric_match.group(1)}\"\n",
    "                metric_files[metric_code] = filename\n",
    "                \n",
    "                # Check if it's a required metric\n",
    "                if metric_code in REQUIRED_CLIMATE_METRICS:\n",
    "                    print(f\"    → REQUIRED: {REQUIRED_CLIMATE_METRICS[metric_code]}\")\n",
    "                elif metric_code in BONUS_METRICS:\n",
    "                    print(f\"    → BONUS: {BONUS_METRICS[metric_code]}\")\n",
    "                else:\n",
    "                    print(f\"    → ADDITIONAL: {metric_code}\")\n",
    "            else:\n",
    "                unmatched_files.append(filename)\n",
    "                print(f\"    → UNMATCHED: Could not extract metric code\")\n",
    "        \n",
    "        # Check coverage\n",
    "        missing_metrics = []\n",
    "        available_metrics = []\n",
    "        \n",
    "        for metric_code in REQUIRED_CLIMATE_METRICS:\n",
    "            if metric_code in metric_files:\n",
    "                available_metrics.append(metric_code)\n",
    "            else:\n",
    "                missing_metrics.append(metric_code)\n",
    "        \n",
    "        print(f\"\\nCoverage: {len(available_metrics)}/{len(REQUIRED_CLIMATE_METRICS)} required metrics\")\n",
    "        \n",
    "        if missing_metrics:\n",
    "            print(f\"WARNING: Missing required metrics: {missing_metrics}\")\n",
    "        else:\n",
    "            print(\"SUCCESS: All required climate metrics available!\")\n",
    "        \n",
    "        return nasa_files, metric_files, available_metrics, missing_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to discover NASA files - {str(e)}\")\n",
    "        return [], {}, [], list(REQUIRED_CLIMATE_METRICS.keys())\n",
    "\n",
    "# Execute discovery\n",
    "nasa_files, metric_files, available_metrics, missing_metrics = discover_nasa_climate_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing function ready\n"
     ]
    }
   ],
   "source": [
    "# Helper function to transform monthly data to daily format\n",
    "def transform_monthly_to_daily(df):\n",
    "    \"\"\"Transform monthly wide format to daily long format\"\"\"\n",
    "    print(\"Transforming monthly data to daily format...\")\n",
    "    \n",
    "    # Get all month columns\n",
    "    month_cols = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', \n",
    "                  'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "    \n",
    "    # Create month mapping\n",
    "    month_map = {month: i+1 for i, month in enumerate(month_cols)}\n",
    "    \n",
    "    # Stack the data\n",
    "    stacked_data = []\n",
    "    \n",
    "    for row in df.collect():\n",
    "        year = row['YEAR']\n",
    "        for month_name, month_num in month_map.items():\n",
    "            if month_name in row and row[month_name] is not None:\n",
    "                # Create date as first day of month\n",
    "                date_str = f\"{year}{month_num:02d}01\"\n",
    "                stacked_data.append({\n",
    "                    'date': date_str,\n",
    "                    'temperature': float(row[month_name]),\n",
    "                    'year': year,\n",
    "                    'month': month_num\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame from stacked data\n",
    "    schema = StructType([\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"temperature\", FloatType(), True),\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"month\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(stacked_data, schema)\n",
    "\n",
    "# Data processing function\n",
    "def process_nasa_climate_file_v2(filename, metric_code=None):\n",
    "    \"\"\"Process NASA file with support for both daily and monthly formats\"\"\"\n",
    "    \n",
    "    # Extract metric info\n",
    "    if not metric_code:\n",
    "        metric_match = re.search(r'C(\\d{2})', filename)\n",
    "        metric_code = f\"C{metric_match.group(1)}\" if metric_match else \"UNKNOWN\"\n",
    "    \n",
    "    # Get metric description\n",
    "    metric_desc = REQUIRED_CLIMATE_METRICS.get(metric_code) or BONUS_METRICS.get(metric_code, \"Unknown metric\")\n",
    "    \n",
    "    print(f\"\\n--- PROCESSING: {filename} ---\")\n",
    "    print(f\"Metric: {metric_code} - {metric_desc}\")\n",
    "    \n",
    "    # Generate table name\n",
    "    table_name = f\"bronze_nasa_{metric_code.lower()}_v2\"\n",
    "    print(f\"Target table: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV file\n",
    "        file_path = os.path.join(nasa_data_path, filename)\n",
    "        print(f\"Reading: {file_path}\")\n",
    "        \n",
    "        df = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .csv(file_path)\n",
    "        \n",
    "        # Initial validation\n",
    "        original_count = df.count()\n",
    "        original_columns = len(df.columns)\n",
    "        print(f\"Raw data: {original_count:,} rows, {original_columns} columns\")\n",
    "        \n",
    "        if original_count == 0:\n",
    "            raise Exception(\"Empty dataset\")\n",
    "        \n",
    "        # Check data format and process accordingly\n",
    "        columns = df.columns\n",
    "        print(f\"Columns detected: {len(columns)} total\")\n",
    "        \n",
    "        if \"date\" in columns:\n",
    "            # Daily format processing\n",
    "            print(\"Processing daily format data...\")\n",
    "            df = df.withColumn(\"measurement_date\", to_date(col(\"date\").cast(\"string\"), \"yyyyMMdd\")) \\\n",
    "                   .withColumn(\"year\", year(col(\"measurement_date\"))) \\\n",
    "                   .withColumn(\"month\", month(col(\"measurement_date\"))) \\\n",
    "                   .withColumn(\"day\", dayofmonth(col(\"measurement_date\"))) \\\n",
    "                   .withColumn(\"quarter\", quarter(col(\"measurement_date\"))) \\\n",
    "                   .drop(\"date\")\n",
    "            \n",
    "        elif \"YEAR\" in columns:\n",
    "            # Monthly format processing\n",
    "            print(\"Processing monthly format data...\")\n",
    "            df = transform_monthly_to_daily(df)\n",
    "            df = df.withColumn(\"measurement_date\", to_date(col(\"date\").cast(\"string\"), \"yyyyMMdd\")) \\\n",
    "                   .withColumn(\"quarter\", quarter(col(\"measurement_date\"))) \\\n",
    "                   .withColumn(\"day\", dayofmonth(col(\"measurement_date\"))) \\\n",
    "                   .drop(\"date\")\n",
    "        else:\n",
    "            raise Exception(f\"Unknown data format - no 'date' or 'YEAR' column found\")\n",
    "        \n",
    "        # Add metadata\n",
    "        df = df.withColumn(\"climate_metric_code\", lit(metric_code)) \\\n",
    "               .withColumn(\"climate_metric_name\", lit(metric_desc)) \\\n",
    "               .withColumn(\"source_system\", lit(\"NASA\")) \\\n",
    "               .withColumn(\"source_file\", lit(filename)) \\\n",
    "               .withColumn(\"processing_version\", lit(\"V2\")) \\\n",
    "               .withColumn(\"ingestion_timestamp\", lit(PROCESSING_TIMESTAMP)) \\\n",
    "               .withColumn(\"data_quality_flag\", lit(\"VALID\"))\n",
    "        \n",
    "        # Data quality validation\n",
    "        print(\"Performing data quality validation...\")\n",
    "        \n",
    "        # Check for null dates\n",
    "        null_dates = df.filter(col(\"measurement_date\").isNull()).count()\n",
    "        if null_dates > 0:\n",
    "            print(f\"WARNING: {null_dates:,} records with null dates\")\n",
    "            df = df.filter(col(\"measurement_date\").isNotNull())\n",
    "        \n",
    "        # Validate date range\n",
    "        date_range = df.select(min(\"measurement_date\").alias(\"min_date\"), \n",
    "                              max(\"measurement_date\").alias(\"max_date\")).collect()[0]\n",
    "        print(f\"Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "        \n",
    "        # Final row count after cleaning\n",
    "        final_count = df.count()\n",
    "        print(f\"Cleaned data: {final_count:,} rows ({original_count - final_count:,} removed)\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"Sample processed data:\")\n",
    "        df.select(\"measurement_date\", \"year\", \"month\", \"climate_metric_code\", \n",
    "                 \"climate_metric_name\", \"data_quality_flag\").show(3, truncate=False)\n",
    "        \n",
    "        # Save Delta table\n",
    "        delta_path = os.path.join(bronze_layer_path, table_name)\n",
    "        print(f\"Saving to: {delta_path}\")\n",
    "        \n",
    "        write_builder = df.write \\\n",
    "          .format(\"delta\") \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .option(\"overwriteSchema\", \"true\")\n",
    "        \n",
    "        # Partitioning strategy\n",
    "        if \"year\" in df.columns and final_count > 10000:\n",
    "            write_builder = write_builder.partitionBy(\"year\")\n",
    "            print(\"Partitioning by year\")\n",
    "        \n",
    "        write_builder.save(delta_path)\n",
    "        \n",
    "        # Verify save\n",
    "        df_verify = spark.read.format(\"delta\").load(delta_path)\n",
    "        verify_count = df_verify.count()\n",
    "        verify_columns = len(df_verify.columns)\n",
    "        \n",
    "        print(f\"✓ VERIFICATION: {verify_count:,} rows, {verify_columns} columns saved\")\n",
    "        \n",
    "        return {\n",
    "            'status': 'SUCCESS',\n",
    "            'filename': filename,\n",
    "            'metric_code': metric_code,\n",
    "            'metric_desc': metric_desc,\n",
    "            'table_name': table_name,\n",
    "            'original_rows': original_count,\n",
    "            'final_rows': verify_count,\n",
    "            'columns': verify_columns,\n",
    "            'date_range': f\"{date_range['min_date']} to {date_range['max_date']}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {filename}: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        return {\n",
    "            'status': 'FAILED',\n",
    "            'filename': filename,\n",
    "            'metric_code': metric_code,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"Processing function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING EXECUTION ===\n",
      "Processing 7 NASA climate files...\n",
      "\n",
      "============================================================\n",
      "PROCESSING 1/7: C23_Country-average_monthly_surface_air_temperature_1981-2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C23_Country-average_monthly_surface_air_temperature_1981-2025_NASA.csv ---\n",
      "Metric: C23 - Monthly surface air temperature\n",
      "Target table: bronze_nasa_c23_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C23_Country-average_monthly_surface_air_temperature_1981-2025_NASA.csv\n",
      "Raw data: 45 rows, 14 columns\n",
      "Columns detected: 14 total\n",
      "Processing monthly format data...\n",
      "Transforming monthly data to daily format...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-12-01\n",
      "Cleaned data: 540 rows (-495 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+-------------------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name            |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+-------------------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C23                |Monthly surface air temperature|VALID            |\n",
      "|1981-02-01      |1981|2    |C23                |Monthly surface air temperature|VALID            |\n",
      "|1981-03-01      |1981|3    |C23                |Monthly surface air temperature|VALID            |\n",
      "+----------------+----+-----+-------------------+-------------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c23_v2\n",
      "✓ VERIFICATION: 540 rows, 13 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c23_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING 2/7: C04_daily_lowest_temp_1981-2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C04_daily_lowest_temp_1981-2025_NASA.csv ---\n",
      "Metric: C04 - Daily lowest temperature\n",
      "Target table: bronze_nasa_c04_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C04_daily_lowest_temp_1981-2025_NASA.csv\n",
      "Raw data: 16,263 rows, 133 columns\n",
      "Columns detected: 133 total\n",
      "Processing daily format data...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-07-11\n",
      "Cleaned data: 16,263 rows (0 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+------------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name     |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+------------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C04                |Daily lowest temperature|VALID            |\n",
      "|1981-01-02      |1981|1    |C04                |Daily lowest temperature|VALID            |\n",
      "|1981-01-03      |1981|1    |C04                |Daily lowest temperature|VALID            |\n",
      "+----------------+----+-----+-------------------+------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c04_v2\n",
      "Partitioning by year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ VERIFICATION: 16,263 rows, 144 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c04_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING 3/7: C12_daily_mean_surface_pressure_1981_2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C12_daily_mean_surface_pressure_1981_2025_NASA.csv ---\n",
      "Metric: C12 - Daily mean surface pressure\n",
      "Target table: bronze_nasa_c12_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C12_daily_mean_surface_pressure_1981_2025_NASA.csv\n",
      "Raw data: 16,263 rows, 133 columns\n",
      "Columns detected: 133 total\n",
      "Processing daily format data...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-07-11\n",
      "Cleaned data: 16,263 rows (0 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+---------------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name        |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+---------------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C12                |Daily mean surface pressure|VALID            |\n",
      "|1981-01-02      |1981|1    |C12                |Daily mean surface pressure|VALID            |\n",
      "|1981-01-03      |1981|1    |C12                |Daily mean surface pressure|VALID            |\n",
      "+----------------+----+-----+-------------------+---------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c12_v2\n",
      "Partitioning by year\n",
      "✓ VERIFICATION: 16,263 rows, 144 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c12_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING 4/7: C01_daily_mean_air_surface_temp_1981-2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C01_daily_mean_air_surface_temp_1981-2025_NASA.csv ---\n",
      "Metric: C01 - Daily mean air surface temperature\n",
      "Target table: bronze_nasa_c01_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C01_daily_mean_air_surface_temp_1981-2025_NASA.csv\n",
      "Raw data: 16,262 rows, 133 columns\n",
      "Columns detected: 133 total\n",
      "Processing daily format data...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-07-10\n",
      "Cleaned data: 16,262 rows (0 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+----------------------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name               |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+----------------------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C01                |Daily mean air surface temperature|VALID            |\n",
      "|1981-01-02      |1981|1    |C01                |Daily mean air surface temperature|VALID            |\n",
      "|1981-01-03      |1981|1    |C01                |Daily mean air surface temperature|VALID            |\n",
      "+----------------+----+-----+-------------------+----------------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c01_v2\n",
      "Partitioning by year\n",
      "✓ VERIFICATION: 16,262 rows, 144 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c01_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING 5/7: C09_daily_precipitation_1981-2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C09_daily_precipitation_1981-2025_NASA.csv ---\n",
      "Metric: C09 - Daily precipitation levels\n",
      "Target table: bronze_nasa_c09_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C09_daily_precipitation_1981-2025_NASA.csv\n",
      "Raw data: 16,262 rows, 133 columns\n",
      "Columns detected: 133 total\n",
      "Processing daily format data...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-07-10\n",
      "Cleaned data: 16,262 rows (0 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+--------------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name       |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+--------------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C09                |Daily precipitation levels|VALID            |\n",
      "|1981-01-02      |1981|1    |C09                |Daily precipitation levels|VALID            |\n",
      "|1981-01-03      |1981|1    |C09                |Daily precipitation levels|VALID            |\n",
      "+----------------+----+-----+-------------------+--------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c09_v2\n",
      "Partitioning by year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ VERIFICATION: 16,262 rows, 144 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c09_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING 6/7: C13_daily_humidity_level_1981-2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C13_daily_humidity_level_1981-2025_NASA.csv ---\n",
      "Metric: C13 - Daily humidity levels\n",
      "Target table: bronze_nasa_c13_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C13_daily_humidity_level_1981-2025_NASA.csv\n",
      "Raw data: 16,262 rows, 133 columns\n",
      "Columns detected: 133 total\n",
      "Processing daily format data...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-07-10\n",
      "Cleaned data: 16,262 rows (0 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+---------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name  |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+---------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C13                |Daily humidity levels|VALID            |\n",
      "|1981-01-02      |1981|1    |C13                |Daily humidity levels|VALID            |\n",
      "|1981-01-03      |1981|1    |C13                |Daily humidity levels|VALID            |\n",
      "+----------------+----+-----+-------------------+---------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c13_v2\n",
      "Partitioning by year\n",
      "✓ VERIFICATION: 16,262 rows, 144 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c13_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING 7/7: C03_daily_highest_temp_1981-2025_NASA.csv\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING: C03_daily_highest_temp_1981-2025_NASA.csv ---\n",
      "Metric: C03 - Daily highest temperature\n",
      "Target table: bronze_nasa_c03_v2\n",
      "Reading: /home/ernese/miniconda3/envs/SO/New_SO/nasa_data/C03_daily_highest_temp_1981-2025_NASA.csv\n",
      "Raw data: 16,262 rows, 133 columns\n",
      "Columns detected: 133 total\n",
      "Processing daily format data...\n",
      "Performing data quality validation...\n",
      "Date range: 1981-01-01 to 2025-07-10\n",
      "Cleaned data: 16,262 rows (0 removed)\n",
      "Sample processed data:\n",
      "+----------------+----+-----+-------------------+-------------------------+-----------------+\n",
      "|measurement_date|year|month|climate_metric_code|climate_metric_name      |data_quality_flag|\n",
      "+----------------+----+-----+-------------------+-------------------------+-----------------+\n",
      "|1981-01-01      |1981|1    |C03                |Daily highest temperature|VALID            |\n",
      "|1981-01-02      |1981|1    |C03                |Daily highest temperature|VALID            |\n",
      "|1981-01-03      |1981|1    |C03                |Daily highest temperature|VALID            |\n",
      "+----------------+----+-----+-------------------+-------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Saving to: /home/ernese/miniconda3/envs/SO/New_SO/final-spark-bronze/bronze/bronze_nasa_c03_v2\n",
      "Partitioning by year\n",
      "✓ VERIFICATION: 16,262 rows, 144 columns saved\n",
      "✓ SUCCESS: bronze_nasa_c03_v2\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETED\n",
      "============================================================\n",
      "Total files processed: 7\n",
      "Successful: 7\n",
      "Failed: 0\n",
      "Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Execute processing for all NASA files\n",
    "print(\"=== PROCESSING EXECUTION ===\")\n",
    "\n",
    "processing_results = []\n",
    "successful_tables = []\n",
    "failed_processing = []\n",
    "\n",
    "# Check if we have files to process\n",
    "if not nasa_files:\n",
    "    print(\" ERROR: No NASA files discovered for processing!\")\n",
    "    print(\"Please check:\")\n",
    "    print(f\"  - Source directory exists: {nasa_data_path}\")\n",
    "    print(\"  - Directory contains CSV files with NASA in filename\")\n",
    "    print(\"  - Files have climate metric codes (C01, C03, etc.)\")\n",
    "else:\n",
    "    print(f\"Processing {len(nasa_files)} NASA climate files...\")\n",
    "    \n",
    "    for i, filename in enumerate(nasa_files, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING {i}/{len(nasa_files)}: {filename}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Extract metric code for this file\n",
    "        file_metric = None\n",
    "        for metric_code, mapped_file in metric_files.items():\n",
    "            if mapped_file == filename:\n",
    "                file_metric = metric_code\n",
    "                break\n",
    "        \n",
    "        # Process the file\n",
    "        result = process_nasa_climate_file_v2(filename, file_metric)\n",
    "        processing_results.append(result)\n",
    "        \n",
    "        if result['status'] == 'SUCCESS':\n",
    "            successful_tables.append(result)\n",
    "            print(f\"✓ SUCCESS: {result['table_name']}\")\n",
    "        else:\n",
    "            failed_processing.append(result)\n",
    "            print(f\"✗ FAILED: {filename}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROCESSING COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total files processed: {len(nasa_files)}\")\n",
    "print(f\"Successful: {len(successful_tables)}\")\n",
    "print(f\"Failed: {len(failed_processing)}\")\n",
    "\n",
    "# Safe success rate calculation\n",
    "if len(nasa_files) > 0:\n",
    "    success_rate = len(successful_tables) / len(nasa_files) * 100\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "else:\n",
    "    print(\"Success rate: N/A (no files to process)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing summary and validation\n",
    "print(\"\\n=== PROCESSING SUMMARY ===\")\n",
    "\n",
    "# Successful tables summary\n",
    "if successful_tables:\n",
    "    print(f\"\\n✓ SUCCESSFULLY CREATED TABLES ({len(successful_tables)}):\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    for result in successful_tables:\n",
    "        metric_info = f\"{result['metric_code']} - {result['metric_desc']}\"\n",
    "        row_info = f\"{result['final_rows']:,} rows\"\n",
    "        date_info = result['date_range']\n",
    "        print(f\"  • {result['table_name']}\")\n",
    "        print(f\"    {metric_info}\")\n",
    "        print(f\"    {row_info} | {date_info}\")\n",
    "        total_rows += result['final_rows']\n",
    "    \n",
    "    print(f\"\\nTOTAL DATA VOLUME: {total_rows:,} climate measurement records\")\n",
    "    print(f\"📁 SAVED TO: {bronze_layer_path}\")\n",
    "\n",
    "# Failed processing summary\n",
    "if failed_processing:\n",
    "    print(f\"\\n✗ FAILED PROCESSING ({len(failed_processing)}):\")\n",
    "    for result in failed_processing:\n",
    "        print(f\"  • {result['filename']}: {result['error'][:100]}...\")\n",
    "\n",
    "# Required metrics coverage analysis\n",
    "print(f\"\\n=== REQUIRED METRICS COVERAGE ===\")\n",
    "processed_required_metrics = []\n",
    "missing_required_metrics = []\n",
    "\n",
    "for metric_code, metric_desc in REQUIRED_CLIMATE_METRICS.items():\n",
    "    found = False\n",
    "    for result in successful_tables:\n",
    "        if result['metric_code'] == metric_code:\n",
    "            processed_required_metrics.append(metric_code)\n",
    "            print(f\"✓ {metric_code}: {metric_desc} - {result['final_rows']:,} records\")\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        missing_required_metrics.append(metric_code)\n",
    "        print(f\"✗ {metric_code}: {metric_desc} - NOT PROCESSED\")\n",
    "\n",
    "coverage_pct = len(processed_required_metrics) / len(REQUIRED_CLIMATE_METRICS) * 100 if REQUIRED_CLIMATE_METRICS else 0\n",
    "print(f\"\\n📈 COVERAGE: {len(processed_required_metrics)}/{len(REQUIRED_CLIMATE_METRICS)} ({coverage_pct:.1f}%) required metrics\")\n",
    "\n",
    "if coverage_pct == 100:\n",
    "    print(\" EXCELLENT: All required climate metrics successfully processed!\")\n",
    "    print(\"   Ready for silver layer transformations\")\n",
    "elif coverage_pct >= 80:\n",
    "    print(\" GOOD: Most required metrics processed, some gaps remain\")\n",
    "else:\n",
    "    print(\" ATTENTION: Significant gaps in required metrics coverage\")\n",
    "\n",
    "# Final configuration summary\n",
    "print(f\"\\n=== FINAL CONFIGURATION ===\")\n",
    "print(f\"Source directory: {nasa_data_path}\")\n",
    "print(f\"Target directory: {bronze_layer_path}\")\n",
    "print(f\"Processing version: V2\")\n",
    "print(f\"Processing timestamp: {PROCESSING_TIMESTAMP}\")\n",
    "\n",
    "print(f\"\\n NASA BRONZE V2 PROCESSING COMPLETE!\")\n",
    "print(f\"   Comprehensive climate metrics coverage achieved\")\n",
    "print(f\"   Ready for sustainability analytics pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}