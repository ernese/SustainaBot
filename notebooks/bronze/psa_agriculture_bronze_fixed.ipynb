{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSA Agriculture Data to Bronze Layer - Fixed CSV Parsing\n",
    "\n",
    "This notebook properly handles the PSA CSV format:\n",
    "- Row 1: Title (quoted string)\n",
    "- Row 2: Empty\n",
    "- Row 3: Column headers (semicolon-separated)\n",
    "- Row 4+: Data (semicolon-separated)\n",
    "\n",
    "Key fixes:\n",
    "1. Skip first 2 rows during CSV reading\n",
    "2. Use row 3 as proper headers\n",
    "3. Parse semicolon-separated data correctly\n",
    "4. Clean column names for Delta compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Delta Lake\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import re\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"PSA-Agriculture-Bronze-Fixed\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and helper functions\n",
    "psa_data_path = "abfss://WS_SO_SustainabilityData_Prd@onelake.dfs.fabric.microsoft.com/SO_raw_to_bronze.Lakehouse/Files/PSA/Agriculture, Forestry, Fisheries"\n",
    "bronze_layer_path = "abfss://WS_SO_SustainabilityData_Prd@onelake.dfs.fabric.microsoft.com/SO_raw_to_bronze.Lakehouse/Tables"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(bronze_layer_path, exist_ok=True)\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names for Delta compatibility\"\"\"\n",
    "    if not col_name or col_name.strip() == \"\":\n",
    "        return \"unnamed_column\"\n",
    "    \n",
    "    # Remove quotes and strip\n",
    "    cleaned = str(col_name).strip().replace('\"', '').replace(\"'\", '')\n",
    "    \n",
    "    # Replace problematic characters with underscores\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', cleaned)\n",
    "    \n",
    "    # Remove multiple underscores and clean up\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    \n",
    "    # Ensure not empty and doesn't start with number\n",
    "    if not cleaned:\n",
    "        cleaned = \"unnamed_column\"\n",
    "    elif cleaned[0].isdigit():\n",
    "        cleaned = f\"col_{cleaned}\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def clean_table_name(filename):\n",
    "    \"\"\"Generate clean table name from filename\"\"\"\n",
    "    table_name = filename.replace('.csv', '')\n",
    "    table_name = clean_column_name(table_name)\n",
    "    table_name = f\"psa_agriculture_{table_name}\"\n",
    "    \n",
    "    # Limit length\n",
    "    if len(table_name) > 100:\n",
    "        table_name = table_name[:100].rstrip('_')\n",
    "    \n",
    "    return table_name\n",
    "\n",
    "def read_psa_csv_with_pandas(file_path):\n",
    "    \"\"\"Read PSA CSV using pandas to handle the format correctly\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file using pandas with proper settings\n",
    "        df_pandas = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=';',\n",
    "            skiprows=2,  # Skip title and empty row\n",
    "            encoding='utf-8',\n",
    "            na_values=['', '....', 'null', 'NULL'],\n",
    "            keep_default_na=True,\n",
    "            dtype=str  # Keep everything as string initially\n",
    "        )\n",
    "        \n",
    "        # Clean column names\n",
    "        df_pandas.columns = [clean_column_name(col) for col in df_pandas.columns]\n",
    "        \n",
    "        # Convert to Spark DataFrame\n",
    "        df_spark = spark.createDataFrame(df_pandas)\n",
    "        \n",
    "        return df_spark, True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, False, str(e)\n",
    "\n",
    "print(\"Helper functions defined\")\n",
    "print(f\"Source path: {psa_data_path}\")\n",
    "print(f\"Target path: {bronze_layer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of PSA Agriculture CSV files\n",
    "psa_files = [f for f in os.listdir(psa_data_path) if f.endswith('.csv')]\n",
    "psa_files.sort()\n",
    "\n",
    "print(f\"Found {len(psa_files)} PSA Agriculture CSV files to process\")\n",
    "print(\"Files to process:\")\n",
    "for i, file in enumerate(psa_files[:10], 1):\n",
    "    print(f\"  {i:2d}. {file}\")\n",
    "if len(psa_files) > 10:\n",
    "    print(f\"  ... and {len(psa_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single file first to test the approach\n",
    "test_file = psa_files[0]\n",
    "test_file_path = os.path.join(psa_data_path, test_file)\n",
    "test_table_name = clean_table_name(test_file)\n",
    "\n",
    "print(f\"Testing with file: {test_file}\")\n",
    "print(f\"Table name: {test_table_name}\")\n",
    "\n",
    "# Process the test file\n",
    "df_test, success, error = read_psa_csv_with_pandas(test_file_path)\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nSuccessfully parsed CSV:\")\n",
    "    print(f\"Rows: {df_test.count()}\")\n",
    "    print(f\"Columns: {len(df_test.columns)}\")\n",
    "    \n",
    "    print(f\"\\nColumn names:\")\n",
    "    for i, col in enumerate(df_test.columns[:10]):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    if len(df_test.columns) > 10:\n",
    "        print(f\"  ... and {len(df_test.columns) - 10} more columns\")\n",
    "    \n",
    "    print(f\"\\nSchema:\")\n",
    "    df_test.printSchema()\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    df_test.show(5, truncate=True)\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to parse CSV: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If test was successful, save the test table\n",
    "if success:\n",
    "    test_delta_path = os.path.join(bronze_layer_path, test_table_name)\n",
    "    print(f\"Saving test table to: {test_delta_path}\")\n",
    "    \n",
    "    df_test.write.format(\"delta\") \\\n",
    "           .mode(\"overwrite\") \\\n",
    "           .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "           .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "           .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "           .save(test_delta_path)\n",
    "    \n",
    "    # Verify saved table\n",
    "    df_verify = spark.read.format(\"delta\").load(test_delta_path)\n",
    "    print(f\"Verified saved table: {df_verify.count()} rows, {len(df_verify.columns)} columns\")\n",
    "    print(\"Test table saved successfully\")\n",
    "else:\n",
    "    print(\"Cannot proceed - test file failed to parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files if test was successful\n",
    "if success:\n",
    "    processed_tables = []\n",
    "    failed_files = []\n",
    "    \n",
    "    print(f\"Processing all {len(psa_files)} files...\")\n",
    "    \n",
    "    for i, filename in enumerate(psa_files, 1):\n",
    "        print(f\"\\nProcessing {i}/{len(psa_files)}: {filename}\")\n",
    "        \n",
    "        table_name = clean_table_name(filename)\n",
    "        file_path = os.path.join(psa_data_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Process the CSV file\n",
    "            df, parse_success, parse_error = read_psa_csv_with_pandas(file_path)\n",
    "            \n",
    "            if not parse_success:\n",
    "                print(f\"  Failed to parse: {parse_error}\")\n",
    "                failed_files.append((filename, parse_error))\n",
    "                continue\n",
    "            \n",
    "            # Basic validation\n",
    "            row_count = df.count()\n",
    "            col_count = len(df.columns)\n",
    "            \n",
    "            print(f\"  Parsed: {row_count} rows, {col_count} columns\")\n",
    "            \n",
    "            if row_count == 0:\n",
    "                print(f\"  Skipped: No data rows\")\n",
    "                failed_files.append((filename, \"No data rows found\"))\n",
    "                continue\n",
    "            \n",
    "            # Save as Delta table\n",
    "            delta_path = os.path.join(bronze_layer_path, table_name)\n",
    "            \n",
    "            df.write.format(\"delta\") \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "              .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "              .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "              .save(delta_path)\n",
    "            \n",
    "            # Verify\n",
    "            df_check = spark.read.format(\"delta\").load(delta_path)\n",
    "            verify_count = df_check.count()\n",
    "            \n",
    "            print(f\"  Saved and verified: {verify_count} rows\")\n",
    "            processed_tables.append(table_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Processing error: {str(e)}\"\n",
    "            print(f\"  {error_msg}\")\n",
    "            failed_files.append((filename, error_msg))\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Successfully processed: {len(processed_tables)} tables\")\n",
    "    print(f\"Failed: {len(failed_files)} files\")\n",
    "    print(f\"Success rate: {len(processed_tables)/(len(processed_tables)+len(failed_files))*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Skipping batch processing - test file failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "if success:\n",
    "    print(\"\\nPROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\nFailed files ({len(failed_files)}):\")\n",
    "        for filename, error in failed_files:\n",
    "            print(f\"  - {filename}: {error}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully created Delta tables:\")\n",
    "    for table in processed_tables[:10]:\n",
    "        print(f\"  - {table}\")\n",
    "    if len(processed_tables) > 10:\n",
    "        print(f\"  ... and {len(processed_tables) - 10} more tables\")\n",
    "    \n",
    "    # Save processing report\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_files': len(psa_files),\n",
    "        'successful_tables': len(processed_tables),\n",
    "        'failed_files': len(failed_files),\n",
    "        'success_rate': len(processed_tables)/(len(processed_tables)+len(failed_files))*100,\n",
    "        'processed_tables': processed_tables,\n",
    "        'failed_files': [{'filename': f, 'error': e} for f, e in failed_files]\n",
    "    }\n",
    "    \n",
    "    report_path = os.path.join(bronze_layer_path, 'processing_report.json')\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nProcessing report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example table validation\n",
    "if success and processed_tables:\n",
    "    print(\"\\nEXAMPLE TABLE VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    example_table = processed_tables[0]\n",
    "    example_path = os.path.join(bronze_layer_path, example_table)\n",
    "    \n",
    "    print(f\"Validating: {example_table}\")\n",
    "    \n",
    "    df_example = spark.read.format(\"delta\").load(example_path)\n",
    "    \n",
    "    print(f\"\\nTable statistics:\")\n",
    "    print(f\"  Rows: {df_example.count():,}\")\n",
    "    print(f\"  Columns: {len(df_example.columns)}\")\n",
    "    \n",
    "    print(f\"\\nColumn names (first 5):\")\n",
    "    for col_name in df_example.columns[:5]:\n",
    "        print(f\"  - {col_name}\")\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    df_example.show(3, truncate=True)\n",
    "    \n",
    "    print(f\"\\nData quality check:\")\n",
    "    total_rows = df_example.count()\n",
    "    for col_name in df_example.columns[:3]:\n",
    "        null_count = df_example.filter(col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        print(f\"  {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}